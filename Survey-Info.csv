ID,Name,Authors,Bibtex,DOI,Year,Venue,XR Type,Main category,Secondary category,Application,Note,User Experiment Description,Questionnaires,Behavioural,Physiological,Performance,Metric Types
ID001,3vr: vice versa virtual reality algorithm to track and map user experience,"Vasic  Iva,  Quattrini  Ramona,  Pierdicca  Roberto,  Mancini  Adriano,  Vasic  Bata",,10.1145/3656346,2024,Journal of Computing and Cultural Heritage,360 video,Behavioural Dynamics & Exploration,,Cultural heritage,"The paper presents 3VR (Vice Versa Virtual Reality) — an algorithm designed to track and map user behavior in virtual museum environments using panoramic data. The approach extracts Regions of Interest (ROIs) by combining gaze direction, field of view, and user interactions (click events, menu selections, navigation) to reveal patterns of user attention and engagement.
The system was tested on an online virtual museum of the Civic Art Gallery of Ascoli Piceno, comprising 81 panoramic views and multiple interactive elements. Data from 171 visitors across 19 countries were collected and analyzed to identify spatial attention patterns. The authors found that users primarily focused on artworks and key exhibits early in the virtual visit, with declining engagement as they progressed through later panoramas.
Main outcomes include:
• Identification of salient viewing regions using panoramic coordinates and interaction logs.
• Behavioral analysis showing attention decay and navigation preferences.
• Development of a privacy-compliant tracking method that avoids personal data collection.
• Visualization of interaction frequencies and ROI heat distributions to guide museum curators in designing more engaging digital experiences.
The paper thus contributes a behavioral analytics method for understanding spatial attention and interaction patterns in VR-based virtual tours.","1) Participants
• N = 171 visitors (global online participation from 19 countries).
• No demographic data collected (GDPR-compliant anonymous logging).
• Data recorded over 18 days (23 Nov – 11 Dec 2022).
2) Study Design
• Ecological, large-scale online experiment within a public virtual museum interface.
• Visitors freely explored 81 interconnected panoramic scenes via hotspots, maps, and thumbnail menus.
• Tracking parameters: horizontal/vertical gaze angles, field of view (FOV), interaction timestamps, and event types.
• Data captured at 1-second intervals to build per-user interaction timelines and panoramic attention maps.
3) Apparatus
• Custom-developed WebVR platform (no external analytics or cookies).
• Tracking implemented via JavaScript capturing:
– Angles (horizontal/vertical movement, FOV)
– Click events (maps, hotspots, thumbnails, artwork info panels)
– Interaction descriptors (24 total; e.g., “map hotspot,” “panel HD painting,” “button help”).
• Data logged in real-time via HTTP requests to spreadsheets and processed in MATLAB for ROI identification.
4) Metrics Collected
Behavioural Metrics:
• ROI coordinates from panoramic gaze movement (derived from horizontal & vertical pixel mapping).
• Interaction frequency with specific elements (maps, menus, hotspots, HD artwork panels).
• Active engagement time per panorama.
• Sequence and duration of interactions.
Derived Metrics:
• ROI clustering (visitor attention density per panorama).
• Interaction network per user and panorama (top 10 most active users and scenes).
• Temporal distribution of engagement (time-on-scene decay).
Descriptive Statistics:
• Frequency histograms of 24 tracked interactive features.
• Correlation between initial view and engagement length.
Main Findings
• High attention concentration on artworks and central regions of panoramas (ROI clustering confirms visual salience).
• Engagement decay over time — most interactions occurred in first 3 panoramas.
• Hotspots were the most frequently used navigation elements, followed by the map and thumbnail menus.
• User attention maps provide actionable insight for curatorial design, suggesting the inclusion of dynamic or interactive content to maintain engagement.
• The algorithm offers an efficient, privacy-friendly, scalable behavioral metric for tracking user focus and activity in online VR environments.",,"Interaction dynamics,  Gaze Analysis",,,Behavioural
ID002,A Comparison of Visual Attention Guiding Approaches for 360° Image-Based VR Tours,"Wallgrun  Jan Oliver,  Bagher  Mahda M.,  Sajjadi  Pejman,  Klippel  Alexander",,10.1109/VR46266.2020.00026,2020,Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops,VR,Visualization Techniques,,Human-Computer Interaction (HCI),"The study demonstrates that all three tested visual guidance mechanisms (Arrow, Butterfly Guide, Radar) significantly improve target-finding performance and subjective user experience in 360° image-based VR tours compared to no guidance. Among the tested tools, the Arrow mechanism was the most consistently preferred — rated highest in ease of use, clarity, comfort, and aesthetics, while minimizing distraction. Although the Butterfly Guide and Radar also improved performance, they showed more polarized reactions, with users criticizing BG's visual distraction and R's complexity. These findings support the value of simple, subtle visual cues in enhancing navigation and user satisfaction in VR educational experiences.","1. Participants
• 33 participants (mean age: 22.87; 57.56% female).
• Recruited from a university; compensated $10.
2. Study Design
• Within-subject design: Each participant experienced four VR tours, each using a different guidance condition:
    ◦ Arrow (A)
    ◦ Butterfly Guide (BG)
    ◦ Radar (R)
    ◦ No Guidance (NG) (baseline)
3. Procedure
• Participants watched four VR tours with audio commentary while seated in swivel chairs using Oculus Go HMDs.
• After each tour, they filled out a post-tour questionnaire.
• The session included pre-study and post-study questionnaires, and lasted ~60 minutes total.
4. Task
• In each tour (3 scenes with 12 total POIs), participants were asked to locate points of interest referenced in the narration using different visual attention guides.
5. Metrics Collected
• Behavioral Metrics: Head tracking data to calculate Normalized Target Finding Time (NTFT).
• Questionnaires:
    ◦ Game Experience Questionnaire (GEQ): 7 dimensions (immersion, competence, flow, etc.).
    ◦ Custom Likert questions: aesthetics, ease of use, annoyance, attention division.
    ◦ Star ratings and open-ended feedback post-study.",Game Experience Questionnaire (GEQ),Head Analysis,,,"Questionnaires,  Behavioural"
ID003,A Neurophysiological Approach for Measuring Presence in Immersive Virtual Environments,"Dey  Arindam,  Phoon  Jane,  Saha  Shuvodeep,  Dobbins  Chelsea,  Billinghurst  Mark",,10.1109/ISMAR50242.2020.00072,2020,International Symposium on Mixed and Augmented Reality (ISMAR),VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study investigates neurophysiological correlates of presence in VR by comparing two immersive environments with intentionally different presence levels. Using EEG, ECG, and GSR alongside two presence questionnaires, the authors show that high presence correlates with increased heart rate, greater theta and beta activity in the frontal cortex, and stronger alpha signals in the parietal region. The findings support a multi-modal, objective approach to presence measurement and highlight how immersive experience can be tracked through physiological and neurological markers.","1. Participants
• Number: 24 participants (9 female, 15 male).
• Age: 20–30 years (Mean = 22.21, SD = 2.12).
• Experience:
    ◦ 6 had prior VR experience (distributed equally across conditions).
    ◦ Most were regular gamers (21/24).
    ◦ All had normal or corrected-to-normal vision.
2. Study Design
• Type: Between-subjects experiment (each participant experienced only one condition).
• Conditions:
    ◦ High Presence (HP): High visual fidelity, embodied hands, interactivity (e.g., touching animals, controlling cart movement), object-based sounds.
    ◦ Low Presence (LP): Lower visual fidelity, detached hands, no interactivity, ambient sounds only.
• Baseline: 30 seconds eyes-open/eyes-closed rest before the experiment.
3. Procedure
1. Setup:
    ◦ Participants seated on a revolving chair to minimize movement artifacts.
    ◦ Fitted with:
        ▪ EEG (Emotiv Epoc+, 14-channel).
        ▪ ECG (Shimmer3, 5-lead) for heart rate.
        ▪ GSR (galvanic skin response) for electrodermal activity (EDA).
    ◦ HTC Vive headset and Logitech headphones for VR.
2. Task:
    ◦ 4-minute VR experience: A jungle ride in a virtual cart with non-aggressive animals.
    ◦ HP: Participants could interact (start/stop cart, touch animals).
    ◦ LP: Passive observation only.
3. Post-Task:
    ◦ Subjective questionnaires:
        ▪ SUS Presence Questionnaire (6 items).
        ▪ Witmer & Singer Presence Questionnaire (32 items).
4. Task
• Primary Activity: Passive navigation through a virtual jungle.
• Interactivity (HP only):
    ◦ Button presses to start/stop the cart.
    ◦ Touching animals triggered reactions (e.g., running away).
• LP: No interaction; static experience.
5. Metrics Collected
• Physiological:
    ◦ EEG: Theta (4–7 Hz), alpha (8–13 Hz), and beta (13–24 Hz) band power, inter-hemispheric coherence.
    ◦ ECG: Heart rate (HR).
    ◦ GSR: Tonic (SCL) and phasic (SCR) electrodermal activity.
• Behavioral:
    ◦ Interaction logs (button presses, animal touches in HP).
• Subjective:
    ◦ Presence ratings (SUS and Witmer & Singer questionnaires).
Key Findings
• HP vs. LP:
    ◦ EEG: Higher theta (frontal), beta (frontal), and alpha (parietal) activity in HP.
    ◦ HR: Significantly higher in HP (HP: 81.9 bpm vs. LP: 73.5 bpm).
    ◦ EDA: No significant difference (contrary to prior stress-inducing VR studies).
• Questionnaires: HP rated significantly higher for presence.
Limitations
• Small sample size (12 per condition).
• Seated design limited natural movement.
• Consumer-grade EEG (lower resolution).
This study validated neurophysiological metrics (EEG, HR) as objective presence indicators, reducing reliance on subjective questionnaires. It meets IC1 (generalizable insights) but not IC2 (no novel metrics proposed). No exclusion criteria (EC1–EC5) are violated. Recommendation: Include for RQ1/RQ2 (metrics for presence) but not RQ3 (outcomes are specific to calm VR).",Presence – General,,"ECG,  EDA / GSR (Skin Conductance),  EEG",,"Questionnaires,  Physiological"
ID004,A Physiological Approach of Presence and VR Sickness in Simulated Teleoperated Social Tasks,"Achanccaray  David,  Sumioka  Hidenobu",,10.1109/SMC53992.2023.10394241,2023,International Conference on Systems Man and Cybernetics (SMC),VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Telecommunications and Collaboration,"This study explores the use of multimodal physiological signals (EEG, skin conductance, temperature, heart rate, eye-tracking, and motion data) to assess the subjective experience of presence and VR sickness during simulated teleoperated social tasks in virtual reality. It is one of the first works to investigate physiological predictors of presence and sickness specifically in teleoperated social interaction contexts. Using 23 participants, the authors found significant correlations between EEG-derived neural features (especially in beta and delta bands in frontal-central and occipital regions) and subjective presence and VR sickness questionnaire scores. These correlations suggest that such biomarkers may help predict user experience trends in teleoperation and immersive social tasks. The findings extend physiological UX evaluation frameworks into a new applied XR domain.","1) Participants
• Number of Participants: 25 (13 males and 12 females)
• Age Range: 21 to 40 years (mean age: 30.48 ± 7.20)
• Exclusion Criteria: Participants had no background 
of neurological diseases or experience in VR systems or multimodal 
sensor systems. Informed consent was obtained, and participants received
 a monetary incentive https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=7162,8092, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=8093,8832.
2) Study Design
• The study was designed to assess the effects of attention and stress
 on teleoperated social tasks in a virtual reality (VR) environment. The
 experiment involved three different task conditions: normal, attention,
 and stress https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=10961,11576, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=1,825.
3) Procedure
• Participants engaged in three runs, one for each task condition, 
with each run consisting of 24 trials. Prior to each task, participants 
practiced to familiarize themselves with the tasks. Each trial lasted 45
 seconds and included interactions with a simulated visitor https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=12331,13174, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=11577,12330.
• After completing the tasks, participants filled out questionnaires 
to assess their sense of presence and any VR sickness experienced https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=7162,8092, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=13175,13872.
4) Task
• Normal Task: Participants selected actions for the avatar without any cues.
• Attention Task: A figure representing an action 
appeared on the screen for five seconds, and participants had to click 
the corresponding button.
• Stress Task: Similar to the attention task, but included a wrong label and a countdown to distract participants https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=12331,13174 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=13873,14531.
5) Metrics Collected
• Physiological Metrics: 
    ◦ EEG data (brain signals) were recorded to assess neural activity.
    ◦ Peripheral signals were collected using an E4 wristband, measuring 
skin conductance response (SCR), temperature, blood volume pulse (BVP), 
and accelerometer data.
    ◦ Eye-tracking data were also collected to measure gaze speed https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=17025,17760, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=21883,22486.
• Questionnaires: 
    ◦ Simulator Sickness Questionnaire (SSQ): Assessed symptoms of VR sickness across three subscales (nausea, oculomotor, disorientation) and an overall severity score.
    ◦ Slater-Usoh-Steed (SUS) Questionnaire: Measured the sense of presence in the virtual environment https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=13873,14531, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681482e747f563a6ef2b1730/?range=26971,27979.
This structured approach allowed the researchers to analyze the 
relationship between physiological responses and subjective experiences 
of presence and VR sickness during teleoperated social tasks.","Presence – General,  SSQ (Simulator Sickness Questionnaire)",,"BVP / PPG,  Eye-Tracking,  EDA / GSR (Skin Conductance),  Skin Temperature,  EEG",,"Questionnaires,  Physiological"
ID005,"A Physiology-Based QoE Comparison of Interactive Augmented Reality, Virtual Reality and Tablet-Based Applications","Keighrey  Conor,  Flynn  Ronan,  Murray  Siobhan,  Murray  Niall",,10.1109/TMM.2020.2982046,2021,Transactions on Multimedia,"AR, Tablet-based, VR",Interaction Techniques & input Modalities,,"Healthcare – Speech and Language Therapy, Human-Computer Interaction (HCI)","The work presented in this paper reports the results of a novel QoE comparison for three immersive technologies, namely, aug- mented reality (AR), tablet, and virtual reality (VR). A multi- media speech and language assessment application, which eval- uates a user’s ability to understand sematic links, was developed for each of the platforms. The comparison between AR, tablet, and VR is based on explicit metrics (post-experience question- naire) and implicit metrics (physiological metrics of heart rate and electrodermal activity (EDA)). The results indicate a higher QoE for the AR and tablet groups, based on the comparative implicit and explicit analysis.

a post-test questionnaire containing fourteen
questions captured user response to aspects such as enjoyment,immersion, interaction and discomfort.","1. Participants
• A total of 67 participants were recruited using convenience sampling.
• 60 participants were included in the final analysis after excluding 7 due to screening and technical errors.
• Participants were divided into three groups: Augmented Reality (AR), Virtual Reality (VR), and Tablet, with each group having no prior experience with the specific AR or VR hardware used https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=21918,22748 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=11134,12049.
2. Study Design
• The study employed a between-subjects design, where
 participants were evenly split among the three test groups (AR, VR, and
 Tablet) to avoid the influence of pre-exposure to assessment stimuli https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=21918,22748, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=20159,21062.
3. Procedure
The experiment consisted of four phases:
1. Information and Screening Phase: Participants were informed about the study and screened for visual defects using the Ishihara and Snellen tests https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=18382,19256, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=43464,43955.
2. Resting Phase: A 5-minute baseline measure of heart rate (HR) and electrodermal activity (EDA) was captured while participants relaxed https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=19259,20156, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=15954,16758.
3. Training Phase: Participants underwent training 
with the technology, which included watching training videos and 
practicing with a simplified version of the assessment https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=21065,21917, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=20159,21062.
4. Testing Phase: Participants interacted with the 
immersive speech and language therapy assessment, which included eleven 
slides (one practice and ten test slides). HR and EDA were recorded 
throughout this phase https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=21065,21917 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=20159,21062.
4. Task
• The task involved a virtual speech and language assessment
 based on the semantic memory assessment from the Comprehensive Aphasia 
Test (CAT). Participants were required to link a central image with a 
semantic target among distracters https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=13660,14531 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=12796,13659.
5. Metrics Collected
• Explicit Measures: A post-test questionnaire with 
14 questions assessed user responses on enjoyment, immersion, 
interaction, and discomfort, using a 5-point Likert scale https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=14532,15277 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=15278,15951.
• Implicit Measures: Continuous monitoring of heart 
rate (HR) using a Fitbit device and electrodermal activity (EDA) using a
 Pip biosensor during the resting, training, and testing phases https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=15954,16758 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b856bb0d925d4e18d711b8/?range=19259,20156.
This structured approach allowed the researchers to evaluate the 
quality of experience (QoE) across different immersive technologies 
effectively.","Custom made,  MOS",,"EDA / GSR (Skin Conductance),  Heart Rate,  Metrics - Heart Rate (BPM)",Task Success / Completion,"Questionnaires,  Physiological,  Performance"
ID006,a quality of experience and visual attention evaluation for 360° videos with non-spatial and spatial audio,"Hirway  Amit,  Qiao  Yuansong,  Murray  Niall",,10.1145/3650208,2024,Transactions on Multimedia Computing Communications and Applications,VR,User states: Cognitive & Affective Experience,Content & System Design.,,"This paper investigates how different types of audio—no sound, non-spatial stereo audio, first-order ambisonics, and third-order ambisonics—influence users’ Quality of Experience (QoE), visual attention, and physiological responses when watching 360° videos in VR. A total of 73 participants viewed ten indoor and outdoor 360° videos while the researchers collected head pose, eye gaze, pupil dilation, and heart-rate data using an HTC Vive with an integrated Tobii eye tracker and an Empatica E4 wristband. Subjective QoE was assessed with a custom 20-item questionnaire. Results show that the richness and spatial fidelity of audio strongly affect attention distribution, arousal indicators (pupil diameter, heart rate), and user-perceived immersion and presence. Third-order ambisonics consistently produced higher engagement, greater sensory involvement, and elevated physiological responses compared to all other audio types. The paper contributes a multi-modal dataset (objective + physiological + subjective data) and insights into how spatial audio modulates viewing behaviour and QoE in immersive 360° video settings.","1. Participants
• 73 participants completed the study out of an initial 80 recruited; 7 were excluded based on vision and hearing screening.
• Participants were distributed across four sound conditions:
    ◦ No sound (n = 18)
    ◦ Stereo (n = 19)
    ◦ First-order ambisonics (n = 18)
    ◦ Third-order ambisonics (n = 18)
• Age range: 14–62 years, average age 28.54 ± 7.26.
• 26 participants had prior VR experience; 26 were VR-novices; experience data missing for the first 21 participants.
• Recruitment used convenience sampling within the university context.

2. Study Design
• Between-subjects experimental design: each participant experienced only one sound condition to avoid cross-condition contamination.
• All participants watched the same set of 10 videos, divided into 5 indoor and 5 outdoor scenes.
• Audio conditions varied across:
    ◦ No sound
    ◦ Stereo
    ◦ First-order ambisonics
    ◦ Third-order ambisonics
• The video order inside each category (indoor/outdoor) was randomized.
• Sound condition assignment was randomized.

3. Procedure
1. Information Phase (10 min)
    ◦ Study description, participant questions, informed consent.
2. Screening Phase (10 min)
    ◦ Vision tests: Snellen, Ishihara.
    ◦ Auditory screening using online hearing test.
3. Training Phase (5 min)
    ◦ Participants viewed a 60-second 360° video to familiarize themselves with the VR setup.
    ◦ Eye-tracking and HMD calibration performed.
4. Testing Phase (10 min viewing)
    ◦ Participants watched two stitched 300-second videos (Indoor segment + Outdoor segment).
    ◦ Viewed on HTC Vive with Tobii eye tracker; seated on a swivel chair to allow 360° exploration.
5. Post-Viewing Phase (5–10 min)
    ◦ Participants completed a 20-item subjective questionnaire about immersion, presence, spatial sound quality, and sensory engagement.
• Total duration of session: 40–50 minutes.

4. Tasks
• Participants passively viewed ten 60-second 360° video clips, grouped into two 5-minute segments (indoor/outdoor).
• Tasks involved:
    ◦ Natural viewing of immersive video content.
    ◦ Exploring the 360° environment via head rotation.
    ◦ No interaction tasks; the study focused on perception, attention, and physiological response.

5. Metrics Collected
Objective Behavioural Metrics
• Head pose (yaw, pitch, roll) at 120 Hz.
• Eye gaze fixations (left/right X,Y coordinates) at 120 Hz.
Physiological Metrics
• Pupil diameter (left and right eye) at 120 Hz.
• Heart rate collected via E4 wristband at 1 Hz.
Subjective Metrics
• Custom questionnaire, 20 items, ACR 5-point scale.
    ◦ Measures: presence, immersion, sound realism, sensory engagement, naturalness, attention retention.
    ◦ Cronbach’s alpha (reliability): 0.749–0.878 depending on sound condition.","Presence – General,  IEQ (Immersion Experience Questionnaire)","Head Analysis,  Gaze Analysis","Heart Rate,  Pupil Analysis",,"Questionnaires,  Behavioural,  Physiological"
ID007,A Questionnaire-Based and Physiology-Inspired Quality of Experience Evaluation of an Immersive Multisensory Wheelchair Simulator,"Salgado  Débora Pereira,  Flynn  Ronan,  Naves  Eduardo Lázaro Martins,  Murray  Niall",,10.1145/3524273.3528175,2022,Multimedia Systems Conference (MMSys),"VR, haptics",User states: Cognitive & Affective Experience,Content & System Design.,"Assistive Technology – Wheelchair Training Simulator, Healthcare and Medical Training","This study evaluates user experience and quality of experience (QoE) in an immersive multisensory wheelchair simulator using a comprehensive multi-metric approach. Three configurations were tested: a non-immersive desktop version and two VR conditions with varying motion acceleration profiles. The study used a combination of physiological (EDA, HRV), performance (task errors, time), and self-reported metrics (SUS, NASA-TLX, IPQ, SAM, SSQ). Results showed that immersive VR with smoother motion acceleration led to improved arousal and presence, better usability scores, and more favorable physiological responses. Findings suggest that motion configuration and display type significantly affect immersive experience quality.","1. Participants
• A total of 62 subjects were recruited, but only 57 participants were included in the analysis after screening (5 were deemed ineligible). Participants were randomly assigned to three groups:
    ◦ Desktop Group: 24 participants (12 male, 12 female)
    ◦ Headset Group 1: 17 participants (10 male, 7 female)
    ◦ Headset Group 2: 16 participants (8 male, 8 female)
• The average ages were approximately:
    ◦ Desktop Group: 27.2 years
    ◦ Headset Group 1: 30.3 years
    ◦ Headset Group 2: 30.1 years https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=21223,21993 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=27471,27816.
2. Study Design
• The study employed a between-groups design to 
compare the effects of different output display devices (2D display vs. 
HMD) on user experience in a virtual wheelchair training application https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=21994,22837, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=21223,21993.
3. Procedure
• The experiment consisted of five phases:
    1. Information Phase: Participants were informed about the study and signed consent forms.
    2. Screening Phase: Assessed visual acuity and color perception, and checked for eligibility (e.g., epilepsy, sleep deprivation).
    3. Training Phase: Participants practiced using the virtual wheelchair in a free environment for about 5 minutes.
    4. Testing Phase: Participants completed the ramp navigation course while interacting with the simulator.
    5. Post-Experience Questionnaire Phase: Participants completed various questionnaires to assess their experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=26503,27146, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=24619,25449.
4. Task
• The primary task involved navigating a ramp navigation course
 using a virtual wheelchair simulator. Participants interacted with the 
simulator, which provided multi-sensory feedback (visual, auditory, and 
haptic) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=13260,14165, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=48971,49588.
5. Metrics Collected
• Explicit Metrics:
    ◦ NASA-TLX scores (cognitive task load)
    ◦ Self-Assessment Manikin (SAM) for emotional responses (valence, arousal, dominance)
    ◦ System Usability Scale (SUS) for usability
    ◦ Igroup Presence Questionnaire (IPQ) for presence
    ◦ Simulator Sickness Questionnaire (SSQ) for headset groups only https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=18799,19645, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=50594,51145.
• Implicit Metrics:
    ◦ Physiological measures using the Empatica E4 wristband, including:
        ▪ Heart Rate (HR)
        ▪ Inter-beat Interval (IBI)
        ▪ Electrodermal Activity (EDA) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=7125,7768 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=16304,17061.
• Performance Metrics:
    ◦ Number of commands executed
    ◦ Number of collisions (errors)
    ◦ Time taken to complete the task https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=24619,25449 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bc3ffe533a01a217d61283/?range=25450,26502.","IPQ (Igroup Presence Questionnaire),  NASA-TLX,  SAM (Self-Assessment Manikin),  SSQ (Simulator Sickness Questionnaire),  System Usability Scale (SUS)",,"EDA / GSR (Skin Conductance),  Heart Rate,  HRV / IBI","Completion Time,  Error Rate","Questionnaires,  Physiological,  Performance"
ID008,"A Structural Equation Modeling Approach to Understand the Relationship between Control, Cybersickness and Presence in Virtual Reality","Venkatakrishnan  Rohith,  Venkatakrishnan  Roshan,  Anaraky  Reza Ghaiumy,  Volonte  Matias,  Knijnenburg  Bart,  Babu  Sabarish V",,10.1109/VR46266.2020.00091,2020,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,Interaction Techniques & input Modalities.,Automotive and Aviation,"This study explores the interplay between motion control, cybersickness, presence, workload, and time perception in a VR driving simulation using structural equation modeling (SEM). Participants experienced one of three conditions: active driving, passive viewing (yoked), or autonomous vehicle control. Using a combination of physiological (EDA), subjective (SSQ, NASA-TLX, SUS, presence questionnaire), and behavioral data, the study reveals that having motion control increases both presence and workload, which in turn increases cybersickness. Cybersickness negatively affects presence and reduces time spent in VR. These findings provide a detailed framework to understand how control mechanisms in VR impact user experience.

This paper offers a robust and generalizable framework for understanding the multi-dimensional user experience in immersive VR, combining questionnaires, physiological sensors, and modeling techniques.","1. Participants
• A total of 63 participants were recruited from Clemson University.
• The average age was 24.1 years (standard deviation = 4.2), with 68% male.
• All participants had normal or corrected-to-normal vision.
• VR Experience: 45 participants reported having less than five hours of VR experience, while eight reported over 25 hours https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=33347,34268 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=52183,53061.
2. Study Design
• The study employed a between-subjects design with three conditions:
    1. Driving Condition: Participants had full control over the car.
    2. Autonomous Car Condition: Participants experienced a self-driving car simulation.
    3. Yoked Pair Condition: Participants experienced a replay of the simulation from a matched participant in the Driving condition https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=31585,32372, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=34269,35245.
3. Procedure
• Participants signed an informed consent form and completed a demographics questionnaire.
• They filled out the Simulator Sickness Questionnaire (SSQ) before the simulation.
• Participants were randomly assigned to one of the three conditions.
• In the Driving condition, participants were briefed on the task and equipped with an Empatica E4 sensor to record physiological data https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=36093,36926, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=35248,36092.
4. Task
• Participants performed a search task where they had
 to locate landmarks in a virtual city. The landmarks were presented in 
random order on a display unit in the virtual car.
• The task was designed to keep participants engaged and exposed to higher levels of optic flow, lasting up to 30 minutes https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=33347,34268, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=32375,33346.
5. Metrics Collected
• Self-Reported Metrics: Participants reported their comfort levels on a scale of 1 to 10 every three minutes during the simulation.
• Physiological Metrics: Skin conductance levels (SCL) were recorded using the Empatica E4 sensor.
• Questionnaires: Participants completed the SSQ, NASA-TLX (Task Load Index), and the SUS (System Usability Scale) after the simulation https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=36927,37848, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c038475d14ec32d82d8686/?range=9834,10580.
This structured approach allowed the researchers to analyze the 
effects of motion control on user experience in immersive virtual 
environments, particularly focusing on aspects like cybersickness and 
presence.","NASA-TLX,  SSQ (Simulator Sickness Questionnaire),  Self-reported comfort levels,  Social Presence",,EDA / GSR (Skin Conductance),,"Questionnaires,  Physiological"
ID009,"A Study of the Influence of AR on the Perception, Comprehension and Projection Levels of Situation Awareness","Truong-Allié  Camille,  Herbeth  Martin,  Paljic  Alexis",,10.1109/VR55154.2023.00069,2023,Conference on Virtual Reality and 3D User Interfaces,AR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This study evaluates how different types of AR guidance (no AR, AR path only, AR path with warnings) affect situation awareness (SA) across perception, comprehension, and projection levels, particularly for secondary elements (e.g., hazards or misplaced objects) in an industrial navigation task. Using a real-world adaptation of the SAGAT method, the study found that while AR improved perceived security and adherence to the expert path, it significantly reduced perception of secondary elements, suggesting attentional tunneling. The addition of virtual cues mitigated but did not eliminate these effects. The study introduces a validated real-world method for SA measurement in AR, offering generalizable insights into attention, safety, and interface design.","1) Participants
• Total: 10 participants.
• Demographics: Factory workers (including 2 storekeepers), aged 19–58 (median age: 36), 6 males, 4 females.
• Recruitment: Employees of an industrial factory (Parker-Hannifin) where AR is used for navigation and logistics tasks.
• Consent: All participants agreed to data collection and were informed they could withdraw at any time.
2) Study Design
• Within-subjects repeated measures design: Each participant experienced all three navigation conditions.
• Three conditions (guidance types for navigation):
    1. No AR (paper map)
    2. Virtual Path (AR-assisted navigation with a path overlay)
    3. Virtual Path + Warnings (AR-assisted navigation with path overlay + hazard warnings)
• Counterbalancing: A Graeco-Latin square design was used to control for order effects across conditions.
3) Procedure
• Training Phase:
    ◦ Participants tested all three navigation conditions and saw examples of virtual hazard warnings.
• Experimental Phase (3 trials per participant):
    ◦ Participants completed three navigation tasks in different conditions.
    ◦ They followed a path to three different locations inside the factory.
    ◦ At each location, a freeze-probe method (SAGAT) was used to assess situation awareness (SA).
    ◦ Freeze probe: Participants closed their eyes and answered SA questions about their environment.
• Post-Experiment:
    ◦ Participants rated their sense of security in each condition.
    ◦ They provided qualitative feedback on the usefulness of virtual cues.
4) Task
• Task Type: Pedestrian navigation in an industrial factory.
• Objective: Navigate to three assigned locations while avoiding hazards.
• Hazards:
    ◦ Physical items (e.g., misplaced carts, boxes)
    ◦ Hazardous areas (e.g., zones requiring helmets or safety gear)
• Guidance Variations:
    ◦ Paper map (No AR condition)
    ◦ AR Virtual Path (highlighting the route but not hazards)
    ◦ AR Virtual Path + Warnings (route + virtual hazard indicators)
• Key Challenge: Maintain awareness of secondary elements (hazards) while focusing on navigation.
5) Metrics Collected
• Situation Awareness (SA) Metrics:
    ◦ SAGAT freeze-probe technique (assessing Perception, Comprehension, and Projection levels).
    ◦ Objective SA Measures (real-time accuracy of responses).
• Performance Metrics:
    ◦ Distance to hazards (proximity to misplaced objects).
    ◦ Deviation from the expert path (how well participants followed the optimal route).
• Behavioral Metrics:
    ◦ Walking patterns (movement deviations).
• Overconfidence Metric:
    ◦ Confidence rating on SA answers (compared to correctness).
• Self-Reported Measures:
    ◦ Perceived security (Likert scale).
    ◦ Preference for guidance type (qualitative feedback).
    ◦ Usefulness of virtual hazard warnings.
Key Findings
• AR navigation reduced SA on secondary elements (especially at the perception level).
• Virtual hazard warnings improved hazard awareness but caused some overconfidence.
• AR-assisted navigation improved path-following accuracy and perceived security.",Situation Awareness Global Assessment Technique,Movement Trajectories,,Path Deviation,"Questionnaires,  Behavioural,  Performance"
ID010,A Study on Collaborative Visual Data Analysis in Augmented Reality with Asymmetric Display Types,"Friedl-Knirsch  Judith,  Stach  Christian,  Pointecker  Fabian,  Anthes  Christoph,  Roth  Daniel",,10.1109/TVCG.2024.3372103,2024,Transactions on Visualization and Computer Graphics,AR,Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,"Data Analysis – Collaborative Immersive Analytics, Human-Computer Interaction (HCI)","This study investigates how three AR device categories (handheld, optical see-through, video see-through) influence collaboration, user experience, and interaction patterns during a co-located immersive data analysis task. Using a mixed-methods approach, it finds that device type significantly affects simulator sickness, interaction with AR and physical space, and subjective experience. OST devices strike a balance between usability and comfort, while VST offers better digital visualization at the cost of motion sickness. HH devices support better real-world interaction but are cumbersome for AR tasks.","1) Participants
• Number of Participants: 18 (10 male, 8 female)
• Groups: Participants were split into six groups of three.
• Demographics: Average age was 31.78 years (SD = 
8.68). All participants had normal or corrected-to-normal eyesight, and 
those wearing glasses were excluded due to hardware limitations of the 
optical see-through (OST) device https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=28437,29172, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=29173,29775.
2) Study Design
• Type: Within-subjects mixed-methods design.
• Focus: The study aimed to explore the impact of 
three distinct categories of augmented reality (AR) display devices 
(handheld, optical see-through, and video see-through) on collaborative 
data analysis https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=1492,2440, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=535,1491.
3) Procedure
• Participants received a brief explanation of the study's purpose and provided informed consent.
• They filled out a pre-study Simulator Sickness Questionnaire (SSQ).
• Participants were introduced to the desktop interface and dataset through screenshots and a demo video.
• After the introduction, participants were given tasks to complete using different AR devices, switching devices after each task.
• After completing each task, participants filled out the SSQ, NASA 
Task Load Index (NASA-TLX), User Experience Questionnaire (UEQ), and a 
subjective measures questionnaire.
• Finally, a semi-structured qualitative group interview was conducted https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=36318,37208, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=43517,44225.
4) Task
• Dataset: The AutoMPG dataset was used for analysis.
• Tasks: Participants collaboratively answered two questions:
    1. A simple search task to find a car meeting three predefined requirements.
    2. A trend identification task to identify visible correlations among four data dimensions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=30636,31525, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=51056,51922.
5) Metrics Collected
• Quantitative Metrics:
    ◦ Speech time during the study.
    ◦ User experience scores from the UEQ.
    ◦ Task load measured by the NASA-TLX.
    ◦ System log data to analyze interaction patterns.
• Qualitative Metrics:
    ◦ Semi-structured interviews to gather insights on collaboration and user experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=43517,44225, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681335347cb155bf8bdd4950/?range=32828,33740.","NASA-TLX,  SSQ (Simulator Sickness Questionnaire),  UEQ (User Experience Questionnaire),  subjective measures questionnaire",Interaction Time,,"Accuracy,  Completion Time","Questionnaires,  Behavioural,  Performance"
ID011,A Study on the Impact of Virtual Reality on User Attention,"Baldoni  Sara,  Wahba  Mahmoud Z. A.,  Carli  Marco,  Battisti  Federica",,10.1109/ICASSPW59220.2023.10193220,2023,International Conference on Acoustics Speech and Signal Processing Workshops (ICASSPW),VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study investigates how immersive VR impacts user attention by comparing performance and physiological responses during a Stroop test conducted in both VR and 2D screen conditions. Participants showed higher accuracy and more stable heart rate during the VR condition, suggesting improved focus and reduced distraction in immersive environments. Although no new user metrics are introduced, the study provides generalizable insights into attention and engagement in VR using validated cognitive and physiological metrics.","1) Participants
• Total: 35 participants.
• Age Range: 18–78 years, distributed across six age groups (e.g., 18–25, 25–35, etc.).
• Experience:
    ◦ 77% had no prior experience with VR.
    ◦ 77% were unfamiliar with the Stroop test.
    ◦ Self-reported attention levels: 80% considered themselves ""mainly attentive,"" 8% ""always attentive,"" and 12% ""easily distracted.""
2) Study Design
• Within-Subjects Design: Each participant performed the Stroop test in two conditions:
    ◦ 2D Screen: Traditional computer display.
    ◦ VR: Oculus Quest 2 headset.
• Order Counterbalancing:
    ◦ 17 participants did VR first; 18 did 2D first (20-minute interval between sessions).
• Independent Variable: Test modality (2D vs. VR).
• Dependent Variables: Performance scores, heart rate, and subjective attention.
3) Procedure
1. Pre-Test:
    ◦ Participants completed a demographic questionnaire (age, VR familiarity, attention habits).
    ◦ Signed consent forms for data collection (heart rate, voice recordings).
2. Stroop Test:
    ◦ Conducted in both 2D and VR environments.
    ◦ Phases:
        1. Phase 1 (Neutral): Words in black (read aloud).
        2. Phase 2 (Coherent): Words matching their color (e.g., ""red"" in red).
        3. Phase 3 (Non-Coherent): Words mismatched with their color (e.g., ""red"" in blue).
    ◦ Duration: 3 minutes 7 seconds per test.
3. Data Collection:
    ◦ Voice recordings (to score accuracy).
    ◦ Heart rate monitored via a Viatom ER-1 chest strap.
4) Task
• Stroop Test: Participants named the color of displayed words while ignoring the word’s meaning.
    ◦ Phases:
        ▪ Neutral/Coherent: Baseline and low-cognitive-load tasks.
        ▪ Non-Coherent: High-cognitive-load task (measuring attention and inhibitory control).
• Goal: Compare performance (accuracy) and physiological responses (heart rate) between 2D and VR.
5) Metrics Collected
• Quantitative:
    ◦ Performance Scores: Accuracy (% correct) for each phase (Phases 1+2 combined vs. Phase 3).
    ◦ Heart Rate: Average beats per minute (bpm) and variability during each test.
• Qualitative:
    ◦ Demographic Data: Prior VR/Stroop test experience, self-reported attention levels.
Key Findings
• Higher Accuracy in VR: Participants scored 80.8% in VR vs. 75.5% in 2D during Phase 3 (non-coherent stimuli).
• Elevated Heart Rate in VR: Average heart rate was 89 bpm (VR) vs. 84 bpm (2D), with more regular trends in VR.
• Prior VR Experience: Users with VR familiarity performed better in both conditions.
• Conclusion: VR’s immersive environment reduced external distractions, enhancing focus and task performance.
This study demonstrates VR’s potential to improve attention during cognitive tasks, supported by both performance and physiological metrics.",,,"ECG,  Heart Rate",Accuracy,"Physiological,  Performance"
ID012,A Study on Virtual Reality Sickness and Visual Attention,"J. Lee,  W. Kim,  J. Kim,  S. Lee",,,2021,Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),VR,Behavioural Dynamics & Exploration,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This paper investigates how virtual reality sickness affects visual attention patterns using eye-tracking data in VR environments. Through a large-scale experiment with 21 participants and 100 VR scenes of varying sickness levels, the authors propose a novel cube-based entropy metric to quantify attention distribution. Findings show that higher VR sickness correlates with increased center-bias and reduced visual explorativeness, suggesting that sickness can constrain immersive engagement. The entropy metric demonstrated strong predictive capability for VR sickness (≈83% correlation), offering an objective method to assess user experience quality based on physiological behavior.","1) Participants:
• Number: 21 participants
• Demographics: 17 males and 4 females
• Age range: 22 to 36 years old
• Experience: All were inexperienced with VR use.
• Selection Criteria: Followed subject criteria recommended by BT.500 guidelines for subjective quality assessment.
2) Study Design:
• Type: Observational experimental study.
• Within-subjects:

Each participant experienced multiple VR scenes from a standardized database (VR-SP).
• Sessions:

Four sessions per participant, each with 25 VR video contents (total 100 VR videos).
3) Procedure:
• Participants sat in a swivel chair wearing an HTC Vive Pro Eye headset.
• Eye-tracker calibration was performed using a 6-point calibration method before the session started.
• Participants first focused on a central fixation cross for 5 seconds to standardize the initial gaze.
• Then they freely viewed each VR scene for 10 seconds.
• Breaks: After every 25 videos, participants had a 5-minute break to reduce accumulated VR sickness.
4) Task:
• Main Task:

Participants were instructed to freely explore and look around the VR environment during the 10-second video presentations.
• No specific instructions were given on where to look (natural gaze behavior was encouraged).
5) Metrics Collected:
• Behavioral Metrics:
    ◦ Eye-tracking data: 3D gaze directions (pitch, yaw, roll) during VR content viewing.
    ◦ Head orientation: Also recorded alongside gaze.
• Performance Metrics:
    ◦ Visual Entropy:

A newly defined measure of gaze distribution, modeling spatial spread of fixation points.
• Subjective Metrics:
    ◦ VR Sickness Scores:

Pre-existing mean opinion scores (MOS) for VR sickness from the VR-SP database were used for correlation analysis.
• Validation Metrics:
    ◦ Pearson’s Linear Correlation Coefficient (PLCC) and Spearman’s Rank-Order Correlation Coefficient (SROCC) between entropy and VR sickness ratings.",MOS,"Gaze heatmap,  Head Analysis,  Entropy Analysis",Eye-Tracking,,"Questionnaires,  Behavioural,  Physiological"
ID013,A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing,"Bai  Huidong,  Sasikumar  Prasanth,  Yang  Jing,  Billinghurst  Mark",,10.1145/3313831.3376550,2020,Conference on Human Factors in Computing Systems,MR,Embodiment Avatars & Social Presence,Interaction Techniques & input Modalities.,Telecommunications and Collaboration,"The paper investigates how sharing eye gaze and hand gestures in real time, as spatialized visual cues, affects task performance, co-presence, and user experience in a remote MR collaboration scenario. The study compares conditions involving only verbal instructions, gaze only, gesture only, and gaze + gesture, focusing on their effectiveness in improving spatial guidance, communication clarity, and sense of shared space.
The authors investigate how gaze and hand gesture sharing affects user experience, task efficiency, and spatial awareness in a mixed reality collaboration setting","1) Participants
• Number of Participants: 24 (12 male and 12 female)
• Age Range: 20 to 47 years old (M = 29.6, SD = 6.6)
• Familiarity: Most pairs knew each other; four 
participants used video conferencing daily, while the rest used it a few
 times a month. Four participants were familiar with AR or VR 
interfaces, rating their experience as four or higher on a 7-point 
Likert scale  .
2) Study Design
• Type: Within-subject design
• Conditions: Four communication conditions were tested:
    1. Verbal Only (Control Condition)
    2. Eye Gaze
    3. Hand Gesture
    4. Eye Gaze + Hand Gesture (Combined Condition)
• Counterbalancing: The order of cue conditions was counterbalanced between participants  .
3) Procedure
• Participants were paired as local workers and remote experts without specific role preferences.
• The study began with consent forms and demographic questions, 
followed by a training session to familiarize participants with the 
system.
• Each session included four trials under different cue conditions, 
with participants completing tasks and providing qualitative feedback 
after each trial  .
4) Task
• The experimental task involved searching for and picking up four 
arbitrary Lego bricks from a total of 20 pieces and placing them with 
the correct orientation on corresponding numbered tags.
• The local worker, wearing an AR headset, followed instructions from 
the remote expert, who provided cues through speech, gestures, and gaze 
 .
5) Metrics Collected
• Objective Measures: Task completion time was recorded to assess performance.
• Subjective Measures: Participants completed several questionnaires, including:
    ◦ NMM Social Presence Questionnaire (measuring social presence)
    ◦ MEC Spatial Presence Questionnaire (measuring the sense of being together)
    ◦ NASA Task Load Index (measuring mental and physical load)
    ◦ System Usability Scale (SUS) for usability assessment  .
This structured approach allowed the researchers to evaluate the 
effectiveness of different communication cues in a mixed reality remote 
collaboration context.","NASA-TLX,  Social Presence,  Spatial Presence Experience Scale,  System Usability Scale (SUS)",,,Completion Time,"Questionnaires,  Performance"
ID014,"Alternative Design For An Interactive Exhibit Learning In Museums: How Does User Experience Differ Across Different Technologies-VR, Tangible, And Gesture","Phichai  Pornphan,  Williamson  Julie,  Barr  Matthew",,10.23919/iLRN52045.2021.9459414,2021,Immersive Learning Research Network,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This study compares user experience across three technologies—VR, gesture-based, and tangible interfaces—in the context of interactive museum exhibits delivering the same scientific content. Using the User Experience Questionnaire (UEQ) and play time as holding power, it found that VR and tangible interfaces significantly outperformed gesture-based interaction in terms of efficiency, perspicuity, dependability, stimulation, and attractiveness. No significant differences were found for novelty or play time. The study emphasizes input precision, task-technology fit, and multimodal feedback as key to positive UX and effective learning in XR environments.","1) Participants
• A total of 31 participants took part in the study, consisting of 17 males, 13 females, and 1 other gender. 
• 22 participants had previously visited a Science and Technology museum, while 19 had experience using VR. 
• 18 participants had never experienced a gesture-based interface, and 22 had no experience with a tangible interface https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=26413,27359, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=17058,17806.
2) Study Design
• The study employed a within-group design, where each participant interacted with three different types of interactive exhibits: VR, Tangible, and Gesture-based interfaces. 
• The same scientific content was delivered across all interfaces, allowing for direct comparison of user experiences https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=11617,12613, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=26413,27359.
3) Procedure
• The experiment lasted approximately 30-40 minutes. 
• Participants signed a consent form and completed a questionnaire about their background and experience with each interface.
• Using a Latin Square design, participants were assigned a random order to interact with the three exhibits sequentially.
• After each interaction, participants completed the User Experience Questionnaire (UEQ) and participated in a semi-structured interview to discuss their experiences https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=11617,12613, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=936,1730.
4) Task
• The task involved exploring an interactive exhibit that presented 
information about biotoxin plants and animals. Participants engaged in drag-and-drop activities to interact with the exhibits, which included picking up objects and placing them in designated areas to receive feedback https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=12614,13646, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=29175,30055.
5) Metrics Collected
• Quantitative Metrics:
    ◦ User experience was measured using the User Experience Questionnaire (UEQ), which assessed six dimensions: attractiveness, perspicuity, efficiency, dependability, stimulation, and novelty.
    ◦ Holding power was measured by the time participants spent interacting with each exhibit https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=25878,26410, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=24964,25875.
• Qualitative Metrics:
    ◦ Semi-structured interviews were conducted to gather detailed 
feedback on user experiences and learning outcomes associated with each 
interface https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=936,1730, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6813458a97dbe29d69877dd3/?range=11617,12613.","UEQ (User Experience Questionnaire),  Semi-structured Interviews",,,,Questionnaires
ID015,Ambient Information Visualisation and Visitors’ Technology Acceptance of Mixed Reality in Museums,"Hammady  Ramy,  Ma  Minhua,  Strathearn  Carl",,10.1145/3359590,2020,Journal of Computing and Cultural Heritage,MR,Visualization Techniques,,Tourism and Cultural Heritage,"This study introduces the Ambient Information Visualisation Concept (AIVC) using Microsoft HoloLens to enhance interactive storytelling in a museum context. Through a mixed reality (MR) experience called ""The Battle,"" participants interacted with layered spatial content presented through holograms. The authors evaluated user experience using the Technology Acceptance Model (TAM) with 47 museum visitors. Findings showed high acceptance of MR with strong scores in ease of use, usefulness, enjoyment, and willingness to use in future museum visits. The study highlights the potential of immersive storytelling and MR holographic interfaces in public educational settings.

Questionnaires – TAM Constructs: Personal Innovativeness (PI), Enjoyment (ENJ), Usefulness (USF), Ease of Use (EOU), Willingness for Future Use (WFU)","1) Participants
• Number: 47 participants
• Demographics:
    ◦ 42.6% male, 57.4% female
    ◦ Age range: primarily 18–40 years old
    ◦ Educational background: mostly university graduates or students
• Recruitment: Daily visitors to the Egyptian department at Manchester Museum, some via social media invitation
• Prior experience:
    ◦ 48.9% were aware of AR/MR
    ◦ 17% had worn AR/MR headsets
    ◦ 0% had used AR in museums before","Custom made,  Technology Acceptance Model",,,,Questionnaires
ID016,An Analysis of Physiological and Psychological Responses in Virtual Reality and Flat Screen Gaming,"Vatsal  Ritik,  Mishra  Shrivatsa,  Thareja  Rushil,  Chakrabarty  Mrinmoy,  Sharma  Ojaswa,  Shukla  Jainendra",,10.1109/TAFFC.2024.3368703,2024,Transactions on Affective Computing,VR,User states: Cognitive & Affective Experience,,Entertainment and Gaming,"This study presents a comparative analysis of emotional and physiological responses in Virtual Reality (VR) versus flat screen (FS) gaming using both self-report (SAM, VAS) and physiological data (EDA, heart rate, HRV). The authors developed and released a multimodal dataset (VRFS), and found that VR gameplay elicited significantly higher arousal, stress, and cognitive load than FS, along with a lower sense of dominance. Although physiological effects (e.g., EDA, LF/HF) showed mixed results, the work provides generalizable insights into emotional arousal and user state modeling in immersive contexts using validated and novel physiological approaches.","1) Participants
• Total: 33 participants (20 male, 16 female; aged 19–22, μ = 20.8).
• Screening: Excluded individuals with motion sickness, neurological conditions, or COVID-19 symptoms.
• VR Experience: 45.4% (*n* = 15) had prior VR exposure.
2) Study Design
• Mixed-participant design: 2 (VR vs. flatscreen) × 4 (game quadrants).
• Games: Four commercial games selected via pilot study to cover all quadrants of the Circumplex Model of Affect (CMA):
    ◦ High Arousal/High Valence (HVHA): Dirt Rally 2.0 (racing).
    ◦ High Arousal/Low Valence (LVHA): Phasmophobia (horror).
    ◦ Low Arousal/Low Valence (LVLA): Minecraft (sandbox).
    ◦ Low Arousal/High Valence (HVLA): War Thunder (combat).
• Counterbalancing: Participants played the same game in VR and flatscreen (order randomized, 45–60 min gap between sessions).
3) Procedure
1. Pre-Session:
    ◦ Consent + demographic questionnaire.
    ◦ Baseline recording: 5-min relaxation period (physiological baseline).
    ◦ Device setup:
        ▪ Empatica E4 wristband (BVP, EDA, HR).
        ▪ Oculus Rift S (VR) or HP monitor (flatscreen).
2. Gameplay:
    ◦ Participants played one game (5–15 min) in both VR and flatscreen.
    ◦ Tasks were standardized (e.g., complete a race lap in Dirt Rally 2.0).
3. Post-Session:
    ◦ Self-reports: SAM (arousal, valence, dominance) + VAS (10 emotions, e.g., joy, fear).
    ◦ Debriefing.
4) Task
• Passive/Active Engagement:
    ◦ Dirt Rally 2.0: Steering wheel controls (VR/flatscreen).
    ◦ Phasmophobia: Horror exploration (head movement in VR).
    ◦ Minecraft/War Thunder: Standard gameplay.
• Goal: Complete predefined objectives (e.g., solve puzzles, race laps) without explicit performance scoring.
5) Metrics CollectedCategoryMetricsToolsPhysiological- Blood Volume Pulse (BVP) → Heart Rate (HR), HR Variability (LF/HF ratio).Empatica E4 wristband- Electrodermal Activity (EDA) → Skin Conductance Level (SCL), Phasic SCR.Questionnaires- SAM: Arousal, valence, dominance (5-point Likert scales).Post-game surveys- VAS: Joy, anger, fear, dizziness, etc. (0–100 sliders).Behavioral (Implied)Head movement (via VR headset tracking), off-hand restriction (for EDA).Oculus Rift S
Key Findings
• VR vs. Flatscreen:
    ◦ Higher arousal (*p* = 0.007), lower dominance (*p* = 0.02), and elevated HR (*p* = 0.016) in VR.
    ◦ No significant EDA difference, but trends toward higher SCL in VR.
• Game Effects:
    ◦ Horror (Phasmophobia) elicited highest arousal; sandbox (Minecraft) lowest.
• Dataset: VRFS dataset (15+ hours of BVP, EDA, HR, SAM/VAS) made publicly available.
Strengths & Limitations
• Strengths: Multimodal metrics, standardized tasks, public dataset.
• Limitations: Small sample, restricted hand movement (for EDA), no longitudinal data.","SAM (Self-Assessment Manikin),  VAS",,"BVP / PPG,  EDA / GSR (Skin Conductance),  HRV / IBI,  Heart Rate",,"Questionnaires,  Physiological"
ID017,An Asymmetric Multiplayer Learning Environment for Room-Scale Virtual Reality and a Handheld Device,"Holly  Michael,  Resch  Sebastian,  Pirker  Johanna,  München  Ludwig-Maximilians-Universität",,,2023,Proceedings of the ACM on Human-Computer Interaction,VR,Embodiment Avatars & Social Presence,,Education and Training,"This study presents and evaluates an asymmetric multiplayer VR learning environment, where one user engages via a room-scale VR headset and another via a tracked tablet. The focus is on collaboration, social presence, user control, and learning outcomes in a co-located, shared immersive experience. Using validated UX and motivation questionnaires (SAM, PXI, GEQ, QCM) and pre-/post-tests, the authors found that both VR and handheld users experienced high enjoyment, presence, and control, with a significant learning gain (from 41.7% to 81.2% correct answers). The study emphasizes the importance of collaborative VR learning setups and provides generalizable insights into XR user experience evaluation.","1. Participants:
    ◦ 14 students from a local educational institution were recruited for the study.
    ◦ Participants were aged between 15 and 32 (average age = 20.00, SD = 5.80).
    ◦ The group consisted of 5 males and 7 females.
    ◦ Participants had varying levels of experience with computers, VR, and digital color theory, but none were experts in the subject matter.
2. Study Design:
    ◦ The study was designed as an AB split-user study, where participants were randomly assigned into pairs.
    ◦ Each pair consisted of one VR user (using an HTC Vive headset) and one handheld user (using a Samsung Galaxy Tab S6 Lite tablet).
    ◦ After completing the first round of tasks, participants swapped roles (VR user became the handheld user and vice versa) to experience the other perspective.
3. Procedure:
    ◦ Pre-Questionnaire: Participants filled out a pre-questionnaire that included general personal information, prior experience with computers, VR, and digital color theory, and a color vision test (Ishihara test).
    ◦ Knowledge Questions: Participants answered four multiple-choice questions about pixels, color models, color channels, and the RGB color model to assess their baseline knowledge.
    ◦ Task Execution: Participants performed a series of tasks in the VR environment, collaborating to solve a color-based problem.
    ◦ Post-Questionnaire: After completing the tasks, participants filled out a post-questionnaire that included standardized questionnaires (SAM, QCM, GEQ, PXI) to measure emotional responses, motivation, social presence, and learning experience.
    ◦ Role Swap: Participants swapped roles and repeated the experiment from the other perspective, followed by the same post-questionnaire.
    ◦ Final Feedback: Participants provided feedback on their preferences, potential use cases, and general comments on the system.
4. Task:
    ◦ The task involved collaborative problem-solving in a VR environment focused on the RGB color model.
    ◦ The VR user was placed in a virtual environment where they could interact with an RGB color cube to select colors.
    ◦ The handheld user viewed the virtual environment from a different perspective on a tablet and provided guidance to the VR user.
    ◦ The goal was to find the correct RGB values for a given color and apply those values to light up a pixel in the correct color.
    ◦ Both users had to communicate and collaborate to complete the task successfully.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ SAM (Self-Assessment Manikin): Measured emotional responses (valence, arousal, dominance) on a 9-point scale.
        ▪ QCM (Questionnaire on Current Motivation): Assessed motivation (fear of failure, interest, probability of success, challenge) on a 7-point Likert scale.
        ▪ GEQ (Game Experience Questionnaire): Measured social presence (psychological and behavioral involvement) on a 4-point Likert scale.
        ▪ PXI (Player Experience Inventory): Evaluated learning experience (e.g., curiosity, mastery, autonomy, immersion) on a 7-point Likert scale.
    ◦ Performance Metrics:
        ▪ Task Success Rate: Measured by the ability of participants to correctly identify and apply RGB values to light up the pixel.
        ▪ Knowledge Improvement: Assessed by comparing the number of correct answers to knowledge questions before and after the experiment.
    ◦ Behavioral Metrics:
        ▪ Collaboration and Communication: Observed through the interaction between the VR user and the handheld user during the task.
    ◦ Physiological Metrics:
        ▪ None explicitly mentioned in the study.
Summary:
The study involved 14 participants in an AB split-user design, where they collaborated in pairs to solve a color-based problem in a VR environment. The tasks required communication and collaboration between the VR user and the handheld user. The study collected data through questionnaires (SAM, QCM, GEQ, PXI) to measure emotional responses, motivation, social presence, and learning experience, as well as performance metrics (task success rate and knowledge improvement). The results showed significant improvements in knowledge and positive user experiences in both VR and handheld perspectives.","PXI,  Questionnaire on Current Motivation (QCM),  SAM (Self-Assessment Manikin),  Game Experience Questionnaire (GEQ)",,,Completion Time,"Questionnaires,  Performance"
ID018,An EEG-based Experiment on VR Sickness and Postural Instability While Walking in Virtual Environments,"Cortes  Carlos Alfredo Tirado,  Lin  Chin-Teng,  Do  Tien-Thong Nguyen,  Chen  Hsiang-Ting",,10.1109/VR55154.2023.00025,2023,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study examines how VR sickness in mobile VR environments affects postural stability and cognitive workload, using a combination of EEG, full-body motion capture, and questionnaires (including SSQ). It compares two participant groups—those with and without VR sickness—showing that while postural stability (as measured by gait) remained similar, users who experienced VR sickness showed significantly higher EEG indicators of mental workload, such as decreased alpha power and increased frontal theta activity. This supports the postural instability theory in mobile VR contexts and highlights the potential of EEG as a continuous, objective measure for assessing immersive discomfort in real-time.


the Between-Trial Questionnaire. At the
end of each trial, each participant reports, on a scale from 1 to 10,
their feelings of dizziness, discomfort, nausea, fatigue, headache,
and eyestrain.","Summary of the Experiments Conducted in the Paper
1) Participants
• Number: 21 healthy adults (17 males, 4 females).
• Age: Mean age = 25.73 years (SD = 3.6).
• Experience:
    ◦ 13 had experience with 3D computer games.
    ◦ 9 had prior VR experience.
    ◦ 15 had previous experience with motion sickness.
2) Study Design
• Between-group comparison: Participants were split into two groups based on post-experiment Simulator Sickness Questionnaire (SSQ) scores:
    ◦ VRS Group: Experienced severe VR sickness (TS score ≥ 18).
    ◦ NoVRS Group: Experienced minor or no VR sickness (TS score < 18).
• Independent Variable: Translation gain (TG) levels (1x, 2x, 4x, 6x, 8x, 10x), increased every 5 trials (a ""block"").
• Dependent Variables:
    ◦ EEG spectral power (delta, theta, alpha, beta, gamma bands).
    ◦ Biomechanical measures (Center of Mass displacement, gait parameters).
    ◦ Subjective ratings (discomfort, nausea, dizziness, etc.).
3) Procedure
1. Pre-Experiment:
    ◦ Participants completed a pre-experiment questionnaire (PEG) about motion sickness susceptibility and VR experience.
    ◦ Setup: Motion capture suit, 64-channel EEG headset, and Oculus CV1 VR headset were fitted.
2. Baseline Trials:
    ◦ Participants walked in the real world without VR to establish baseline EEG and biomechanical data.
3. VR Trials:
    ◦ Participants walked in a virtual environment with increasing TG levels (6 blocks × 5 trials each).
    ◦ After each trial, they rated symptoms (dizziness, nausea, etc.) on a 1–10 scale.
4. Post-Experiment:
    ◦ Full SSQ and semi-structured interviews were conducted.
4) Task
• Virtual Walking Task:
    ◦ Participants walked between two points (A to B and back) in a virtual city street.
    ◦ TG amplified virtual movement (e.g., 2x TG = virtual movement twice as fast as real walking).
    ◦ Task required maintaining postural stability while adapting to mismatched visual-vestibular inputs.
5) Metrics Collected
• EEG Metrics:
    ◦ Spectral power in delta, theta, alpha, beta, and gamma bands.
    ◦ Focused on midline electrodes (Fz, Cz, Pz, T8).
• Biomechanical Metrics:
    ◦ Center of Mass (CoM) displacement: Compared to baseline walking.
    ◦ Gait Parameters: Step distance, cadence, trial completion time.
• Subjective Metrics:
    ◦ Between-Trial Questionnaire: Rated symptoms (dizziness, nausea, etc.) after each trial.
    ◦ Post-Experiment SSQ: Quantified VR sickness severity (Nausea, Oculomotor, Disorientation subscales).
Key Findings
• EEG:
    ◦ VRS Group: Showed reduced alpha power (higher cognitive workload) and increased theta power (postural instability).
    ◦ NoVRS Group: Exhibited higher delta power (better cognitive-motor control).
• Behavioral:
    ◦ Significant CoM differences in VRS group at TG 2x and 10x.
    ◦ No differences in gait parameters (step distance, cadence).
• Subjective:
    ◦ VRS group reported higher discomfort and nausea, especially at higher TG levels.
Conclusion
The study demonstrated that VR sickness in mobile setups is linked to postural instability and increased cognitive effort, detectable via EEG and CoM metrics. The results support the postural instability theory and suggest EEG could complement behavioral measures for VR sickness detection.
Inclusion in Scoping Review:
• RQ1/RQ2/RQ3: Addresses metrics (EEG, biomechanical, subjective) for VR sickness and postural instability.
• User Metrics: Validates EEG as a physiological metric and questionnaires as subjective metrics.
• IC1/IC2: Provides generalizable insights into VR sickness and validates EEG-based detection.
• Exclusion Criteria: No violations (peer-reviewed, human-focused, broader relevance to XR).
Verdict: Include—aligns with review goals.","Per-trial symptom ratings,  SSQ (Simulator Sickness Questionnaire)",Body Analysis,"EEG,  Metrics - EEG (Delta Theta Alpha Beta Gamma","Steps / Cadence,  Completion Time","Questionnaires,  Behavioural,  Physiological,  Performance"
ID019,An Evaluation of Lower Facial Micro Expressions as an Implicit QoE Metric for an Augmented Reality Procedure Assistance Application,"Hynes  Eoghan,  Flynn  Ronan,  Lee  Brian,  Murray  Niall",,10.1109/ISSC49989.2020.9180173,2020,Irish Signals and Systems Conference (ISSY),AR,User states: Cognitive & Affective Experience,,Education and Training,"This paper investigates lower facial micro-expressions (MFEs) as implicit Quality of Experience (QoE) metrics for an Augmented Reality (AR) procedural assistance application. Using a controlled lab experiment, facial landmarks were tracked and analyzed across three task conditions (AR, paper instructions, and a hybrid interface) while participants assembled a physical object. The study found significant differences in facial muscle movements associated with lip corner depressor and chin raiser activity, particularly during AR interaction, suggesting that MFEs can reflect cognitive effort and emotional responses. The work provides evidence that facial expression tracking offers a viable non-intrusive metric for evaluating user experience in AR.","
1. Participants:
    ◦ 48 test subjects were recruited, with a gender-balanced sample (24 males and 24 females).
    ◦ Age range: 20 to 64 years old (mean age = 32, SD = 10 years).
    ◦ Participants were divided into two groups: AR group (24 participants) and paper-based control group (CG) (24 participants).
    ◦ No prior experience with solving a Rubik’s Cube was required.
2. Study Design:
    ◦ The study was a comparative experiment between two conditions:
        ▪ AR condition: Participants used an AR headset (Meta2) to receive step-by-step instructions for solving a Rubik’s Cube.
        ▪ Paper-based condition (CG): Participants used a 22-page printed instruction manual to solve the Rubik’s Cube.
    ◦ The study aimed to compare the Quality of Experience (QoE) between the two conditions using both explicit (questionnaires) and implicit (micro facial expressions) metrics.
3. Procedure:
    ◦ Phase 1 (Information Sharing): Participants were informed about the task and gave informed consent.
    ◦ Phase 2 (Screening): Participants were screened for visual acuity and spatial cognition. No participants were excluded.
    ◦ Phase 3 (Baseline): A 5-minute baseline recording of participants' facial expressions was conducted using the OpenFace application to establish baseline facial action units (AUs).
    ◦ Phase 4 (Training): Participants were trained on how to manipulate the Rubik’s Cube using the instructions provided in their respective condition (AR or paper-based).
    ◦ Phase 5 (Practice): Participants practiced solving the Rubik’s Cube to ensure they understood the instructions.
    ◦ Phase 6 (Testing): Participants solved the Rubik’s Cube using the instructions provided in their condition. Facial expressions were recorded throughout the task.
    ◦ Post-Task Questionnaires: After completing the task, participants filled out the Self-Assessment Manikin (SAM) questionnaire and a 5-point Likert scale questionnaire to report their affective state and subjective QoE.
4. Task:
    ◦ The task was to solve a Rubik’s Cube using either AR-based instructions (displayed in the AR headset) or paper-based instructions (printed manual).
    ◦ The Rubik’s Cube was set to the superflip position, which requires the maximum number of moves (20) to solve, ensuring standardized difficulty across participants.
    ◦ Participants in the AR group received step-by-step instructions in their field of view, while the control group turned pages in the manual to progress through the instructions.
5. Metrics Collected:
    ◦ Explicit Metrics:
        ▪ SAM Questionnaire: Measured affective state (valence, arousal, dominance) on a 9-point scale.
        ▪ Likert Scale Questionnaire: 14 statements rated on a 5-point scale to assess utility, interaction, aesthetics, usability, efficiency, and acceptability of the assistance medium.
    ◦ Implicit Metrics:
        ▪ Micro Facial Expressions: Recorded using OpenFace, which detected facial action units (AUs) associated with basic emotions (e.g., disgust, happiness, sadness, fear, surprise, and neutral). Micro expressions were defined as AUs lasting between 170ms and 500ms.
        ▪ Normal Facial Expressions: AUs lasting longer than 500ms were classified as normal expressions.
    ◦ Performance Metrics:
        ▪ Task Success Rate: Percentage of participants who successfully solved the Rubik’s Cube.
        ▪ Task Completion Time: Time taken to solve the Rubik’s Cube.
Summary:
The study involved 48 participants divided into two groups (AR and paper-based) to solve a Rubik’s Cube using either AR instructions or a printed manual. The experiment included baseline facial expression recording, training, practice, and the main task. Metrics collected included explicit (SAM and Likert questionnaires), implicit (micro and normal facial expressions), and performance metrics (task success rate and completion time). The study aimed to evaluate the relationship between micro facial expressions and QoE in AR compared to traditional paper-based instructions.","Likert Scale QoE,  SAM (Self-Assessment Manikin)",,Facial Expressions,,"Questionnaires,  Physiological"
ID020,An Integrated Framework for Understanding Multimodal Embodied Experiences in Interactive Virtual Reality,"Robert  Florent,  Wu  Hui-Yin,  Sassatelli  Lucile,  Ramanoël  Stephen,  Gros  Auriane,  Winckler  Marco",,10.1145/3573381.3596150,2023,Conference on Interactive Media Experiences,VR,Embodiment Avatars & Social Presence,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"This paper presents and validates a novel framework for studying multimodal embodied user experience in VR, grounded in Dourish’s theory of embodiment (ontology, intersubjectivity, intentionality). It combines task modeling, sensor-based behavioral logging, and physiological tracking (EDA, heart rate, gaze, motion capture) to analyze how users perform complex, real-world inspired tasks (e.g., crossing streets) under normal vs. low vision and real vs. simulated walking. Early findings show that multimodal synchronization enables deeper understanding of user stress, presence, and intention in VR. The work proposes an integrated toolkit and methodology that generalizes beyond the specific study, making it highly relevant for user metric development in XR.","1. Participants
• A total of 16 participants (14 men and 2 women) were recruited, aged between 18 and 34. Most participants had limited prior experience with VR, with three using it for the first time during the study https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=55649,56530 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=7988,8642.
2. Study Design
• The study employed a within-subjects design with four conditions:
    ◦ Movement Conditions: Real walking (RW) and simulated walking (SW).
    ◦ Vision Conditions: Normal vision (NV) and simulated low vision (LV) using a virtual scotoma https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=38288,39084, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=7988,8642.
• Each participant completed six scenarios under each of the four conditions, resulting in a total of 24 scenarios https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=49750,50533 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=36535,37432.
3. Procedure
• The study lasted approximately two hours, including
 setup and equipment fitting. Participants signed informed consent, 
completed a pre-study questionnaire, and were briefed on potential risks
 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=39085,39976 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=47248,48083.
• Each condition began with a pilot scenario to familiarize participants with the environment and tasks. After every three scenarios, participants performed a spatial perspective-taking test to assess their sense of presence https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=40774,41532 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=41533,42457.
• Post-condition questionnaires were administered after each condition, and a final post-study questionnaire was given at the end https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=41533,42457 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=38288,39084.
4. Task
• The tasks involved various interactions in a road crossing scenario, where participants had to navigate and interact with objects (e.g., picking up items, observing traffic lights) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=36535,37432, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=35632,36534.
• The tasks were designed to vary in cognitive load and interaction complexity, with scenarios ranging from simple object interactions to more complex tasks involving multiple interactions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=35632,36534 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=34062,34929.
5. Metrics Collected
• The study collected a variety of metrics, including:
    ◦ Physiological Metrics: Electrodermal activity (EDA) to assess emotional arousal.
    ◦ Behavioral Metrics: Motion capture data to analyze user interactions and navigation.
    ◦ Questionnaires: Pre- and post-condition questionnaires assessing cognitive load, presence, and emotional responses https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=43276,44044, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=38288,39084.
• The data was synchronized using Unix timestamps to facilitate comprehensive analysis of the embodied experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=21350,22282, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c967cf1ffdac3fb4bee735/?range=53882,54869.
This structured approach allowed for a detailed examination of user 
experiences in XR, particularly in the context of low vision navigation.","Presence – General,  SAM (Self-Assessment Manikin),  SSQ (Simulator Sickness Questionnaire),  Technology Acceptance","Head Analysis,  System-derived Interaction Metrics,  Gaze Analysis","EDA / GSR (Skin Conductance),  Heart Rate","Accuracy,  Completion Time","Questionnaires,  Behavioural,  Physiological,  Performance"
ID021,Analysis of Cybersickness through Biosignals: An Approach with Symbolic Machine Learning,"Nunes Da Silva  Wedrey,  Porcino  Thiago Malheiros,  Castanho  Carla Denise,  Jacobi  Ricardo Pezzuol",,10.1145/3691573.3691582,2024,Symposium on Virtual and Augmented Reality,VR,User states: Cognitive & Affective Experience,,Cybersickness Personalization,"This paper investigates the causes of cybersickness in VR by combining user-reported symptoms, game-interaction data, and biosignals within symbolic machine-learning models. Building on earlier work that relied solely on subjective measures, the authors integrate physiological signals—electrodermal activity, ECG-derived RR intervals, and accelerometer-based body movement—to examine how these indicators relate to discomfort levels reported during two VR games (a racing game and a flight simulation). Using Random Forest classifiers and a novel Potential Discomfort Indicator (PDI) ranking method, the study identifies the most influential factors contributing to cybersickness and shows that including biosignals significantly improves classification performance (AUC = 0.95). The results confirm known contributors such as exposure time, rotations, speed, and visual deficiencies, while also demonstrating the importance of EDA, ECG, and body-motion fluctuations in predicting cybersickness. The paper contributes both methodological advancements and empirical insights relevant to assessing and mitigating cybersickness in VR.","Participants: 17 total; data from ~13 valid participants after exclusions (car game: 8; flight game: 9).Procedure:
• 5-minute baseline physiological recording.
• VR exposure for 5 minutes (car racing or flight simulation).
• Real-time verbal reporting of cybersickness level (0–3).
• Completion of CSPQ (user profile) and VRSQ pre/post questionnaires.Collected data:
• Subjective: CSPQ, VRSQ-Pre, VRSQ-Post, real-time symptom verbal scores.
• Game data: speed, acceleration, rotations, region of interest, frame rate, FoV size, locomotion mode.
• Physiological data: ECG → RR intervals; EDA; accelerometer motion (ACC).Analysis: Data integrated per-second, preprocessed, and fed into symbolic ML classifiers (decision trees, Random Forest). Attribute-importance rankings produced using Gini-based feature evaluation.","Cybersickness Profile Questionnaire (CSPQ),  VRSQ (Virtual Reality Sickness Questionnaire)",Movement Trajectories,"ECG,  EDA / GSR (Skin Conductance)",,"Questionnaires,  Behavioural,  Physiological"
ID022,Assessing the User Experience of Consumer Haptic Devices for Simulation-based Virtual Reality,"Prattico  F. Gabriele,  Calandra  Davide,  Piviotti  Matteo,  Lamberti  Fabrizio",,10.1109/ICCE-Berlin53567.2021.9719998,2021,International Conference on Consumer Electronics,VR,Interaction Techniques & input Modalities,Content & System Design.,Human-Computer Interaction (HCI),"This study compares two consumer-grade haptic configurations in VR—(1) a force-feedback glove and (2) a vibrotactile glove with a passive haptic mock-up—used for a realistic screwdriving task. The evaluation covers task performance, usability, presence, and subjective user experience, using both questionnaires (SUS, UEQ, SIM-TLX, VRUSE, SSQ) and performance metrics (e.g., tip centering accuracy, number of slips, task times). Results show the passive haptic configuration was more usable and accurate, while the force-feedback glove enhanced sensory fidelity. The study provides a generalizable framework for comparing haptic VR interfaces and contributes to metric development for immersive tool-based interaction.","1. Participants
• The study included 15 volunteers aged between 24 and 55 years
 (mean age = 36.4, standard deviation = 11.9), selected from the staff 
of Leonardo. Participants were not suffering from color blindness https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=17152,17914 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=20915,21618.
2. Study Design
• The experiment followed a within-subject design, where each participant experienced both configurations of haptic devices (FFG and MK) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=28197,29162, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=27370,28196.
3. Procedure
• Participants first executed a procedure using a real electric 
screwdriver (ES) and three sets of real screws on both wood and aluminum
 to establish a common reference for haptic sensations.
• They then practiced interaction with various objects in a ""sandbox"" 
VR environment to familiarize themselves with the virtual environment 
and the haptic device.
• Finally, participants performed the screwing task in VR, alternating between the two configurations to mitigate biases https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=17915,18756, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=17152,17914.
4. Task
• The task involved a screwdriving activity where 
participants were required to relocate a bar with threaded holes and fix
 three equal-sized blocks of the same material (wood or aluminum) using 
screws of different lengths (30mm, 50mm, and 80mm) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=14268,14918 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=20215,20914.
5. Metrics Collected
• Objective Measures: 
    ◦ Accuracy of centering the screws’ tip
    ◦ Time spent manipulating objects
    ◦ Time spent in different phases of screw driving
    ◦ Number of slips during the task https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=26732,27369, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=26258,26732.
• User Feedback: 
    ◦ Collected through a multi-section questionnaire, including:
        ▪ Before Experience Questionnaire (BEQ): Demographics, previous knowledge, and the Simulator Sickness Questionnaire (pre-SSQ).
        ▪ After Experience Questionnaire (RESQ): Evaluated users' perception of haptic sensations.
        ▪ After Experience Questionnaire (AEQ): Scored both 
configurations on human-computer interaction (HCI), fidelity, and user 
experience aspects, including the System Usability Scale (SUS), User 
Experience Questionnaire (UEQ), and the Simulation Task Load Index 
(SIM-TLX) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=18757,19446, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c5810924bfec25f54bf628/?range=19447,20214.
This structured approach allowed the researchers to gather 
comprehensive data on both the performance metrics and subjective user 
experiences related to the different haptic configurations in XR.","SIM-TLX,  System Usability Scale (SUS),  UEQ (User Experience Questionnaire),  VRUSE",Haptic Response,,"Accuracy,  Completion Time","Questionnaires,  Behavioural,  Performance"
ID023,Assessment and Quantification of Virtual Reality Induced Sickness in Relation to Age and Gender: A Multi-Modal Approach,"Reddy  Abhiram,  Kim  Jin Ryong",,10.1109/URTC60662.2023.10534999,2024,IEEE MIT Undergraduate Research Technology Conference,VR,User states: Cognitive & Affective Experience,,"Entertainment and Gaming, Healthcare and Medical Training","This paper investigates how VR-induced sickness varies across age and gender, using a multimodal approach that combines physiological metrics (EDA, HRV) with a standardized self-report questionnaire (SSQ). Participants (N=54) were exposed to a 3-minute roller coaster VR experience, and changes in electrodermal activity were found to be significantly correlated with increased sickness symptoms, especially in females and older participants. The study demonstrates the value of integrating objective and subjective data for quantifying discomfort in immersive experiences, and suggests EDA is a more reliable predictor than HRV.","
1. Participants:
    ◦ The study recruited 54 participants from the general population.
    ◦ Participants were stratified by age and gender to ensure a representative sample.
    ◦ Four age groups were formed: 18-29, 30-39, 40-49, and 50-59.
    ◦ The sample consisted of an equal number of men and women.
2. Study Design:
    ◦ The study employed a mixed-methods approach, combining physiological measures and self-report measures to assess VR-induced sickness.
    ◦ The design was experimental,
 with participants exposed to a standardized VR experience, and 
physiological and subjective data collected before and after the 
exposure.
3. Procedure:
    ◦ Baseline Measurement: Upon arrival, participants were equipped with a Fitbit Sense 2 to record baseline physiological measures, including Heart Rate Variability (HRV) and Electrodermal Activity (EDA).
    ◦ VR Exposure: Participants were then exposed to a standardized 3-minute VR clip using the Meta Quest 2 VR headset. The VR clip featured a roller coaster ride from the Epic Roller Coasters application, chosen for its high potential to induce sensory conflict (a key factor in VR sickness).
    ◦ Post-Exposure Measurement: After the VR exposure, the same physiological parameters (HRV and EDA) were measured again using the Fitbit Sense 2.
    ◦ Questionnaire: Participants completed a Simulator Sickness Questionnaire (SSQ),
 a standardized tool used to assess symptoms of VR-induced sickness, 
such as dizziness, nausea, headache, eyestrain, and fatigue.
4. Task:
    ◦ The primary task for participants was to experience a 3-minute VR roller coaster ride.
 This task was chosen because it is known to create a strong sensory 
conflict between visual and vestibular signals, which is a common cause 
of VR-induced sickness.
    ◦ Participants were not required to
 perform any specific interactive tasks during the VR experience; they 
simply observed and experienced the roller coaster ride.
5. Metrics Collected:
    ◦ Physiological Metrics:
        ▪ Heart Rate Variability (HRV): Measured before and after VR exposure to assess changes in autonomic nervous system activity.
        ▪ Electrodermal Activity (EDA):
 Measured before and after VR exposure to assess changes in skin 
conductance, which is an indicator of physiological arousal or stress.
    ◦ Self-Report Metrics:
        ▪ Simulator Sickness Questionnaire (SSQ):
 A standardized questionnaire used to assess the severity of VR-induced 
sickness symptoms, including dizziness, nausea, headache, eyestrain, and
 fatigue.
    ◦ Demographic Data:
        ▪ Age and gender were recorded to analyze their impact on VR sickness susceptibility.
Summary:
The
 study involved exposing participants to a 3-minute VR roller coaster 
ride while collecting physiological data (HRV and EDA) and self-reported
 symptoms of VR sickness (via the SSQ). The goal was to understand how 
age and gender influence VR-induced sickness, using a combination of 
objective physiological measures and subjective self-reports. The 
results showed that females and older individuals were more susceptible to VR sickness, and changes in EDA were positively correlated with VR sickness scores.",SSQ (Simulator Sickness Questionnaire),,"EDA / GSR (Skin Conductance),  HRV / IBI",,"Questionnaires,  Physiological"
ID024,Attention Score: Objective Measure of Attentiveness in Immersive Omnidirectional Videos,"Bhanushali  Jay,  John  Achsah Steffi,  Muniyadi  Manivannan",,10.1109/AIVR56993.2022.00033,2022,International Conference on Artificial Intelligence and Virtual Reality (AIVR),VR,Behavioural Dynamics & Exploration,Metric Design & Validation.,Education and Training,"This study introduces two novel, objective behavioral metrics—Attention Score (ATNS) and Maximum Attention Span Score (MASS)—to quantify user attentiveness in immersive 360° video experiences. These metrics are computed from head orientation data relative to predefined Points of Interest (PoIs), requiring no additional eye-tracking hardware. A VR zoo application, ReptilesVR, was developed, and 25 participants evaluated six immersive animal encounters. Subjective engagement (presence, immersion, fear, curiosity) was measured via a custom Likert-scale questionnaire. Results showed strong correlations between objective and subjective metrics, particularly in scenarios with close proximity to animals, validating ATNS and MASS as lightweight, generalizable indicators of engagement in immersive VR.

Attention Score (ATNS) 
What it measures:
The average attentiveness of a user toward a predefined PoI during a VR scenario.
How it works:
It computes the dot product between the user’s view direction and the PoI direction for every frame, producing a scalar value from −1 (looking away) to 1 (looking directly at the PoI).

Maximum Attention Span Score (MASS)
• What it measures:
The longest continuous time span the user maintains high attention toward the PoI (above a threshold).
• How it works:
It identifies the longest streak of frames where attention is above a threshold (e.g., 0.5), and calculates its ratio over the total video duration.","1) Participants:
• Number: 25 participants
• Demographics: 18 males, 7 females; ages 19–32 (mean = 25.4 years, SD = 3.2)
• Inclusion criteria: Healthy individuals without phobias related to reptiles (herpetophobia or ophidiophobia), fully vaccinated (due to COVID-19 restrictions).
2) Study Design:
• Type: Mixed-methods (quantitative and qualitative)
• Approach: Within-subjects design — all participants experienced all six VR scenarios.
• Approval: Ethics approval from IITM-IEC (IEC/2022-01/MM/02/10).
3) Procedure:
• Participants familiarized themselves with VR for about 5 minutes with a non-experimental scenario.
• They then experienced six curated VR experiences based on reptile-related events.
• After completing the VR sessions, participants completed a subjective questionnaire.
• Objective attention metrics were collected during the VR experiences.
4) Task:
• Main Task: Experience six VR scenarios related to reptiles (e.g., crocodile feeding, venom extraction).
• Secondary Task: Maintain natural engagement (no forced attention), allowing attention to be measured organically.
• Post-Task: Fill out a detailed questionnaire rating presence, immersion, vicinity, fear, awareness, curiosity, and satisfaction on a 1–10 Likert scale.
5) Metrics Collected:
• Objective Metrics:
    ◦ Attention Score (ATNS): Average directional attention towards predefined Points of Interest (PoI).
    ◦ Maximum Attention Span Score (MASS): Longest continuous period of sustained attention above a threshold.
• Subjective Metrics (via Questionnaire):
    ◦ Presence: Feeling of ""being at the park"" vs. watching a video.
    ◦ Immersion: Absorption into the VR world.
    ◦ Vicinity: Perceived proximity to animals.
    ◦ Fear: Degree of fear induced by close encounters.
    ◦ Awareness: Awareness of conservation issues.
    ◦ Curiosity: Interest in learning more after the experience.
    ◦ Satisfaction: Comfort and absence of nausea.","Custom made,  Custom made (Presence Immersion Satisfaction Fear Curiosity Awareness Vicinity,  Likert scale 1-10)","Gaze Analysis,  Interaction Time",,,"Questionnaires,  Behavioural"
ID025,Augmenting Virtual Spatial UIs with Physics- and Direction-Based Visual Motion Cues to Non-Disruptively Mitigate Motion Sickness,"Qiu  Zhanyan,  McGill  Mark,  Pöhlmann  Katharina Margareta Theresa,  Brewster  Stephen Anthony",,10.1145/3677386.3682079,2024,Symposium on Spatial User Interaction,VR,Visualization Techniques,User states: Cognitive & Affective Experience.,,"This paper examines how different types of visual motion cues embedded within virtual spatial interfaces can mitigate motion sickness when VR is used on a moving platform. Through two controlled VR studies using a yaw-motion chair, the authors evaluate physics-based cues (swinging balls), direction-based arrow cues, and speed-and-direction cues that simulate full optic flow. Their results show that these visual cues can significantly reduce motion sickness without harming task performance, with speed-and-direction cues achieving the strongest mitigation but also causing the greatest distraction. In contrast, physics-based cues provide strong mitigation with substantially lower distraction, making them a more practical choice for UI designers seeking to preserve usability while supporting user comfort. The work offers generalizable insights into how VR interface elements can be designed to help users tolerate motion while maintaining cognitive performance and minimizing discomfort.","This paper examines how different types of visual motion cues embedded within virtual spatial interfaces can mitigate motion sickness when VR is used on a moving platform. Through two controlled VR studies using a yaw-motion chair, the authors evaluate physics-based cues (swinging balls), direction-based arrow cues, and speed-and-direction cues that simulate full optic flow. Their results show that these visual cues can significantly reduce motion sickness without harming task performance, with speed-and-direction cues achieving the strongest mitigation but also causing the greatest distraction. In contrast, physics-based cues provide strong mitigation with substantially lower distraction, making them a more practical choice for UI designers seeking to preserve usability while supporting user comfort. The work offers generalizable insights into how VR interface elements can be designed to help users tolerate motion while maintaining cognitive performance and minimizing discomfort.","Real-time Motion Sickness Slider,  SSQ (Simulator Sickness Questionnaire)",,,Accuracy,"Questionnaires,  Performance"
ID026,Avatars for Teleconsultation: Effects of Avatar Embodiment Techniques on User Perception in 3D Asymmetric Telepresence,"Yu  Kevin,  Gorbachev  Gleb,  Eck  Ulrich,  Pankratz  Frieder,  Navab  Nassir,  Roth  Daniel",,10.1109/TVCG.2021.3106480,2021,Transactions on Visualization and Computer Graphics,"AR, VR",Embodiment Avatars & Social Presence,,Telecommunications and Collaboration,"This study compares two avatar embodiment techniques—point cloud reconstruction (PCR) and 3D virtual characters (3DVC)—in an asymmetric VR/AR telepresence system. Conducted during collaborative tasks (a 20-questions game and puzzle-solving), the study finds that PCR avatars consistently outperform 3DVC avatars in terms of copresence, social presence, humanness, and behavioral impression, despite missing facial data. System usability was also higher for PCR, while no significant difference in task performance or motion sickness was found. The study provides strong, generalizable evidence on how different avatar representations affect social user experience in XR.","
1. Participants:
    ◦ Number of Participants: 24 participants (18 students, mainly from STEM fields).
    ◦ Demographics: 8 female, 16 male; average age = 23.83 years (SD = 2.31).
    ◦ Experience with XR:
 21 participants had used VR systems before, and 13 had used AR systems 
before. Most participants had limited prior experience with AR (average =
 1.04 times), while VR usage was more common (average = 6.04 times).
2. Study Design:
    ◦ Design: Two-factor (Avatar Type × Task Type) repeated-measures within-subjects experiment.
    ◦ Conditions:
        ▪ Avatar Type: Point-cloud reconstruction-based avatar (PCR) vs. 3D virtual character-based avatar (3DVC).
        ▪ Task Type: Verbal interaction task (20 Questions Game) vs. goal-directed collaborative task (Puzzle Solving).
    ◦ Roles: Each participant performed tasks in both roles (local user and remote user) across the two avatar conditions.
3. Procedure:
    ◦ Pre-Study: Participants completed demographic questionnaires, an Ishihara color-blindness test, and a Landolt-C vision acuity test.
    ◦ Familiarization: Participants were given up to 10 minutes to familiarize themselves with the AR/VR devices.
    ◦ Task Execution:
        ▪ Participants performed two tasks (20 Questions Game and Puzzle Solving) in both avatar conditions (PCR and 3DVC).
        ▪ After each task, participants completed questionnaires to assess their experience, presence, and perception of the avatar.
    ◦ Role Switching: After completing tasks in one role (e.g., local user), participants switched roles and repeated the tasks in the other role.
    ◦ COVID-19 Measures: Participants were placed in separate rooms, and equipment was disinfected between trials.
4. Tasks:
    ◦ 20 Questions Game (Verbal Interaction Task):
        ▪ Description:
 A verbal communication task where the local user thinks of an item, and
 the remote user asks up to 20 yes/no questions to guess the item.
        ▪ Purpose: To evaluate the impact of avatar representation on verbal communication and social interaction.
    ◦ Collaborative Puzzle Solving (Goal-Directed Task):
        ▪ Description:
 A puzzle task where participants arranged symbols and shapes in a 
specific order, orientation, and color. The remote user could draw 3D 
sketches in the air to guide the local user.
        ▪ Purpose: To assess the impact of avatar representation on collaborative, goal-oriented tasks.
5. Metrics Collected:
    ◦ Objective Performance Metrics:
        ▪ Task Completion Time: Time taken to complete the puzzle task (maximum of 8 minutes).
        ▪ Error Rates: Number of incorrect placements of puzzle pieces.
    ◦ Subjective Metrics (Questionnaires):
        ▪ Presence Measures:
            • Self-perceived co-presence, perceived other’s co-presence, telepresence, social presence, and self-location.
        ▪ Behavior Impression:
            • Naturalness, realism, and synchronicity of the avatar’s behavior.
        ▪ Humanness and Eeriness:
            • Perceived humanness and eeriness of the avatar.
        ▪ Visual Coherence:
            • How well the avatar fit with the environment and whether it disturbed the perception of environmental cues.
        ▪ System Usability Scale (SUS):
            • Usability of the system.
        ▪ Fast Motion Sickness Scale (FMSS):
            • Motion sickness experienced during the tasks.
    ◦ Behavioral Metrics:
        ▪ Body Movement Latency: Measured using a high-speed camera to assess the delay between real movements and their replication in the virtual environment.
    ◦ Additional Feedback:
        ▪ Participants provided open-ended feedback on positive and negative aspects of the avatar representations.
Summary:
The
 study involved 24 participants who performed two tasks (20 Questions 
Game and Puzzle Solving) in two avatar conditions (PCR and 3DVC). The 
study design was a repeated-measures within-subjects experiment, with 
participants switching roles between local and remote users. Metrics 
collected included objective performance measures (task completion time 
and error rates), subjective measures (presence, behavior impression, 
humanness, eeriness, and visual coherence), and behavioral metrics (body
 movement latency). The study provided insights into how different 
avatar representations impact user experience in XR, particularly in 
teleconsultation and collaborative tasks.","Custom made,  Fast Motion Sickness Scale,  Spatial Presence Experience Scale,  System Usability Scale (SUS),  Uncanniness and eeriness,  behavior impression,  self-location,  telepresence copresence and social presence",,,"Completion Time,  Task Success / Completion","Questionnaires,  Performance"
ID027,avoiding virtual characters: the effects of proximity and gesture,"Mousas  M. G. Nelson,  F.-C. Yang,  A. Koilias,  C.-N. Anagnostopoulos,  C.",,10.1109/ISMAR62088.2024.00018,2024,International Symposium on Mixed and Augmented Reality (ISMAR),VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,"Human–Virtual Character Interaction, Social Interaction","This paper investigates how proximity and gesture behavior of virtual characters affect user experience, avoidance movements, and social perception in immersive VR. Twenty-six participants performed a walking task, avoiding two virtual characters positioned at varying distances (close, middle, far) and performing either passive or active gestures. The study measured both subjective ratings (co-presence, attentional allocation, behavioral interdependence, emotional reactivity, perceived politeness) and objective behavioral metrics (trajectory, duration, speed, avoidance decisions). Results showed that proximity influenced co-presence, behavioral interdependence, and avoidance decisions—participants were less likely to walk between characters when close. Gestures affected behavioral interdependence, emotional reactivity, perceived politeness, and movement duration and trajectory: active gestures elicited stronger emotional responses, longer trajectories, and lower perceived politeness. The study highlights how nonverbal social cuessuch as spatial distance and body gesture shape behavioral and affective responses in social VR contexts.","1) Participants
• Number of Participants: 26 (15 male, 9 female, 1 nonbinary, 1 undisclosed).
• Age: 18–31 years (M = 21.5, SD = 3.51).
• Experience: All participants had previous VR experience; six had room-scale VR experience.
• Recruitment: University students; volunteers, no compensation.
• Health: No motor impairments reported; all had normal or corrected vision.
2) Study Design
• Type: Within-subjects 3 × 2 factorial design.
• Factors:
– Proximity: close, middle, far.
– Gesture: passive vs. active.
• Conditions: Six total combinations (3 proximities × 2 gesture types).
• Objective: Examine how proximity and gesture of two virtual characters affect users’ social perception and avoidance behavior.
3) Procedure
• Conducted in a replicated 3D model of a motion-capture lab (8×8×4 m).
• Participants performed walking tasks from start to target (7 m distance), avoiding two virtual characters placed at the midpoint.
• Each participant completed two trials per condition; order balanced via Latin square.
• Characters faced each other performing passive (idle) or active (argument-like) gestures.
• Participants completed a self-report survey after each condition and gave qualitative feedback at the end.
• Total session duration < 60 min.
4) Task
• Walk from start to target while avoiding two characters interacting mid-path.
• No explicit avoidance strategy given.
• Characters’ proximity adjusted: 0.46 m (close), 1.22 m (middle), 3.70 m (far).
• Gesture types:
– Passive: subtle idle motions.
– Active: expressive, high-amplitude argument gestures.
5) Apparatus
• VR Hardware: HTC VIVE Pro HMD with trackers and base stations.
• Software: Unity 2019.1.4; Mecanim animation engine for character control.
• Environment: Identical virtual replica of lab; two identical male avatars.
• Data capture: 100 Hz tracking for movement metrics.
6) Metrics Collected
• Questionnaires – Custom-made (based on validated scales):
◦ Co-presence (Biocca et al.)
◦ Attentional Allocation (Biocca et al.)
◦ Behavioral Interdependence (Biocca et al.)
◦ Emotional Reactivity (Mousas et al.)
◦ Perceived Politeness (Trivedi & Mousas)
• Behavioral Metrics:
◦ Movement Duration (s), Trajectory Length (m), Average Speed (m/s).
◦ Avoidance Decision (pass through vs. around; min. distance side).
• Qualitative Feedback: Post-study comments on realism, comfort, and perceived emotion.","Attentional Allocation,  Behavioral Interdependence,  Emotional Reactivity,  Perceived Politeness,  Presence – General","Movement Trajectories,  Avoidance Behaviour",,,"Questionnaires,  Behavioural"
ID028,Aw … The Museum Is so “Dark”: The Effect of Thermal Stimuli for Virtual Reality Experience and Emotion,"Widi Tamtama  Gabriel Indra,  Santoso  Halim Budi,  Wang  Jyun-Cheng,  Windasari  Nila Armelia",,10.1109/ICIC56845.2022.10006901,2022,International Conference on Informatics and Computing,VR,Content & System Design,User states: Cognitive & Affective Experience.,Tourism and Cultural Heritage,"This study investigates how thermal stimuli (warm, cold, neutral) affect emotional responses during a multisensory VR tourism experience, using a 360° virtual tour of a dark tourism site. It employs both subjective (valence, arousal, dominance via verbal scales) and physiological measures (EDA, HRV). Findings show that cold thermal stimuli led to significantly lower valence and arousal, while warm and neutral conditions induced more emotional engagement. The study also explores qualitative user reflections, revealing links between immersive stimuli, emotional reactions, and perceived realism. It contributes a methodologically rich, multimodal framework for evaluating emotional experience in immersive environments.","1. Participants
• Total Participants: 15 undergraduate students (67% male, 33% female).
• Age Range: Primarily aged 18-24 (95%).
• Location: Majority from Yogyakarta (60%).
• Exclusion: One participant who experienced 
headaches during the VR session was excluded from the final dataset, 
resulting in 14 valid responses https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=19920,20867 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=18326,19096.
2. Study Design
• Type: Between-subjects randomized experiment.
• Conditions: Participants were assigned to one of three thermal stimuli conditions: cooling, warming, or control (no thermal stimuli).
• Group Distribution: 6 participants received cooling stimuli, 4 received warming stimuli, and 5 received no thermal stimuli https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=14088,14661, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=22183,22832.
3. Procedure
• Setting: Conducted in a controlled laboratory environment using VR head-mounted displays (HMD) (Oculus Quest 2).
• Pre-experiment Preparation: Participants were instructed to avoid caffeine, alcohol, and smoking for three hours before the experiment.
• Experiment Duration: Each session lasted a maximum of 15 minutes https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=18326,19096, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=16049,16589.
• Thermal Stimuli Application: Thermal stimuli were applied to the thenar eminence of the left hand using a custom-built Peltier stimulator https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=16590,17427 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=13190,14087.
4. Task
• VR Experience: Participants explored the Museum 
Sisa Hartaku virtually, which is a dark tourism site showcasing the 
effects of a volcanic eruption.
• Navigation: Participants used a handheld controller to navigate through the VR environment, following a guided path https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=18326,19096 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=16590,17427.
5. Metrics Collected
• Self-Reported Measures: Participants rated their 
emotional responses using verbal self-report measures based on three 
dimensions: valence, arousal, and dominance https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=20868,21461, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=17430,18323.
• Physiological Metrics: Heart rate variability (HRV)
 and electrodermal activity (EDA) were recorded using the Empatica E4 
device to capture real-time emotional responses https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=17430,18323 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=23779,24602.
• Perceived Realism: Participants rated the perceived realism of visual and auditory stimuli using a 5-point Likert scale https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=14662,15466, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811e60881cfc53238042858/?range=29295,30203.
This structured approach allowed the researchers to assess the impact
 of thermal stimuli on user emotions and experiences in a virtual 
reality context, particularly in relation to dark tourism.","Self-report emotion measurement,  Custom made (Valence Arousal Dominance realism)",,"EDA / GSR (Skin Conductance),  HRV / IBI",,"Questionnaires,  Physiological"
ID029,Back to the Virtual Future: Presence in Cinematic Virtual Reality,"O Fearghail  Colm,  Gadipudi  Nivesh,  Young  Gareth William",,,2024,Conference on Interactive Media Experiences,VR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,cinematic VR,"This paper investigates how device type (HMD vs. traditional screen) influences presence and immersion during a cinematic virtual reality (CVR) storytelling experience. Using a within-subjects design, sixty participants viewed the 360° narrative film Faoladh (a Viking raid on an Irish village) either via an HTC Vive Pro HMD or a 4K monitor, and completed the Igroup Presence Questionnaire (IPQ) and adapted System Usability Scale (SUS) after each condition.
Results showed that HMD viewing significantly increased spatial presence, involvement, and overall sense of “being there” compared to the screen-based version (p < .001 for spatial presence and involvement). The “experienced realism” subscale showed no significant difference, suggesting that realism depends more on content quality than viewing medium. Qualitative thematic analysis identified three key experiential themes:
1. Engagement and immersion – participants described stronger emotional connection and embodiment through the HMD.
2. Technical influence – lag and low resolution reduced immersion, particularly for screen-based viewing.
3. Sensory and physical interaction – freedom of head movement and environmental awareness enhanced engagement.
The study concludes that spatial presence and involvement are the main psychological factors affected by the display medium, highlighting the importance of spatial design and narrative viewpoint in CVR storytelling.","1) Participants
• N = 60 (24 female, 35 male, 1 non-binary; M_age ≈ 23.6).
• Recruited via social media; screened for VR familiarity (mixed levels).
2) Study Design
• Within-subjects (each participant viewed Faoladh twice: on HMD and monitor).
• Counterbalanced order of presentation to avoid learning bias.
• Stimulus: Faoladh (360° live-action film, Google Jump camera).
• Duration: ~10 minutes per condition.
3) Apparatus
• HMD: HTC Vive Pro (90 Hz, 110° FOV, 2880×1600 px).
• Screen condition: 4K monitor (3840×2160 px) with mouse-based viewpoint control.
• Audio: Hi-Res stereo headphones.
4) Metrics Collected
Questionnaires:
• Igroup Presence Questionnaire (IPQ): Spatial Presence, Involvement, Experienced Realism, General Presence.
• Adapted System Usability Scale (SUS): simplified to assess usability of the viewing setup.
Qualitative Metrics:
• Open-ended responses analyzed via Thematic Analysis (engagement, technical influence, sensory/physical interaction).
Descriptive Data:
• Additional self-reported ratings of enjoyment and ease of use.

Quantitative: HMD yielded significantly higher scores on all presence factors except realism.Qualitative:
• HMD enhanced engagement and embodiment (“felt part of the story”).
• Technical quality affected immersion more in screen condition.
• Free head movement and spatial orientation in HMD created deeper presence.Interpretation: Spatial presence and involvement drive perceived immersion more strongly than realism or interactivity in narrative VR.","IPQ (Igroup Presence Questionnaire),  Presence – General,  Usability – Custom / Rating-based,  Semi-structured Interviews",,,,Questionnaires
ID030,"Behavioural Analysis in a 6-DoF VR System: Influence of Content, Quality and User Disposition","Rossi  Silvia,  Viola  Irene,  Cesar  Pablo",,10.1145/3552483.3556454,2022,Workshop on Interactive EXtended Reality,VR,Behavioural Dynamics & Exploration,Content & System Design.,Human-Computer Interaction (HCI),"This study presents a comprehensive behavioral analysis of user navigation patterns in 6-DoF VR environments, focusing on the influence of content dynamics, visual quality, and user disposition. Using a publicly available dataset of volumetric video viewed in VR, the authors assess user trajectories through traditional metrics (velocity, heatmaps)and advanced tools like actual entropy and viewing direction change. Key findings include that user disposition (i.e., personal navigation style) has a stronger impact on interaction behavior than content quality or dynamics. The paper also proposes using these behavioral patterns to define user profiles for future immersive system optimization.","1) Participants
• The study involved 27 users who participated in a visual quality assessment study in a 6-DoF VR environment https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=5950,6837, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=10603,11417.
2) Study Design
• The study utilized a within-subjects design, where 
each participant interacted with multiple stimuli (volumetric content) 
across different quality levels. The stimuli included four dynamic point
 cloud sequences, each distorted at four different bit rate levels using
 two compression algorithms https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=9343,9907, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=9908,10602.
3) Procedure
• Participants were asked to focus on a single volumetric content for 
the entire session and rate its visual quality before moving to the next
 content. They had the freedom to decide how long to display each visual
 content https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=9908,10602, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=21818,22733.
4) Task
• The primary task for participants was to navigate within the VR 
environment and assess the visual quality of the displayed volumetric 
content. They were required to rate the quality after viewing each 
sequence https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=9908,10602, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=21818,22733.
5) Metrics Collected
• The study collected various metrics to analyze user behavior, including:
    ◦ Heatmaps of user positions on the floor (XY plane).
    ◦ Distribution of viewing direction per user across different volumetric content.
    ◦ Relative distance that each user maintained from the displayed sequence.
    ◦ Exploratory velocity and total time spent in each quality session per content.
    ◦ Actual entropy of navigation trajectories to assess consistency in user behavior.
    ◦ Clustering analysis to identify patterns in user navigation across different content and quality https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=27687,28553, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6826f8f39cc0a1ef4dea4553/?range=26345,27087.",,"Entropy Analysis,  Clustering Analysis,  Gaze Analysis,  Movement Kinematics,  Movement Trajectories",,,Behavioural
ID031,Breaking The Experience: Effects of Questionnaires in VR User Studies,"Putze  Susanne,  Alexandrovsky  Dmitry,  Putze  Felix,  Höffner  Sebastian,  Smeddinck  Jan David,  Malaka  Rainer",,10.1145/3313831.3376144,2020,Conference on Human Factors in Computing Systems,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Human-Computer Interaction (HCI),"This paper investigates how in-VR questionnaires (INVRQ) compare to traditional out-of-VR questionnaires (OUTVRQ) in terms of breaks in presence (BIP) and their effects on user experience measurement validity. Using a mixed-methods study with 50 participants, the authors measured electrodermal activity (EDA), heart rate, and self-reported presence across two immersive fidelity conditions. Results showed that OUTVRQ caused significantly higher and longer BIPs, affecting physiological arousal and emotional continuity. INVRQs were less intrusive and led to more reliable, consistent self-report data, suggesting that in-situ survey delivery helps maintain immersive experience integrity.","1) Participants
• Total: 50 participants (originally 53; 3 excluded due to technical issues)
• Demographics: Mostly university students (8 female, mean age = 26.08, SD = 4.39)
• Handedness: 45 right-handed, 3 left-handed, 2 ambidextrous (chose right hand)
• Experience:
    ◦ Game experience: Mean = 2.76 (SD = 2.17) on a scale of 1–8
    ◦ VR experience: Mean = 6.48 (SD = 2.22) on a scale of 1–8
    ◦ 20 participants used vision correction
2) Study Design
• Mixed-methods design
• Two independent variables:
    1. Questionnaire Modality (Within-subjects)
        ▪ In-VR Questionnaire (INVRQ)
        ▪ Out-of-VR Questionnaire (OUTVRQ)
    2. Fidelity Level (Between-subjects)
        ▪ High-fidelity VR environment (HiFi)
        ▪ Low-fidelity VR environment (LoFi)
• Study flow:
    ◦ Within-subjects: Each participant experienced both questionnaire modalities in randomized order
    ◦ Between-subjects: Participants were assigned to either HiFi or LoFi VR conditions
3) Procedure
1. Preparation:
    ◦ Consent form, briefing
    ◦ Random assignment to HiFi or LoFi group
    ◦ Physiological sensor attachment (skin conductance, heart rate, respiration)
    ◦ Put on HMD (VR headset)
2. First Questionnaire Condition:
    ◦ Play tutorial round (60s)
    ◦ Blackout event (Break in Presence - BIP) introduced
    ◦ Play Game Round #1 (90s)
    ◦ Fill out PENS questionnaire (INVRQ or OUTVRQ)
    ◦ Play Game Round #2 (90s)
    ◦ Fill out PENS questionnaire (same modality)
    ◦ Play Game Round #3 (90s)
    ◦ Remove HMD, complete IPQ questionnaire (always out of VR)
3. Break (optional)
4. Second Questionnaire Condition: (Opposite modality from first condition)
    ◦ Repeat steps from first questionnaire condition
5. Final Questionnaire:
    ◦ Demographics, BIP ranking, usability feedback
    ◦ Participants ranked most disruptive interruptions (e.g., blackouts, leaving VR, answering questionnaires)
4) Task
• VR Shooting Game (implemented in Unity, using HTC Vive Pro)
• Objective:
    ◦ Protect three crystals from attacking drones
    ◦ Use a pistol (VR controller) to shoot drones (each drone requires two hits to be eliminated)
    ◦ Balancing mechanism ensures that players lose at the last second if they don’t eliminate drones
• Manipulation of Fidelity:
    ◦ HiFi: High-resolution textures, sound, physics, particle effects
    ◦ LoFi: Simple geometric shapes, no particle effects or sound
5) Metrics Collected
(A) Physiological Metrics
• Electrodermal Activity (EDA) (Skin Conductance) → Measures Break in Presence (BIP)
• Heart Rate (HR) and Heart Rate Variability (HRV) (Blood Volume Pressure sensor)
• Respiration Rate (RR)
(B) Self-Report Metrics
• PENS (Player Experience of Need Satisfaction) → Assesses presence, autonomy, competence, relatedness, and intuitive controls
• IPQ (igroup Presence Questionnaire) → Measures general presence, spatial presence, involvement, and realism
• Post-experiment questionnaire → Ranking of most disruptive interruptions, usability, demographics
(C) Performance Metrics
• Number of drones eliminated in each game round
• Time taken to complete tasks (INVRQ vs. OUTVRQ completion times)
Key Findings from the Experiment
• OUTVRQs induced a stronger Break in Presence (BIP) than INVRQs (higher EDA response).
• OUTVRQs took significantly longer to complete than INVRQs.
• BIPs lasted longer for OUTVRQs (over 45 seconds) compared to INVRQs (6–9 seconds).
• No significant differences in self-reported presence (IPQ) between HiFi and LoFi conditions.
• No significant effect of questionnaire modality on game performance (drone eliminations).
This study provides strong evidence that INVRQs reduce disruption compared to traditional OUTVRQs while maintaining reliable self-report measures.","Group Presence Questionnaire (IPQ),  Player Experience of Need Satisfaction,  Post Experiment Questionnaire",,"EDA / GSR (Skin Conductance),  HRV / IBI,  Respiration","Completion Time,  Task Success / Completion","Questionnaires,  Physiological,  Performance"
ID032,Cognitive Load Classification with a Stroop Task in Virtual Reality Based on Physiological Data,"Souchet  Alexis D.,  Diallo  Mamadou Lamarana,  Lourdeaux  Domitile",,10.1109/ISMAR55827.2022.00083,2022,International Symposium on Mixed and Augmented Reality (ISMAR),VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study examines how different VR conditions (control, stereoscopy, and dual-task) influence cognitive load (CL)using a Stroop task in VR, combined with eye-tracking (ET), electrodermal activity (EDA), and ECG data. A machine learning framework was developed to classify CL based on physiological signals, using NASA-TLX labels. The results show that dual-task conditions significantly increased CL (subjective and physiological), and Gradient Boosting outperformed other classifiers with 87% accuracy and F1-score. This paper contributes a validated, multimodal approach to measure and classify CL in immersive environments.","Summary of the Experiments in Souchet et al. (2022)
1) Participants
• Total: 92 participants (mean age = 25.5 ± 4.83 years).
• Groups: Randomly assigned to three conditions:
    ◦ Control (n=32): Stroop task only, no stereoscopy.
    ◦ Dual-Task (n=31): Stroop task + additional audio task (pressing a button when hearing ""red"").
    ◦ Stereoscopy (n=29): Stroop task with negative parallax (3D depth effect).
• Inclusion Criteria: Healthy volunteers (18–39 years), normal/corrected vision, no neurological/musculoskeletal disorders.
2) Study Design
• Between-subjects design (each participant in only one condition).
• Purpose: Compare cognitive load (CL) across conditions while controlling for VR-induced visual fatigue.
• Baseline & Task Phases:
    ◦ Baseline (Rest): 3-minute relaxation period in a virtual biophilic environment.
    ◦ Stroop Task: 25 congruent + 25 incongruent word trials.
    ◦ Post-Task: NASA-TLX questionnaire (subjective CL assessment).
3) Procedure
1. Pre-Experiment:
    ◦ Participants signed consent forms, completed profile surveys, and received standardized instructions.
    ◦ Practiced Stroop task on a physical board to ensure understanding.
2. VR Session (26 min total):
    ◦ Baseline (3 min): Relaxation in a virtual room with calming music.
    ◦ Stroop Task (~10 min): Performed in one of the three conditions.
    ◦ NASA-TLX (self-reported workload).
3. Post-Experiment: Debriefing and payment (€12).
4) Task (Stroop Task in VR)
• Description: Participants verbally named the font color of displayed words while ignoring the word’s meaning (e.g., ""RED"" written in blue ink).
• Conditions:
    ◦ Control: Standard Stroop task.
    ◦ Dual-Task: Stroop + secondary audio task (press trigger when hearing ""red"").
    ◦ Stereoscopy: Words displayed with 3D depth (negative parallax).
• Response Modality: Voice recognition (Microsoft Speech SDK).
5) Metrics CollectedCategoryMetricsPurposePhysiological- ECG (Heart Rate Variability, RMSSD, IQR of RR intervals)- EDA (Skin Conductance Response, SCL, SCR features)- Eye-Tracking (Pupil diameter, saccades, blinks)Measure autonomic and cognitive load responses.Behavioral- Stroop Accuracy (% correct answers)- Response Time (RT)- Stimuli Frequency (SF, time between trials)Assess task performance and cognitive effort.Subjective (NASA-TLX)- Mental demand, effort, frustration, etc.Self-reported workload assessment.Machine Learning- SVM, Logistic Regression, Gradient Boosting classifiersClassify CL levels from physiological data.
Key Findings
• NASA-TLX: Dual-Task induced significantly higher subjective CL than Control/Stereoscopy.
• Behavioral: Dual-Task led to slower RT and lower accuracy. Stereoscopy slightly improved performance.
• Physiological: ECG features (HRV, RMSSD) and eye-tracking (pupil dilation) were most predictive of CL.
• Best Classifier: Gradient Boosting achieved 87.23% accuracy in CL classification.
Conclusion
The study successfully used multimodal metrics (physiological, behavioral, subjective) to assess cognitive load in VR, demonstrating that machine learning can effectively classify CL levels. The findings have implications for XR user experience research, particularly in workload assessment and adaptive VR systems.",NASA-TLX,,"ECG,  EDA / GSR (Skin Conductance),  Eye-Tracking","Accuracy,  Response Time","Questionnaires,  Physiological,  Performance"
ID033,Cognitive Load Estimation Based on Pupillometry in Virtual Reality with Uncontrolled Scene Lighting,"Eckert  Marie,  Habets  Emanuel A. P.,  Rummukainen  Olli S.",,10.1109/QoMEX51781.2021.9465417,2021,International Conference on Quality of Multimedia Experience,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study proposes a method for estimating cognitive load in VR via pupil dilation—measured using the integrated eye tracker of the HTC Vive Pro Eye—under uncontrolled lighting conditions. A novel light compensation model was implemented to isolate task-evoked pupillary responses (TEPR) from light-induced pupil changes (PLR). Participants performed 1-, 2-, and 3-back tasks in VR under normal and corrupted lighting. Results confirmed that pupil size correlates with cognitive load and that, after brightness correction, lighting had no significant effect, enabling more robust and scalable pupillometry-based CL monitoring in immersive systems.","1. Participants
• Number of Participants: 14 (originally 16 were recruited, but only those not wearing glasses were included).
• Demographics: 5 females, age range 23 - 37 years (M = 28.4, SD = 4.5).
• Recruitment: Participants were recruited from the 
Fraunhofer Institute for Integrated Circuits (IIS) and all provided 
written informed consent https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=6085,6846 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=2708,3573.
2. Study Design
• Type: Within-subjects design.
• Independent Variables: 
    ◦ Task difficulty (1-back, 2-back, 3-back).
    ◦ Lighting conditions (normal and corrupted) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=7695,8498, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=13575,14218.
3. Procedure
• Participants completed an n-back task in a VR environment using the HTC Vive Pro Eye headset.
• The task involved identifying the color of balls presented in a 
spaceship-like VR environment and indicating matches by moving them into
 corresponding bins using a handheld controller.
• Each participant underwent a brightness calibration before each run,
 and they completed a NASA-TLX questionnaire after each experimental 
block https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=8499,9378, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=7695,8498.
4. Task
• Task Description: The n-back task required 
participants to indicate if the current color matched the one presented n
 balls before. The task was adapted for VR and included variations in 
difficulty (1-back, 2-back, 3-back) and lighting conditions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=6847,7694, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=7695,8498.
5. Metrics Collected
• Pupil Size: Measured using the integrated eye tracker of the HTC Vive Pro Eye headset.
• Cognitive Load: Estimated from pupil dilation data, with a method developed to correct for lighting-induced changes in pupil size.
• Self-Reported Cognitive Load: Collected using the NASA-TLX questionnaire after each task https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=18307,18754, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bdc6236d7e5948e1705980/?range=1806,2707.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, task, and metrics collected",NASA-TLX,,Pupil Analysis,Accuracy,"Questionnaires,  Physiological,  Performance"
ID034,Cognitive Load Measurement with Physiological Sensors in Virtual Reality during Physical Activity,"Ahmadi  Mohammad,  Michalka  Samantha W.,  Lenzoni  Sabrina,  Ahmadi Najafabadi  Marzieh,  Bai  Huidong,  Sumich  Alexander,  Wuensche  Burkhard,  Billinghurst  Mark",,10.1145/3611659.3615704,2023,ACM Symposium on Virtual Reality Software and Technology,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study evaluates the robustness of four physiological measures—pupil dilation (PD), electrodermal activity (EDA), heart rate (HR), and EEG—for detecting cognitive load (CL) in an interactive VR memory task involving physical movement. Participants performed a tile sequence recall game while standing and using arm gestures. Results showed that PD and EDA had strong, consistent correlations with increased CL, while EEG showed selective frequency band sensitivity (notably decreases in delta and alpha power). HR showed no consistent effects. The study confirms that PD, EDA, and EEG can be used to monitor CL in physically active VR tasks, extending the ecological validity of CL detection methods in immersive settings.","1) Participants
• Number of Participants: 19 (13 males and 6 females)
• Age Range: 18-30 years (Mean age = 23.5)
• Inclusion Criteria: Participants with good uncorrected vision and no prior eye surgeries.
• Compensation: Each participant received a $20 supermarket voucher https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=16954,17876, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=2504,3267.
2) Study Design
• Type of Study: Experimental study using a VR sequence memory game called ""PlayMeBack.""
• Conditions: Participants completed trials with varying cognitive load, represented by sequences of 3 to 6 tiles https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=17877,18665, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=26184,26826.
3) Procedure
• Duration: Approximately 80 minutes.
• Setup: Participants were equipped with an EEG cap 
and a head-mounted display (HMD). The tile positions in the VR 
environment were adjusted according to each participant's height, and 
the eye-tracking system was calibrated https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=20270,21056, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=18668,19458.
• Practice Trials: Participants practiced the game until they felt comfortable to proceed https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=18668,19458, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=16954,17876.
4) Task
• Game Description: Participants memorized and pressed a specific sequence of tiles in a VR environment. Each trial consisted of:
    ◦ Study Phase: Tiles were presented sequentially for 2000 ms each.
    ◦ Maintenance Phase: A 3-second period with no stimuli.
    ◦ Response Phase: Participants pressed the tiles in the order they were presented, typically completing this phase in 3-7 seconds https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=17877,18665, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=21926,22736.
• Total Trials: Each participant completed 80 trials, with 20 trials for each condition (3, 4, 5, or 6 tiles) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=17877,18665, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=26184,26826.
5) Metrics Collected
• Physiological Measurements:
    ◦ Pupil Dilation: Collected using integrated eye-tracking hardware with a sampling rate of 120 Hz.
    ◦ Skin Conductance Responses (SCR): Measured using Shimmer hardware, with a sampling rate of 256 Hz.
    ◦ Heart Rate (HR): Also collected using Shimmer hardware.
    ◦ EEG Data: Collected using an Enobio 20 gel cap with a sampling rate of 500 samples per second https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=20270,21056, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=22739,23513.
• Data Analysis: The study focused on analyzing the response phase data, with baseline corrections applied to physiological measurements https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=21926,22736, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6824aca69af4ebf70004e4e1/?range=21059,21925.",,,"EDA / GSR (Skin Conductance),  Heart Rate,  Pupil Analysis,  EEG",,Physiological
ID035,Common Cues? Toward the Relationship of Spatial Presence and the Sense of Embodiment,"Halbig  Andreas,  Latoschik  Marc Erich",,10.1109/ISMAR62088.2024.00128,2024,International Symposium on Mixed and Augmented Reality (ISMAR),VR,User states: Cognitive & Affective Experience,Embodiment Avatars & Social Presence.,Foundational Research / Theoretical Model Validation,"This paper explores the theoretical and empirical relationship between spatial presence and the sense of embodiment (SoE) in virtual reality. The authors propose a novel “Common Cues Perspective,” arguing that all cues influencing one of these two fundamental VR qualia (presence or embodiment) also affect the other. Across three empirical studies (N = 42, N = 42, N = 32), the paper systematically tested how traditional spatial presence cues (head-tracking, passive depth cues) and embodiment cues (visuotactile and visuoproprioceptive synchrony) influence both qualia. Results support the hypothesis that presence and embodiment share overlapping cue mechanisms: head-tracking increases both agency and ownership; passive depth cues affect spatial presence and trend toward influencing embodiment; and synchronous visuotactile/visuoproprioceptive feedback strongly enhances both presence and embodiment. The study advances conceptual understanding of cross-qualia dependencies and highlights implications for XR design and metric development.","1) Participants
• Study 1: 42 participants (21 per condition; 36 female; Age = 20.78, SD = 1.62).
• Study 2: 42 participants (20 RICH, 22 REDUCED; 36 female; age = 20.76, SD = 1.61).
• Study 3: 32 participants (29 female; age = 20.94, SD = 1.98).
• All participants were students with low prior VR experience (<5 h).
2) Study Design
• Study 1: One-factor between-subjects (head-tracking ACTIVE vs FIX).
• Study 2: One-factor between-subjects (RICH vs REDUCED depth cues).
• Study 3: 2×2 mixed design (SYNC vs ASYNC visuotactile/visuoproprioceptive synchrony × monoscopic vs stereoscopic display).
• Goal: To examine whether traditional spatial presence cues (head-tracking, depth cues) act as embodiment cues, and vice versa.
3) Procedure
• All participants provided informed consent and completed demographics and Fast Motion Sickness Scale (FMS).
• Study 1: Participants viewed an aquarium environment for 4 minutes with or without head-tracking.
• Study 2: Participants viewed environments differing in depth cues, then performed a cube ordering task by distance.
• Study 3: Participants underwent a virtual hand illusion (VHI) task using synchronous/asynchronous visuotactile and visuoproprioceptive stimulation.
• After each exposure, participants completed the Igroup Presence Questionnaire (IPQ), the Virtual Embodiment Questionnaire (VEQ), and the FMS.
4) Task
• Study 1: Passive viewing of the environment with head-tracking manipulation.
• Study 2: Depth perception and ordering task.
• Study 3: Virtual Hand Illusion task—participants experienced synchronized or delayed tactile and movement feedback.
5) Metrics Collected
• Questionnaires – IPQ (Igroup Presence Questionnaire): Spatial Presence Subscale.
• Questionnaires – VEQ (Virtual Embodiment Questionnaire): Body Ownership and Agency Subscales (adapted or original).
• Questionnaires – FMS (Fast Motion Sickness Scale): Cybersickness assessment.
• Behavioral/Performance Metrics: Task accuracy in depth ordering (Study 2).
• Qualitative Metrics: None (quantitative focus).","Fast Motion Sickness Scale,  IPQ (Igroup Presence Questionnaire),  VEQ",,,,Questionnaires
ID036,Comparative Analysis of Artefact Interaction and Manipulation Techniques in VR Museums: A Study of Performance and User Experience,"Wang  Yifan,  Li  Yue,  Liang  Hai-Ning",,10.1109/ISMAR59233.2023.00091,2023,International Symposium on Mixed and Augmented Reality (ISMAR),VR,Interaction Techniques & input Modalities,,Tourism and Cultural Heritage,"This study compares four interaction techniques—controller-based vs. hand-tracking, each with direct and indirect manipulation—for interacting with artefacts in a VR museum. The evaluation combines task performance metrics(completion time) and user experience questionnaires covering acceptance, learnability, presence, sickness, and fatigue. Results show that controller-based direct manipulation outperforms other methods in terms of speed, usability, and user preference. Hand-tracking with indirect manipulation scored lowest across most UX categories. The study provides generalizable insights into interaction design and user comfort in object manipulation tasks for immersive VR environments.","1) Participants
• The study involved 20 participants (12 males and 8 females) aged between 19 and 28 years (M = 22.6, SD = 2.24).
• 11 participants had prior experience with VR devices, while 9 did not.
• Participants reported moderate proficiency in controller-based 
interactions (M = 2.65, SD = 1.39) and slightly lower familiarity with 
hand-tracking interactions (M = 2.10, SD = 1.30) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=25178,25992, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=43652,44327.
2) Study Design
• A within-subjects experimental design was employed to measure performance and user experience with four different interaction techniques in a VR museum setting:
    ◦ Controller-based Direct (CD)
    ◦ Controller-based Indirect (CI)
    ◦ Hand-tracking Direct (HD)
    ◦ Hand-tracking Indirect (HI) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=18977,19892, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=43652,44327.
3) Procedure
• Participants provided informed consent and demographic information before the experiment.
• An instructional tutorial was given to familiarize participants with the four interaction techniques.
• The experiment consisted of four sessions, one for each interaction technique, with the order determined by a Latin Square Design to counterbalance order effects.
• Each session involved transform manipulation tasks where participants manipulated virtual objects to match target positions, orientations, and sizes.
• Participants completed five repetitive trials for each of the four artefacts, resulting in a total of 20 task trials per condition.
• Task completion time was recorded, and participants filled out a post-condition questionnaire after each session https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=21693,22602, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=28234,29095.
4) Task
• The main task involved transform manipulation, 
which included selecting, grabbing, and scaling virtual artefacts. 
Participants were required to manipulate the artefacts to match 
specified target positions, orientations, and sizes https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=19895,20665, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=16833,17674.
5) Metrics Collected
• Task Performance: Measured by the time taken to 
complete each trial. The maximum and minimum completion times for each 
artefact were excluded to calculate the average time of the remaining 
trials.
• User Experience: Collected through a 28-item questionnaire assessing various aspects such as:
    ◦ Acceptance
    ◦ Learnability
    ◦ Presence
    ◦ Sickness
    ◦ Fatigue
• The questionnaire items were rated on a 5-point Likert scale https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=22603,23561, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68121d367cb155bf8bc7ca15/?range=36809,37297.","Chalder Fatigue Scale,  Custom made,  Presence – General,  SSQ (Simulator Sickness Questionnaire),  System Usability Scale (SUS),  User Acceptance",Hand Movement,,"Completion Time,  Task Success / Completion","Questionnaires,  Behavioural,  Performance"
ID037,Comparative Evaluation of the EEG Performance Metrics and Player Ratings on the Virtual Reality Games,"Paranthaman  Pratheep Kumar,  Bajaj  Nikesh,  Solovey  Nicholas,  Jennings  David",,10.1109/CoG52621.2021.9619043,2021,Conference on Games,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study evaluates the reliability of EEG-derived emotion metrics (from the Emotiv EPOC X headset) during gameplay in three VR games (action, puzzle, and relaxation genres) by comparing them to player-reported ratings (PR). The authors show that EEG performance metrics (PM) often do not align with subjective player experience, particularly for emotional states like excitement, stress, and focus. They propose an ad-hoc linear model that maps raw EEG signals to self-reported PR data more accurately than the built-in Emotiv emotion classifiers. This highlights the need for transparent and validated physiological metrics in XR experience measurement.","1) Participants
• Total Number: 14 participants
• Demographics:
    ◦ All were undergraduate or graduate students
    ◦ Mean age: 21.57 years (SD = 2.17)
    ◦ Gender: 9 male, 5 female
• VR Experience:
    ◦ 5 had never used VR
    ◦ 2 had moderate experience
    ◦ 7 had rarely used VR
• Prior Game Experience:
    ◦ 2 had played Space Pirate Trainer, 1 had played The Room VR or Tripp
2) Study Design
• Design Type: Within-subjects experimental design
• Conditions: Each participant played three different VR games representing distinct gameplay intensities:
    ◦ High-action (Space Pirate Trainer)
    ◦ Moderate-action (The Room VR: A Dark Matter)
    ◦ Low-action (Tripp – relaxation/meditation game)
• Game Order: Randomized across participants to control for order effects
• Duration: ~5 minutes per game, with 10-minute breaks in between
• Total Session Time: ~60–100 minutes per participant
3) Procedure
1. Pre-experiment:
    ◦ Consent form and pre-test survey (demographics, game and VR experience)
2. Equipment setup:
    ◦ EEG headset (Emotiv EPOC X) fitted and calibrated
    ◦ VR headset (Oculus Quest) worn over EEG device
3. Gameplay:
    ◦ Participants played all three VR games in randomized order
    ◦ After each game:
        ▪ 10-minute rest break
        ▪ Completion of Player Ratings (PR) questionnaire
4. COVID Protocol: Pre-screening, on-site screening, and follow-up surveys for safety compliance
4) Task
• Play three VR games:
    ◦ Space Pirate Trainer: Action shooter requiring aiming, dodging, shooting
    ◦ The Room VR: Puzzle-solving with object manipulation
    ◦ Tripp: Guided meditation using gaze-based control
• Report subjective emotions after each game using a 6-item PR questionnaire (Likert scale)
5) Metrics Collected
a) Physiological Metrics (EEG):
• Raw EEG: 14 channels (128Hz), captured during gameplay
• Frequency Bands Analyzed: Theta (θ), Alpha (α), Low-beta (βL), High-beta (βH), Gamma (γ)
• Performance Metrics (PM): Provided by Emotiv Pro software every 10s, scoring 6 emotional states (0–1 scale):
    ◦ Engagement, Excitement, Stress, Relaxation, Focus, Interest
b) Questionnaires (Player Ratings – PR):
• Self-report ratings on the same six emotional states, matched to PM
• 5-point Likert scale per emotion (mapped to 0–1 scale for comparison)
c) Analytical Metrics:
• Statistical Comparisons: T-tests, Spearman's correlation between PM and PR
• Modeling: Linear regression models to estimate emotional states from raw EEG (validated against PR)",Custom made - one per emotion,,EEG,,"Questionnaires,  Physiological"
ID038,Comparing Continuous and Retrospective Emotion Ratings in Remote VR Study,"Warsinke  Maximilian,  Kojić  Tanja,  Vergari  Maurizio,  Spang  Robert,  Voigt-Antons  Jan-Niklas,  Möller  Sebastian",,10.1109/QoMEX61742.2024.10598301,2024,International Conference on Quality of Multimedia Experience,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Affective interaction,"This paper compares continuous emotional ratings (in-VR, during 360° video playback) and retrospective emotional ratings (post-video questionnaire) in a remote VR study conducted via Zoom with Meta Quest devices shipped to participants’ homes.
20 participants viewed 8 VR 360° videos (1 per quadrant of the circumplex model).
Emotion was measured as valence and arousal, using self-assessment manikin (SAM)–based interfaces:
• Continuous method: 2D grid overlaid on video (rating every 10 sec)
• Retrospective method: classic SAM Likert scale (after video)
Main findings:
• Valence showed significant differences between continuous and retrospective ratings (moderate–strong effect sizes).
• Arousal showed no significant differences between methods.
• Retrospective ratings correlated strongly with baseline valence/arousal from the annotated Stanford 360° database.
• Continuous ratings were more centered, retrospective more extreme → evidence of peak-end bias.
• Remote VR emotional elicitation is feasible but raises challenges (setup difficulty, attention, headset comfort).
Contribution:
A methodological comparison of two emotional-rating metrics in VR, highlighting bias, timing effects, and the feasibility of remote VR user studies.","Participants
• N = 20 (after excluding 1 incomplete dataset)
• Age 20–29
• Mixed VR experience levels
• Remote study: headset shipped to each participant
Design
• Within-subjects, counterbalanced
• Each participant rated 8 videos (1 per quadrant × 2 methods)
• Two rating methods: continuous vs retrospective
• Measures valence & arousal for each stimulus
• Online supervision via Zoom
Procedure
1. Delivery of headset + preinstalled app
2. Zoom session: calibration, SAM explanation
3. Practice phase
4. Main study: 8 randomized videos
    ◦ 4 continuous ratings
    ◦ 4 retrospective ratings
5. Final questionnaire on method preference
6. Debriefing
Stimuli
• 360° video clips from Stanford’s annotated public database
• Each clip 60s, taken from 4 affect quadrants:
    ◦ LVHA
    ◦ HVHA
    ◦ LVLA
    ◦ HVLA
Rating Interfaces
Retrospective SAM:
• Digitized Likert scale (1–9) with SAM pictograms
Continuous SAM:
• 2D grid (valence × arousal)
• Acoustic cue every 10 seconds
• 5 ratings per video, averaged to 1 per dimension
Metrics collected
Self-Report Metrics (Validated)
• SAM – Valence
• SAM – Arousal
→ Both standard, validated emotion self-report metrics.
Derived Metrics
• Difference between continuous vs retrospective
• Correlation with baseline annotated values
• Effect sizes (Cohen’s d, Wilcoxon r)
Qualitative Metrics
• Participant preference (retrospective vs continuous)
• Feedback on comfort, difficulty, realism, and distraction",SAM (Self-Assessment Manikin),,,,Questionnaires
ID039,Comparing Different Grasping Visualizations for Object Manipulation in VR Using Controllers,"Ganias  Giorgos,  Lougiakis  Christos,  Katifori  Akrivi,  Roussou  Maria,  Ioannidis  Yannis,  Ioannidis  Loannis Panagiotis",,10.1109/TVCG.2023.3247039,2023,Transactions on Visualization and Computer Graphics,VR,Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,Human-Computer Interaction (HCI),"This study compares three grasping visualizations for VR controller-based object manipulation: Auto-Pose (AP), Simple-Pose (SP), and Disappearing-Hand (DH). It evaluates their impact on task performance, embodiment (ownership and agency), and user preference. Results show no significant performance difference, but AP induced the highest sense of ownership and agency and was preferred overall. The study introduces a realistic pose-based grasping model and shows that visual realism in controller-based VR enhances user experience, even though controllers already simplify the interaction.","1) Participants
• A total of 38 participants were recruited, consisting of 22 women and 16 men.
• Participants had varying levels of experience with immersive VR: 
    ◦ 27 had little to no experience (0-3 times),
    ◦ 6 had moderate experience (4-10 times),
    ◦ 5 had considerable experience (more than 10 times).
• Most participants (except 3) were right-handed https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=32839,33655, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=48552,49049.
2) Study Design
• The study employed a within-subjects design, where each participant experienced all three grasping visualizations: 
    ◦ Auto-Pose (AP): Hand automatically adjusts to the object.
    ◦ Simple-Pose (SP): Hand closes fully when selecting the object.
    ◦ Disappearing-Hand (DH): Hand becomes invisible after selecting an object https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=21976,22966, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=21159,21975.
• The grasping visualizations were presented in a counterbalanced order using a Latin-square design to mitigate ordering effects https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=32839,33655, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=29447,30365.
3) Procedure
• Participants were welcomed, informed about VR technology and the experimental process, and asked to sign a consent form.
• After donning the VR equipment, participants completed three 
sessions of tasks, each corresponding to one of the grasping 
visualizations.
• Each session included a series of pick-and-place tasks, followed by a semi-structured interview https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=36195,36975, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=33656,34615.
• The entire procedure lasted approximately 60 minutes https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=33656,34615, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=35396,36194.
4) Task
• The main task involved selecting and positioning virtual objects on a table within a virtual office environment.
• Participants were instructed to be as accurate and fast as possible while completing the tasks https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=35396,36194, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=34618,35393.
• Each session consisted of 24 repetitions of object positioning, with obstacles introduced in some tasks to assess their impact on performance https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=32151,32838, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=43572,44344.
5) Metrics Collected
• Logged Data: 
    ◦ Task duration (selection, positioning, and total task duration).
    ◦ Position error (distance between the center of mass of the virtual object and the target) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=35396,36194, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=22967,23666.
• Questionnaires and Interviews: 
    ◦ Participants completed a series of statements on a 7-point Likert 
scale regarding their sense of embodiment, naturalness, comfort, and fun
 after each session https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=36976,37927, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=37928,38849.
    ◦ Semi-structured interviews were conducted both in-VR and out-of-VR to gather qualitative insights https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=36976,37927, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6819d276f3b4006cf3a26587/?range=37928,38849.","Custom made,  Embodiment Questionnaire,  Semi-structured Interviews",,,"Accuracy,  Completion Time","Questionnaires,  Performance"
ID040,"Comparing Gaze, Head and Controller Selection of Dynamically Revealed Targets in Head-Mounted Displays","Sidenmark  Ludwig,  Prummer  Franziska,  Newn  Joshua,  Gellersen  Hans",,10.1109/TVCG.2023.3320235,2023,Transactions on Visualization and Computer Graphics,VR,Interaction Techniques & input Modalities,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"This study compares gaze, head, and controller pointing modalities in VR for selecting dynamically revealed targets, addressing tasks that require target discovery beyond the field of view. The work provides strong empirical evidence that gaze and controller input are significantly faster and more accurate than head-based pointing, especially under conditions without prior target knowledge. It evaluates selection behavior using Fitts' Law, Peephole, and Magic Lensmodels and shows that two-stage pointing models fit best. It also uses NASA-TLX to assess perceived workload, finding gaze as the least physically demanding. The findings are generalizable to design of immersive interaction in HMD-based XR systems.","Summary of Experiments in the Paper
1) Participants
• Number: 24 participants (11 female, 13 male).
• Age: Mean age 24.21 ± 4.7 years.
• Experience:
    ◦ VR: 8 novices, 12 occasional users, 1 weekly, 2 daily users.
    ◦ Eye tracking: 11 novices, 11 occasional, 2 daily users.
2) Study Design
• Type: Within-subjects experiment.
• Independent Variables:
    ◦ Modality: Gaze, head, controller pointing.
    ◦ Screen Width (FOV): 40°, 70°, 100°.
    ◦ Target Amplitude (A): 30°, 60°, 120°.
    ◦ Target Width (W): 2°, 4°, 8°.
    ◦ Prior Knowledge (PK): Known vs. unknown target location.
• Counterbalancing: Modality order randomized via Latin square.
3) Procedure
1. Setup:
    ◦ VR System: HTC Vive Pro with Tobii eye tracker (120Hz).
    ◦ Input Devices: Vive controller for selection confirmation (all modalities).
    ◦ Visualization:
        ▪ Gaze/Head: On-screen cursor (1° diameter).
        ▪ Controller: Raycast from the controller.
2. Task Training:
    ◦ Short practice session per modality.
    ◦ 5-point eye-tracking calibration before each modality block.
3. Data Collection:
    ◦ Each participant completed 81 selection sequences per modality (243 total).
    ◦ NASA TLX questionnaire after each modality block.
4) Task
• Reciprocal 1D Pointing Task (horizontal plane, 2m radius):
    1. Start Target: Selected outside FOV (Figure 1A).
    2. First Target: Appeared opposite start position (unknown location, Figure 1B).
    3. Second Target: Returned to start position (known location, Figure 1C).
• Targets: Cylindrical objects; participants selected by button press.
• Error Handling: Missed selections triggered repetition of the sequence.
5) Metrics CollectedCategorySpecific MetricsPerformance Metrics- Movement Time (selection start to completion).- Error Rate (missed selections).- Reach Time (time to first hover over target).- Overshooting Time (duration cursor overshot target).Behavioral Metrics- Head Rotation (degrees moved during selection).- Trajectory Analysis (gaze/controller paths).Questionnaires- NASA TLX (workload: physical demand, performance).Model ValidationFit of Fitts’ Law, Peephole, Magic Lens, and Carpenter’s models to movement time data.
Key Findings
• Modality Performance:
    ◦ Fastest: Controller ≈ Gaze > Head.
    ◦ Most Accurate: Controller > Gaze > Head.
• FOV Impact: Performance improved from 40° to 70° but plateaued at 100°.
• Prior Knowledge: Reduced movement time (15–20%) and errors for all modalities.
• Model Fit: Peephole/Magic Lens models best predicted performance for dynamically revealed targets.
Limitations:
• 1D task design (horizontal only).
• Small sample size (24 participants).
Significance:
• Guides XR interface design for target selection and FOV optimization.
• Validates performance metrics for comparing input modalities in VR.",NASA-TLX,"Head Analysis,  Movement Trajectories",,"Error Rate,  Movement Time","Questionnaires,  Behavioural,  Performance"
ID041,Comparing Gaze-Supported Modalities with Empathic Mixed Reality Interfaces in Remote Collaboration,"Jing  Allison,  Gupta  Kunal,  McDade  Jeremy,  Lee  Gun A.,  Billinghurst  Mark",,10.1109/ISMAR55827.2022.00102,2022,International Symposium on Mixed and Augmented Reality (ISMAR),VR,Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,Telecommunications and Collaboration,"This study investigates how combining gaze-supported communication, hand pointing/gesturing, and real-time physiological biofeedback (heart rate) affects user experience, task coordination, and joint attention in remote Mixed Reality (MR) collaboration. Using a custom-built 360° MR system, the authors compared two interface types—Near-Gaze (NG) and Embodied (EM)—and two modalities—Gaze+Hand vs. Hand-Only. Results showed that Gaze+Hand in Near-Gaze led to significantly better co-presence, mutual understanding, and lower cognitive load(NASA-TLX). Gaze cues reduced reliance on hand pointing, while biofeedback was less useful in task-driven settings but could be valuable in emotionally or cognitively adaptive systems. The study offers a validated, multi-metric frameworkfor immersive collaboration analysis.","Summary of Experiments Conducted in the Paper
1) Participants
• Pilot Study: 8 participants (2 female), aged 19–32 years (Mean: 24.88, SD: 2.45).
    ◦ Identified as proficient AR/VR/MR users.
    ◦ Participants were friends or colleagues to ensure familiarity.
• Main Study: 24 participants (9 female), aged 19–35 years (Mean: 27.17, SD: 4.96).
    ◦ Mixed levels of experience with AR/VR, ranging from novice to expert.
    ◦ 66.7% were strangers; 33.3% were co-workers/familiar.
2) Study Design
• Within-subjects 2×2 factorial design:
    ◦ Independent Variables:
        1. Modality: Gaze+Hand (G+H) vs. Hand-Only (H-Only).
        2. Interface: Near-Gaze (NG) vs. Embodied (EM).
    ◦ 4 conditions tested:
        1. Gaze+Hand in Near-Gaze (G+H in NG)
        2. Gaze+Hand in Embodied (G+H in EM)
        3. Hand-Only in Near-Gaze (H-Only in NG)
        4. Hand-Only in Embodied (H-Only in EM) [Baseline]
    ◦ Counterbalanced order using a Balanced Latin Square to minimize bias.
3) Procedure
1. Pre-Experiment Setup
    ◦ Participants signed consent forms and completed a demographics questionnaire.
    ◦ Eye trackers and Zephyr BioHarness3 sensors were calibrated for gaze tracking and heart rate monitoring.
    ◦ Participants completed a three-level n-Back task (cognitive workload baseline).
2. Training & Familiarization
    ◦ Participants were given a brief introduction to the system.
    ◦ They practiced all four conditions in randomly assigned roles (local worker or remote expert).
3. Main Experiment
    ◦ Each participant performed all four conditions in both roles (local & remote).
    ◦ The study was conducted in two separate rooms, where participants could hear but not see each other.
    ◦ Task performance and interaction behaviors were video recorded.
4. Post-Condition Surveys & Interviews
    ◦ After each condition, participants completed a questionnaire assessing their experience.
    ◦ At the end, they ranked their preferred condition and participated in semi-structured interviews.
    ◦ The entire study lasted 1–1.5 hours per participant.
4) Task
• Collaborative puzzle-solving task:
    ◦ The local worker interacted with puzzle pieces in a Mixed Reality (MR) environment.
    ◦ The remote expert viewed a 360° panoramic video and guided the local worker.
    ◦ Step 1: Both participants had partial reference information and had to combine their knowledge to locate symbols.
    ◦ Step 2: The remote expert guided the local worker in placing symbols with correct orientation, position, and direction.
    ◦ The goal was to accurately complete the puzzle within a time limit.
5) Metrics Collected
Objective Metrics:
1. Completion Time – Time taken to complete each task.
2. Joint Attention Behavior:
    ◦ Joint Gaze (%) – % of time both users looked at the same target.
    ◦ Point-to-Signal – User pointed at their own gaze focus to signal a target.
    ◦ Point-to-Clarify – User pointed at the partner’s gaze focus to clarify intent.
Subjective Metrics:
1. Co-Presence Scale – From the Networked Minds Social Presence Measures.
2. User Ratings:
    ◦ Perceived Message Understanding
    ◦ Perceived Behavioral Interdependence
3. User Preferences:
    ◦ Ranked most to least preferred conditions.
4. NASA Task Load Index (NASA-TLX):
    ◦ Measures cognitive workload for each condition.
5. System Usability Scale (SUS):
    ◦ Evaluates overall system usability.
Qualitative Data:
1. Semi-Structured Interviews & Observations
    ◦ Explored user preferences, communication behaviors, and modality effectiveness.
Key Findings from the Experiment
• Gaze+Hand (G+H) interactions improved co-presence and task efficiency.
• Near-Gaze (NG) reduced cognitive load by preventing users from frequently shifting attention.
• Hand gestures were primarily used for confirmation, while gaze was the dominant method for joint attention.
• Heart rate biofeedback was not actively used, but participants saw potential value for empathic communication in different contexts.","NASA-TLX,  Social Presence,  Subjective Mental Effort Questionnaire,  System Usability Scale (SUS),  User Preference Rankings,  Single Ease Question (SEQ),  Semi-structured Interviews",Joint Attention Behaviour,Heart Rate,Completion Time,"Questionnaires,  Behavioural,  Physiological,  Performance"
ID042,Comparison of Different Types of Augmented Reality Visualizations for Instructions,"Jasche  Florian,  Hoffmann  Sven,  Ludwig  Thomas,  Wulf  Volker",,10.1145/3411764.3445724,2021,Conference on Human Factors in Computing Systems,AR,Visualization Techniques,User states: Cognitive & Affective Experience.,Education and Training,"This paper compares five types of instructional visualizations in an industrial Augmented Reality (AR) setting: paper-based, abstract AR (AAR), concrete AR (CAR), and both AR types with video (AAR+V, CAR+V). Using task completion time, error rate, cognitive load (NASA-TLX, RSME), and user experience questionnaires (UEQ, SUS), the study finds that while completion time was unaffected, CAR+V and AAR+V significantly reduced errors and cognitive load compared to AAR alone. Participants found videos particularly helpful in reducing uncertainty. The study contributes a nuanced understanding of how visualization detail and modality (AR + video) influence XR task performance and mental effort.","1. Participants
• A total of 48 participants were recruited (7 female and 41 male).
• Most participants (33) were undergraduate students from various 
disciplines (21 in industrial engineering, 5 in human-computer 
interaction, 6 in mechanical engineering, and 1 in business 
administration).
• The remaining 15 participants were professionals from different 
industries (e.g., banker, journalist, soldier, software developer).
• Participants were divided into five groups based on their jobs and prior knowledge of mechanical tasks https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=27644,28544 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=29441,30360.
2. Study Design
• The study employed a between-subject design to avoid learning effects that could influence the dependent variables.
• Participants were assigned to different visualization types for the 
same task, which included paper-based instructions, abstract AR (AAR), 
concrete AR (CAR), and combinations with videos (AAR+V, CAR+V) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=16136,16861, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=16862,17805.
3. Procedure
• Participants were informed about the study's goals and signed a consent form.
• Each participant's Microsoft HoloLens was calibrated before starting the task.
• Participants were trained on the application and gestures required for interaction.
• After completing the task, participants filled out several 
questionnaires (RSME, NASA-TLX, UEQ, SUS) and participated in a 
semi-structured interview https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=26825,27643, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=29441,30360.
4. Task
• The task involved equipping a Wafos RBV 35 bending machine with a set of bending tools, following a predefined sequence of eleven steps.
• Participants were instructed to mount the tool components according 
to the instructions provided, which varied by visualization type https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=22665,23540, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=21960,22664.
5. Metrics Collected
• Task Completion Time: Measured using an integrated logging function of the HoloLens applications.
• Error Rate: Recorded manually, with errors counted 
when the position or orientation did not match the specifications of an 
instruction step.
• Mental Workload: Assessed using the NASA-TLX and RSME questionnaires.
• Perceived Usability and User Experience: Evaluated through the SUS and UEQ questionnaires.
• Qualitative Data: Collected from video recordings of the task execution and audio recordings of follow-up discussions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=26825,27643, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cac3b1a4157fc31c2842f4/?range=25063,25915.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, task, and metrics collected.","NASA-TLX,  Rating Scale Mental Effort (RSME),  System Usability Scale (SUS),  UEQ (User Experience Questionnaire)",,,"Error Rate,  Completion Time","Questionnaires,  Performance"
ID043,Comparison of Physiological Cues for Cognitive Load Measures in VR,"Ahmadi  Mohammad,  Bai  Huidong,  Chatburn  Alex,  Najatabadi  Marzieh Ahmadi,  Wünsche  Burkhard C.,  Billinghurst  Mark",,10.1109/VRW58643.2023.00261,2023,Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Human-Computer Interaction (HCI),"This paper evaluates and compares four physiological signals—EEG (alpha band), galvanic skin response (GSR), pupil dilation, and heart rate (HR)—for their ability to detect cognitive load (CL) in an interactive VR task. Participants performed a memory-based tile sequence game in VR with increasing difficulty levels. Results showed that EEG alpha power (particularly in the frontal and central regions) and GSR (skin conductance) reliably reflected increased cognitive load. Pupil dilation and HR did not significantly correlate with task difficulty. The study introduces a short, moderately active, and ecologically valid task design that supports physiological monitoring for cognitive state assessment in immersive systems.","Summary of the Experiments in the Paper
1) Participants
• Number: 19 participants (13 male, 6 female).
• Age: Mean age 23.5 years.
• Compensation: Received a NZ$20 supermarket voucher.
• Ethics Approval: Approved by the University of Auckland Human Participants Ethics Committee (UAHPEC23449).
2) Study Design
• Type: Within-subjects experiment with four difficulty levels (3/4/5/6-tile sequences).
• VR Setup:
    ◦ HMD: HP Omnicept (with integrated eye-tracking).
    ◦ Hand Tracking: Leap Motion controller.
    ◦ Physiological Sensors:
        ▪ EEG: Enobio 20 gel cap (10/20 system).
        ▪ GSR & Heart Rate: Shimmer sensor (wrist-mounted).
• Task Structure: 80 trials total (20 per difficulty level).
3) Procedure
1. Preparation:
    ◦ EEG cap and GSR sensors were fitted.
    ◦ Participants familiarized themselves with the VR environment.
2. Trial Structure (Per Task):
    ◦ Study Phase: A sequence of 3–6 tiles lit up in a random pattern (2–5 sec).
    ◦ 3-Second Rest: Baseline physiological data collected.
    ◦ Encoding Phase: Participants replicated the sequence by pressing tiles.
3. Post-Experiment:
    ◦ Data cleaned (e.g., EEG artifacts removed).
    ◦ No questionnaires were administered (purely physiological/performance metrics).
4) Task
• Primary Task: ""PlayMeBack"" VR game.
    ◦ Goal: Memorize and replicate tile-lighting sequences of increasing length (3–6 tiles).
    ◦ Difficulty Manipulation: Longer sequences = higher cognitive load (CL).
    ◦ Interaction: Hand gestures (Leap Motion) to select tiles.
• Key Features:
    ◦ Short task duration (2–5 sec per trial) to minimize movement artifacts.
    ◦ Randomized order of sequence lengths.
5) Metrics CollectedMetric TypeSpecific MeasuresPurposePhysiological- EEG: Alpha band power (8–13 Hz) at frontal/central electrodes (F4, F8, C4, T8).- GSR: Skin conductance delta (change from baseline).- Pupil Dilation: Change from rest period.- Heart Rate: Average BPM per difficulty level.Measure cognitive load (CL) via neural/autonomic responses.Performance- Hit Rate: % of correctly replicated sequences.- Miss Rate: Incorrect attempts.Quantify task performance as a CL proxy.BehavioralHand movement data (Leap Motion) – recorded but not analyzed.Potential future use for CL correlation.
Key Findings
• EEG: Alpha band power (11 Hz) significantly decreased with higher CL (p < 0.05).
• GSR: Significantly increased with task difficulty (e.g., 3 vs. 6 tiles: p = 0.009).
• Pupil Dilation/Heart Rate: No significant CL correlation.
• Performance: Hit rates declined with difficulty (97% → 79%).
Conclusion: The study validated EEG and GSR as reliable CL metrics in VR, with implications for adaptive XR systems. The short-task design minimized movement artifacts, offering a methodological advance for physiological XR research.",,,"EDA / GSR (Skin Conductance),  Heart Rate,  Pupil Analysis,  EEG",Accuracy,"Physiological,  Performance"
ID044,Construction of the Virtual Embodiment Questionnaire (VEQ),"Roth  Daniel,  Latoschik  Marc Erich",,10.1109/TVCG.2020.3023603,2020,Transactions on Visualization and Computer Graphics,VR,Embodiment Avatars & Social Presence,Metric Design & Validation.,Human-Computer Interaction (HCI),"This paper introduces and validates the Virtual Embodiment Questionnaire (VEQ), a standardized and psychometrically validated tool to measure three components of embodiment in VR: ownership, agency, and change in body schema. The authors conducted three main studies (N=196) to develop the questionnaire and a fourth study (N=22) for validation, showing the tool’s reliability, construct validity, and sensitivity to manipulations such as latency and avatar realism. The VEQ is positioned as a foundational tool for future immersive system evaluations and can be used across diverse applications like therapy, social VR, and training.","In these instructions, users were asked to perform actions, meaning to move certain body parts and further focus on one’s own/the avatar body parts, as well as the body parts of their avatar presented in a virtual mirror, followed by a relaxing pose. Participants were tracked by an OptiTrack Flex 3 tracking system. A Unity3D simulation displayed the stimulus via a fake mirror projection or an Oculus Rift CV1 HMD","IPQ (Igroup Presence Questionnaire),  Self-presence,  Sense of Ownership,  VEQ",,,,Questionnaires
ID045,CoplayingVR: Understanding User Experience in Shared Control in Virtual Reality,"Zhou  Hongyu,  Ayesh  Treshan,  Fan  Chenyu,  Sarsenbayeva  Zhanna,  Withana  Anusha",,10.1145/3678508,2024,Interact. Mob. Wearable Ubiquitous Technol.,VR,Interaction Techniques & input Modalities,,"Entertainment and Gaming, Telecommunications and Collaboration","The paper investigates how shared control of a single avatar by two users (each controlling one virtual hand) affects user experience, performance, and agency/body ownership in VR. The authors developed CoplayingVR, a networked VR system allowing collaborative control of avatar limbs, and conducted a 48-participant within-subjects experimentacross three VR game tasks (control / balancing / shooting). Results show that shared control increases positive affect, immersion, and challenge, improves novice users’ performance, and leads to increased sense of agency and body ownership over time, though fast-paced tasks reduce perceived competence. Findings provide design guidelines for VR shared control systems.","The authors conducted a within-subjects study with 48 participants divided into novice, experienced, and mixed-experience pairs. Each dyad performed three VR game tasks (peg-in-hole, balancing maze, archery) under single-playerand shared control conditions, counterbalanced. Participants completed training, then played all tasks in both modes. Quantitative metrics included GEQ scores, task completion times, error rates, and body-ownership/agency scores administered before and after the session. Behavioural video observation captured hand coordination patterns. Semi-structured individual and pair interviews followed the tasks.","Embodiment Questionnaire,  Game Experience Questionnaire (GEQ),  Semi-structured Interviews","Hand Movement,  Head Analysis",,"Error Rate,  Completion Time","Questionnaires,  Behavioural,  Performance"
ID046,Correlation between Entropy and Prediction Error in VR Head Motion Trajectories,"Rossi  Silvia,  Toni  Laura,  Cesar  Pablo",,10.1145/3607546.3616805,2023,Workshop on Interactive EXtended Reality,VR,Behavioural Dynamics & Exploration,Metric Design & Validation.,"Adaptive Streaming and Personalization, Human-Computer Interaction (HCI)","The study focuses on VR head motion trajectories and investigates how entropy can be used as a metric to assess the predictability of user navigation behaviors in immersive environments. Entropy is analyzed as an information-theoretic metric to determine whether users with higher entropy exhibit less predictable movement patterns, which can affect adaptive streaming, content delivery, and personalization in VR. Findings indicate that users with highly regular movement styles (low entropy) have more predictable navigation trajectories, while users with diverse exploration behaviors (high entropy) are harder to predict. This has implications for adaptive VR streaming, personalized interactions, and system efficiency.","Existing head motion trajectory datasets from prior VR studies were used for analysis.

1_MMsys2017_1 [ 7 ]: this dataset collects navigation trajectories of 57 users who navigated within 5 sequences of the same duration (70 seconds);

• 2_MMsys2017_2 [ 17 ]: this collection is composed of 10 videos with a length of 60 seconds. Originally, the trajectories of 50 participants have been collected but only half of them have been used in the further experiments of prediction presented in [12] and in [23];

• 3_MMsys2017_3 [37 ]: the original dataset is composed of 18 videos displayed by 48 users. However, the first set of content has been experienced in a free-navigation experiment to identify the natural behaviour, while the second one is more specific to capture the user’s attention on the video content with structured questionnaires. In our study, we consider only the first group of data;

• 4_CVPR2018 [ 39 ]: this collection has the highest number of VR content, i.e., 208, characterised by a variable length ranging from 20 to 60 seconds (36 seconds on average). However, each video have been displayed only by 34 participants (at least 31 per each video);

• 5_MMSys2018 [ 10 ]: a collection of 57 navigation trajectories is presented for 19 different content. These videos, however, are short with a fixed duration of only 20 seconds,

• 6_PAMI2018 [38]: this dataset is composed of head-motion trajectories collected by 58 viewers in 76 different VR content. In this case, the videos have a variable length (i.e., between 10 and 80 seconds, on average of 25 seconds),

• 7_MM2022 [ 15]: the most recent VR collection is characterised by the highest number of participants (i.e., 100) who displayed 27 different 360◦ videos, each of 60 seconds.",,"Head Analysis,  Entropy Analysis",,,Behavioural
ID047,Crossing Rays: Evaluation of Bimanual Mid-air Selection Techniques in an Immersive Environment,"Kim  DongHoon,  Han  Dongyun,  Bak  Siyeon,  Cho  Isaac",,10.1109/ISMAR62088.2024.00047,2024,International Symposium on Mixed and Augmented Reality (ISMAR),VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Navigation & Selection Techniques,"This paper investigates four bimanual mid-air selection techniques—Simple-Ray, Simple-Stripe, Precision-Stripe, and Cursor-Sync—and compares them to a unimanual baseline to understand how visual aids and support features affect selection performance and user experience in VR. Through two tasks with and without a reference object, the authors analyze how users select spatial positions at different distances (3 m, 6 m, 9 m). Using quantitative measures of selection time and accuracy, as well as post-condition NASA-TLX and UEQ responses, the study finds that Simple-Ray and Simple-Stripe offer the fastest performance, while Precision-Stripe and Cursor-Sync provide higher accuracy and lower workload in specific conditions. Cursor-Sync and Precision-Stripe also deliver improved perceived attractiveness, efficiency, and dependability, especially when a reference object is present. The paper provides generalizable insights into how visual cues and control strategies shape selection precision, cognitive load, and subjective usability in VR mid-air interaction.","Participants
• 20 participants, ages 18–22
• Mostly VR-experienced
• All had normal or corrected vision
Procedure
• Two tasks:
    1. Task 1 (With reference): select a 3D point 0.5 m in front of a box
    2. Task 2 (No reference): remember and select a position of a briefly shown sphere
• Distances tested: 3 m, 6 m, 9 m
• Techniques: 4 bimanual (Simple-Ray, Simple-Stripe, Precision-Stripe, Cursor-Sync) + 1 unimanual baseline
• Each participant performed 180 trials (2 tasks × 5 techniques × 3 distances × 6 repetitions)
• After every technique × task combination → NASA-TLX and UEQ questionnaires completed
Metrics Collected
• Selection time
• Error distance (cm)
• NASA-TLX (mental, physical, temporal demand, effort, performance, frustration)
• UEQ (Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, Novelty)","NASA-TLX,  UEQ (User Experience Questionnaire)",,,Error Rate,"Questionnaires,  Performance"
ID048,"Cybersickness, Cognition, & Motor Skills: The Effects of Music, Gender, and Gaming Experience","Kourtesis  Panagiotis,  Linnell  Josie,  Argelaguet  Ferran,  MacPherson  Sarah E.",,10.1109/TVCG.2023.3247062,2023,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,,Entertainment and Gaming,"This study investigates the effects of cybersickness on cognition, motor skills, pupil size, and reading ability in immersive VR. It also examines whether music (calming vs. joyful) can mitigate cybersickness, and how individual differences (e.g., gender, gaming experience) influence symptom severity. The authors use a VR-based rollercoaster simulation with integrated eye-tracking, customized VR cognitive tasks, and a validated cybersickness questionnaire (CSQ-VR). Key findings include: (1) joyful music significantly reduces overall cybersickness, (2) cybersickness impairs verbal working memory and slows reaction times, and (3) pupil size is a strong physiological marker of cybersickness. Notably, gender effects disappeared when controlling for gaming experience."," Experiment 1: Online Music Selection Study
1) Participants
• N = 92 (51 males, 39 females, 2 non-binary)
• Recruited via social media
2) Study Design
• Within-subjects design
• Participants rated and ranked 12 instrumental music tracks (6 calming, 6 joyful)
3) Procedure
• Participants listened to 30-second clips of each track
• Rated each track on a 7-point Likert scale (1 = not at all calming/joyful, 7 = extremely)
• Then ranked the six tracks in each category by preference
4) Task
• Listen to clips and complete rating and ranking tasks for calming and joyful tracks
5) Metrics Collected
• Subjective ratings (calmness/joyfulness)
• Rankings of preference
• Statistical analyses (Wilcoxon tests) to select most suitable tracks for VR experiment

Experiment 2: Immersive VR Study
1) Participants
• N = 39 (22 females, 17 males; ages 22–36)
• Pre-screened using Motion Sickness Susceptibility Questionnaire (MSSQ); excluded top 25% most susceptible
2) Study Design
• Within-subjects repeated measures design
• Each participant completed:
    ◦ 1 baseline assessment
    ◦ 3 VR ""rides"" with different music conditions: Joyful, Calming, No Music
    ◦ 3 post-ride assessments (1 after each ride)
• Order of music conditions counterbalanced
3) Procedure
• Baseline in VR: rating music preference, cognitive and psychomotor tasks, cybersickness questionnaire
• 3 VR rides (∼5 mins each) with varying music type
• After each ride: same assessment battery as baseline
• Total session time: ~100 minutes
• Equipment: HTC Vive Pro Eye headset (with eye tracking), Unity3D, SteamVR, and other plugins
4) Tasks
Participants performed during baseline and after each ride:
• Cybersickness Questionnaire (CSQ-VR) (6 items)
• Verbal Working Memory: VR Backward Digit Span Task (BDST)
• Visuospatial Working Memory: VR Backward Corsi Block Test (BCBT)
• Psychomotor Task: VR version of Deary–Liewald Reaction Time task (Simple and Choice RT)
5) Metrics Collected
• Questionnaire data:
    ◦ CSQ-VR scores (nausea, oculomotor, disorientation subscores + total)
• Performance metrics:
    ◦ Verbal working memory (BDST total score)
    ◦ Visuospatial working memory (BCBT score)
    ◦ Reaction Time (RT, attentional time, motor time)
• Behavioral metrics:
    ◦ Eye tracking: reading time (fixation duration), pupil size
• Demographics:
    ◦ Age, gender, gaming/VR/computing experience (self-rated skill + frequency)","MSSQ,  Game Experience Questionnaire (GEQ)",Reaction Time,Eye-Tracking,,"Questionnaires,  Behavioural,  Physiological"
ID049,Decoding Realism of Virtual Objects: Exploring Behavioral and Ocular Reactions to Inaccurate Interaction Feedback,"Terfurth  Leonie,  Gramann  Klaus,  Gehrke  Lukas",,10.1145/3660345,2024,ACM Trans. Comput.-Hum. Interact.,VR,Behavioural Dynamics & Exploration,Metric Design & Validation.,"Foundational Research / Theoretical Model Validation, Interaction Quality","This paper investigates how behavioral and ocular signals can serve as implicit indicators of interaction realism during object manipulation in VR. Fourteen participants performed repeated precision grasping tasks while feedback timing and modality were experimentally varied—visual-only vs. visuo-haptic feedback, and realistic (synchronous) vs. unrealistic (premature or glitch) conditions. The authors analyzed motion, gaze, and pupil dilation data to identify features that distinguish realistic from unrealistic interactions. Results showed that interaction difficulties (“glitch” trials) significantly increased grasping duration, fixation time on the hand, and altered gaze–action timing. Specifically, unrealistic feedback led to longer fixations on the avatar hand and delayed gaze shifts, indicating increased uncertainty. Pupil dilation was not sensitive to feedback incongruence. These findings demonstrate that eye- and motion-based behavioral features can objectively index the perceived realism of VR interactions, contributing to continuous user experience evaluation without disrupting immersion.","1) Participants
• Number of Participants: 14 (9 female, 5 male).
• Age: Mean = 28.71 years (range 23–39).
• Experience: Most participants had little or no prior VR or haptic interface experience.
• Recruitment: University participants, compensated €12/hour or course credit.
• Screening: All right-handed, normal or corrected-to-normal vision (contact lenses allowed).
2) Study Design
• Type: Within-subjects experimental design.
• Factors:
– Feedback Congruity: match vs. mismatch (premature sensory feedback).
– Feedback Modality: visual-only vs. visuo-haptic.
– Technological Performance: non-glitch vs. glitch (unexpected technical grasping failure).
• Objective: To identify behavioral and ocular markers of unrealistic feedback during grasping interactions in VR.
3) Apparatus
• VR Setup: HTC VIVE Pro Eye headset (Tobii eye-tracker, 120 Hz sampling).
• Haptic Device: SenseGlove Nova, delivering vibro-tactile and force feedback on thumb and index finger.
• Motion Tracking: Vive Tracker attached to glove; data captured via LSL (LabStreamingLayer).
• Environment: Unity3D virtual tabletop scene with precision grasping task.
• Audio: White noise masking through in-ear headphones.
4) Task
• Participants repeatedly grasped and placed virtual spheres at target positions using a pincer grip.
• Manipulations:
– Feedback Congruity: 75% matched (synchronous feedback) vs. 25% mismatch (premature).
– Feedback Modality: visual-only (color change) vs. visuo-haptic (vibration + force).
• Glitch Trials: Post-hoc classification of trials with repeated grasp attempts due to technical malfunction.
• Goal: Place the object accurately and quickly; placement accuracy displayed after each trial.
5) Procedure
• Participants underwent setup, device calibration, and training blocks to familiarize themselves with the glove and VR interface.
• Two training (“embodiment”) blocks of 15 trials each (one per haptic modality).
• Four main experimental blocks of 76 trials each, alternating modality conditions (visual-only ↔ visuo-haptic).
• Breaks between blocks; presence questionnaire (Multidimensional Presence Scale – Physical and Self subscales) administered twice during the session.
6) Metrics Collected
• Behavioral Metrics:
◦ Motion durations (grasp, transport, placement).
◦ Placement accuracy (Euclidean distance to target).
• Eye-Tracking Metrics:
◦ CFH – Accumulated Fixation on Hand (duration & %).
◦ EAL – Eye Arrival Latency (time gaze arrives on object).
◦ ELL – Eye Leaving Latency (time gaze departs after grasp).
• Physiological Metrics:
◦ Pupil Diameter (PD mismatch / PD grab – 0.5–2.5 s windows post-event).
• Questionnaires:
◦ Presence – Multidimensional Presence Scale (Physical and Self-presence subscales).
• Derived Metric Goal: Identify gaze/motion features that distinguish realistic vs. unrealistic feedback in real time.",Presence – General,"Movement Trajectories,  Interaction Time,  Gaze Analysis,  eye gaze - Accumulated Fixation on Hand",Pupil Analysis,,"Questionnaires,  Behavioural,  Physiological"
ID050,Delay Threshold for Social Interaction in Volumetric eXtended Reality Communication,"Cortés  Carlos,  Viola  Irene,  Gutiérrez  Jesús,  Jansen  Jack,  Subramanyam  Shishir,  Alexiou  Evangelos,  Pérez  Pablo,  García  Narciso,  César  Pablo",,10.1145/3651164,2024,Transactions on Multimedia Computing Communications and Applications,"VR, social VR",Content & System Design,Embodiment Avatars & Social Presence.,Telecommunications and Collaboration,"This paper presents the first systematic investigation of interaction delay thresholds in a photorealistic volumetric Social XR videoconferencing system. Using the VR2Gather platform, pairs of participants collaboratively assembled block-based shapes while experiencing controlled end-to-end delays of 300, 600, 900, 1,200, and 1,500 ms. The study collected subjective data—overall quality, delay perception, presence, system annoyance, interruptions, and social presence—as well as objective measures such as task-completion time and conversational behaviour extracted from audio recordings. The results show that delays above 900 ms significantly degrade perceived quality, increase interruption effort, reduce presence, and negatively impact social interaction, especially for the builder role. Although objective performance degradation is limited due to user adaptation, conversational patterns show increased intervention frequency at high delays. The paper also provides a standardized evaluation protocol for Social XR interaction assessments.","Participants
• 60 participants (29 female, 31 male), aged 20–33
• All non-experts in VR
• Paired into dyads, assigned roles: instructor and builder
Task & Procedure
• Adapted ITU-T P.920 collaborative block-building task
• Each pair completed five tasks, each with a different delay condition
• Training session (300 ms and 1,500 ms conditions) + 10-minute break
• Roles placed in separate rooms, communicated via photorealistic volumetric capture
• After each figure, participants filled out subjective questionnaires inside VR
Experimental Conditions
• End-to-end delays: 300, 600, 900, 1,200, 1,500 ms
• Five block-based figures (see Figure 5 on page 11): TRex, Bird, Dog, Rocket, Mazinger
• Balanced and randomized using a Graeco-Latin design
Metrics Collected
Subjective Metrics (validated & standardized):
• Global QoE (ITU-T standards)
• System annoyance, interruption effort (ITU P.1305)
• Delay perception
• Presence: involvement, adaptation, accomplishment (validated in [12, 30])
• Social presence, social adaptation, collaboration (Gupta et al., 2016)
Objective/Behavioural Metrics:
• Task completion time (Unity logs)
• Speaking activity:
    ◦ Voice activity detection (200 ms windows)
    ◦ Number of interventions
    ◦ Role-based activity time
• Total conversation duration patterns
• Interaction difficulty inferred from conversation dynamics","Presence – General,  Social Factors,  Subjective performance",Communication Analysis,,Completion Time,"Questionnaires,  Behavioural,  Performance"
ID051,Demographic and Behavioral Correlates of Cybersickness: A Large Lab-in-the-Field Study of 837 Participants,"Luong  Tiffany,  Plechata  Adela,  Mobus  Max,  Atchapero  Michael,  Bohm  Robert,  Makransky  Guido,  Holz  Christian",,10.1109/ISMAR55827.2022.00046,2022,International Symposium on Mixed and Augmented Reality (ISMAR),VR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"This large-scale, field-based VR study (N = 837) explores how demographic, user experience, and behavioral variables relate to cybersickness during a narrated VR simulation. The researchers used a custom cybersickness question, along with measures of presence, social presence, body ownership, and HMD movement metrics (e.g., head rotation, navigation distance). Key findings include: (1) female participants and those with lower VR experience report more cybersickness; (2) low head movement and low body ownership are linked to greater discomfort; and (3) exploratory behavior (more head rotation and longer distances) correlates with lower cybersickness, suggesting an adaptive behavioral response. A predictive model achieved 67.1% accuracy in classifying cybersickness levels.

Questionnaires – Custom made: cybersickness (1–5 scale), Presence, Social Presence, Body Ownership (7-point scale, adapted from validated tools)","1. Participants
• Total Sample: N = 837 (after exclusions)
• Age Range: 18 to 80 years (M = 29.34, SD = 9.50)
• Gender Distribution: 431 males (51.5%), 400 females (47.8%), 6 non-binary/other (0.7%)
• VR Experience: Most were novices (median = “Never used VR”)
2. Study Design
• Type: Large-scale, lab-in-the-field experimental study
• Setting: Conducted at the National Museum of Berlin over 16 days
• Design: Between-subjects with 3 versions of the VR environment (differing in gamification/empathy content), but version type did not affect main cybersickness outcomes
• Assignment: Randomized to one VR version
• Pre-registration: Yes (November 9, 2021)
3. Procedure
1. Participants gave informed consent and completed a pre-treatment questionnaire (demographics, VR experience).
2. They were randomly assigned to one VR version and completed the experience (avg. 11.7 minutes).
3. Afterward, they completed a post-treatment questionnaire (cybersickness, presence, etc.).
4. Participants received a small gift and a voucher.
4. Task
• Participants embodied an elderly, gender-matched avatar at a virtual wedding.
• Tasks included:
    ◦ Navigating a ballroom
    ◦ Signing a guest book
    ◦ Placing a gift
    ◦ Talking to the bride
    ◦ Avoiding contact with other wedding guests (who moved on predefined paths)
• Interaction: Using Oculus Quest 2 controllers and headset (joystick locomotion, object interaction via hand collisions)
5. Metrics Collected
a) Self-Reported Measures
• Cybersickness: 1 (not at all) to 5 (extremely) based on motion sickness (nausea, dizziness)
• Presence & Social Presence: 1 to 5 Likert scale
• Body Ownership: 1 to 7 Likert scale
b) Demographic Data
• Gender, age, prior VR experience, language preference
c) Behavioral Data (automatically logged)
• Time spent in VR (seconds)
• Distance traveled in VR (meters)
• HMD movement (meters)
• HMD angular movement (degrees)",custom - cybersickness presence body ownership,"Total Distance Travelled,  Head Analysis,  Interaction Time",,,"Questionnaires,  Behavioural"
ID052,"Designing Visuo-Haptic Illusions with Proxies in Virtual Reality: Exploration of Grasp, Movement Trajectory and Object Mass","Feick  Martin,  Regitz  Kora Persephone,  Tang  Anthony,  Krüger  Antonio",,10.1145/3491102.3517671,2022,Conference on Human Factors in Computing Systems,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This paper investigates how three factors—grasp type, object mass, and movement trajectory—affect the perceptual thresholds for visuo-haptic illusions using proxies in VR. Through two controlled experiments (N=48), the authors show that grasping type and object mass (≤500g) do not significantly influence the threshold at which visual-proprioceptive mismatches are detected. However, restricted movements (e.g., guided by sliders) allow for significantly greater undetected visual offsets compared to unrestricted ones. The study proposes initial design guidelines for incorporating visuo-haptic illusions in VR systems and highlights the need for personalized thresholds based on users’ proprioceptive sensitivity and VR experience.


Conservative Detection Thresholds (CDT) [26]: Quantitative measure of perceptual thresholds for detecting discrepancies in visuo-haptic illusions.","1) Participants
• Study 1: 24 right-handed participants (11 females, 
13 males), aged 20-36 (mean: 26.42; SD: 3.65). Participants had diverse 
educational and professional backgrounds, including fields like media 
informatics, computer science, and artificial intelligence. They 
reported varying levels of VR experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=30612,31398, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=43241,44067.
• Study 2: A new set of 24 right-handed participants 
(9 females, 15 males), aged 20-37 (mean: 26.70; SD: 5.01). Similar to 
Study 1, participants had diverse backgrounds and reported varying 
levels of VR experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=30612,31398, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=43241,44067.
2) Study Design
• Both studies utilized a within-subjects design with an adaptive psychophysical 1-up-1-down interleaved staircase procedure. 
• Study 1 focused on the effects of grasping type and movement trajectory, while Study 2 examined the effects of grasping type and object mass https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=29900,30611, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=41760,42510.
3) Procedure
• Participants were seated and wore a head-mounted display (HMD) with 
their dominant hand tracked. They manipulated a physical proxy object to
 match a target position in a virtual environment.
• After reaching the target position, they answered a forced-choice question regarding whether they noticed a manipulation https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=26935,27774, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=35598,36458.
• Each study included a warm-up phase to familiarize participants with the task and the effects of manipulation https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=35598,36458, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=42513,43240.
4) Task
• Participants were instructed to manipulate proxy objects using 
different grasping types (lateral, medium wrap, tripod, writing tripod) 
and movement trajectories (linear and circular in Study 1; unrestricted 
in Study 2). The goal was to match the position of the proxy to a target
 in the virtual environment https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=27777,28676, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=34769,35597.
5) Metrics Collected
• The primary metric was the Conservative Detection Threshold (CDT), which was assessed through the number of trials participants took to reach convergence in the staircase procedure.
• Additional data included subjective responses to forced-choice 
questions, system logs (trial times, object positions, velocities), 
field notes, and post-study Simulator Sickness Questionnaire (SSQ) 
responses https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=36459,37310, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821d1632cfd3f05ee9486d8/?range=38175,38856.",SSQ (Simulator Sickness Questionnaire),,,Classification Outcomes,"Questionnaires,  Performance"
ID053,Development and Validation of the Collision Anxiety Questionnaire for VR Applications,"Ring  Patrizia,  Tietenberg  Julius,  Emmerich  Katharina,  Masuch  Maic",,10.1145/3613904.3642408,2024,Conference on Human Factors in Computing Systems,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Human-Computer Interaction (HCI),"This paper presents the development and validation of the Collision Anxiety Questionnaire (CAQ), a self-report tool designed to measure anxiety related to real-world collisions while using VR. Based on two studies (N=97 and N=62), the authors conducted exploratory and confirmatory factor analyses, resulting in a 10-item, 3-subscale questionnaire covering General Collision Anxiety, Orientation, and Interpersonal Collision Anxiety. The CAQ demonstrated good internal reliability, construct validity, and significant correlations with NASA-TLX workload dimensions. Findings highlight that collision anxiety can impair presence, increase workload, and reduce movement freedom—especially among inexperienced users and in multi-user VR settings.

The CAQ uses a 5-point Likert scale to assess how fear and discomfort from potential real-world collisions can interfere with user immersion and overall experience in VR. It comprises three subscales: (1) General Collision Anxiety (worry and avoidance behavior regarding collisions with objects and room boundaries), (2) Orientation (sense of spatial awareness in the real-world environment while wearing the headset), and (3) Interpersonal Collision Anxiety (fear of and distraction from potential collisions with other people present in the physical space).","1) Participants
• First Study (Exploratory Factor Analysis):
    ◦ Total: 103 recruited, 98 included after data cleaning
    ◦ Demographics: 33 women, 64 men, 1 non-binary
    ◦ Mean age: 28.9 years (SD = 12.0)
    ◦ VR experience: Varied; lab participants had less experience than online ones
    ◦ Recruitment: Online (Reddit, Facebook, Discord) and in a university lab
    ◦ Diversity: Participants from Germany, the USA, and 13 other countries
• Second Study (Confirmatory Factor Analysis):
    ◦ Total: 62 participants
    ◦ Demographics: 54.84% female, 45.16% male
    ◦ Mean age: 25.69 years (SD = 8.97)
    ◦ VR experience: Most had little or no prior experience
    ◦ Recruitment: University students, participation for course credit
2) Study Design
• First Study: Combined online and lab-based; exploratory factor analysis (EFA)
• Second Study: Lab-based, confirmatory factor analysis (CFA), randomized between two conditions (co-located vs. remote two-player VR)
3) Procedure
• First Study:
    ◦ Participants played a VR game (e.g., Superhot or a self-selected game) for at least 5 minutes
    ◦ Completed the CAQ 1.0 afterward
    ◦ Provided optional open feedback
• Second Study:
    ◦ Participants played a custom-designed two-player VR escape game (co-located or remote)
    ◦ Completed CAQ 2.0 and NASA-TLX questionnaire post-game
    ◦ Included attention checks and feedback queries
4) Task
• First Study:
    ◦ Engage with a VR game/application of their choice or the provided game (Superhot)
    ◦ Afterwards, complete the Collision Anxiety Questionnaire (CAQ)
• Second Study:
    ◦ Solve puzzles in a custom two-player VR escape room
    ◦ Movement encouraged through room-scale play
    ◦ Post-task questionnaires (CAQ + NASA TLX)
5) Metrics Collected
• Quantitative:
    ◦ Collision Anxiety Questionnaire (CAQ) responses (Likert scale)
    ◦ Demographics and VR experience data
    ◦ NASA Task Load Index (TLX) in Study 2 (mental demand, effort, frustration, etc.)
• Qualitative:
    ◦ Open feedback on the questionnaire and personal experiences with VR and collision anxiety
• Other:
    ◦ Attention check questions
    ◦ Questions about actual collisions during VR use (obstacles, people, walls)","NASA-TLX,  Open Ended Feedback",,,,Questionnaires
ID054,Differences in Heritage Tourism Experience between VR and AR: A Comparative Experimental Study Based on Presence and Authenticity,"Guo  Yuqing,  Lu  Shizhu,  Shen  Min,  Huang  Wei,  Yi  Xiaolie,  Zhang  Jifa",,10.1145/3648001,2024,Journal of Computing and Cultural Heritage,"AR, VR",User states: Cognitive & Affective Experience,Interaction Techniques & input Modalities.,Tourism and Cultural Heritage,"This study compares the impact of VR and AR technologies on the user experience of remote heritage tourism using the Stimulus–Organism–Response (S-O-R) model. It finds that presence and perceived authenticity are shaped differently by vividness and interactivity depending on whether the user is in VR or AR. Specifically, vividness significantly affects presence in both VR and AR, but interactivity only affects presence in VR. Perceived authenticity has a strong positive influence on satisfaction and behavioral intentions, particularly in the AR setting. The results underscore that VR and AR generate different psychological responses even with similar content, offering practical guidance for heritage site designers and immersive content developers.

Questionnaires - multi-items:
• Vividness – Adapted from Steuer’s media richness constructs
• Interactivity – Measured via interaction depth and control
• Presence – Adapted from Witmer & Singer’s Presence Questionnaire
• Perceived Authenticity – Likert-scale ratings based on content realism and credibility
• Satisfaction – Subjective enjoyment and overall evaluation
• Behavioral Intention – Likelihood to revisit or recommend the experience","1) Participants
• Total: 254 (121 VR, 133 AR).
• Demographics:
    ◦ Age: 24 ± 2.6 years.
    ◦ No prior visits to the heritage site.
    ◦ No motion sickness or VR/AR experience required.
• Recruitment: Chinese university students; ethical approval obtained.
2) Study Design
• Design: Between-subjects (VR vs. AR).
• Independent Variables:
    ◦ Technology type (VR: HTC Vive; AR: HoloLens2).
    ◦ Stimuli: Vividness, interactivity.
• Dependent Variables:
    ◦ Organism: Presence, perceived authenticity.
    ◦ Response: Satisfaction, behavioral intention.
3) Procedure
• Pre-Experiment:
    ◦ Training on VR/AR device use.
    ◦ Historical background briefing.
• Experimental Phase:
    ◦ VR Group: 5-min HMD exploration + flag-sewing task.
    ◦ AR Group: 5-min HoloLens2 exploration + same task.
• Post-Experiment: Questionnaires (PQ, authenticity, satisfaction).
4) Task
• Virtual Heritage Exploration:
    ◦ Navigate a revolutionary heritage site.
    ◦ Perform a culturally symbolic task (sewing a flag).
5) Metrics Collected
• Subjective (7-point Likert):
    ◦ Vividness: ""This experience was clear/detailed.""
    ◦ Interactivity: ""I controlled my navigation.""
    ◦ Presence: ""I felt I was in the heritage world.""
    ◦ Authenticity: ""I truly experienced the heritage.""
    ◦ Satisfaction: ""My expectations were met.""
    ◦ Behavioral Intention: ""I’d recommend this experience.""",multi-item new questionnaires,,,,Questionnaires
ID055,Digital Proxemics: Designing Social and Collaborative Interaction in Virtual Environments,"Williamson  Julie R.,  O'Hagan  Joseph,  Guerra-Gomez  John Alexis,  Williamson  John H,  Cesar  Pablo,  Shamma  David A.",,10.1145/3491102.3517594,2022,Conference on Human Factors in Computing Systems,VR,Behavioural Dynamics & Exploration,Embodiment Avatars & Social Presence.,Telecommunications and Collaboration,"This study introduces the concept of digital proxemics and presents an experimental comparison of how audio design(background chatter vs. silence) and display modality (HMD vs. desktop PC) influence social interaction behavior in virtual environments. Using a custom instrumented version of Mozilla Hubs, the authors found that HMD users maintained greater personal space, exhibited more head movement and attention behavior, and avoided intimate zone collisions more effectively than desktop users. Audio conditions also influenced proximity, with background chatter (cocktail mode) leading to closer group formations. The study offers new metrics and design insights for collaborative and social interaction in XR, contributing a rich behavioral dataset and quantitative methodology.","1. Participants
• 24 participants (distributed remotely, not co-located).
• Grouped into 4 conditions (6 participants per group):
    ◦ HMD (Oculus Quest) + Cocktail audio
    ◦ HMD + Bubble audio
    ◦ Desktop PC + Cocktail audio
    ◦ Desktop PC + Bubble audio
• 2 facilitators (authors) per session.
• Participants were monetarily compensated.
2. Study Design
• Between-subjects design with two factors:
    1. Display Modality:
        ▪ HMD (Oculus Quest): Full body/head tracking, hand gestures.
        ▪ Desktop PC: Keyboard/mouse controls, no hand tracking.
    2. Audio Design:
        ▪ Cocktail (Inverse model): Background noise audible at all distances (simulating real-world chatter).
        ▪ Bubble (Exponential model): Background noise silenced beyond 8 meters (isolated ""audio bubble"").
• Environment: Mozilla Hubs ""Outdoor Meetup"" VR space (70m × 40m).
3. Procedure
1. Pre-Session:
    ◦ Participants completed technical checks and familiarized themselves with the VR environment.
2. Consensus Task:
    ◦ Large group (6 participants + facilitators): Introduced to a ""lost at sea"" survival scenario.
    ◦ Small groups (3 participants): Collaborated to select 5 most useful items from a list of 15 (10–15 minutes).
    ◦ Reconvened: Presented decisions and discussed the ""best"" solution as a large group.
3. Post-Session:
    ◦ Group discussion about the experience.
    ◦ Exit survey (questionnaire implied but not detailed in excerpt).
4. Task
• Sea Survival Consensus Task:
    ◦ Participants imagined being stranded at sea with limited resources.
    ◦ Goal: Agree as a group on 5 most critical items (e.g., shaving mirror, mosquito netting) for survival.
    ◦ Design Purpose: Encouraged verbal engagement and negotiation, mimicking real-world collaborative discussions.
5. Metrics Collected
• Behavioral Metrics (Quantitative):
    ◦ Proxemic Distances:
        ▪ Logged avatar positions (x, y, z) at 30 FPS to calculate:
            • Pairwise distances (intimate: <0.45m, personal: 0.45–1.2m, social: 1.2–3.6m, public: >3.6m).
            • Nearest-neighbor distances to assess personal space.
    ◦ Head Orientation/Field of View:
        ▪ Tracked head movements to measure attention (e.g., turning toward speakers).
    ◦ Microphone Activation:
        ▪ Correlated speech with proximity and group dynamics.
• Qualitative Data:
    ◦ Post-session discussions and exit surveys (implied but not detailed).
Key Findings
1. HMD vs. Desktop PC:
    ◦ HMD users maintained larger personal space (social zone) and avoided intimate-zone collisions.
    ◦ Desktop PC users crowded closer (personal/intimate zones) due to limited social signals (no hand tracking).
    ◦ HMD users oriented their heads toward speakers more naturally.
2. Audio Design:
    ◦ Cocktail condition: Groups stood closer (leaning in to hear over background noise).
    ◦ Bubble condition: Groups spread farther apart (audio isolation reduced distractions).
3. Social Signals:
    ◦ HMD users leveraged non-verbal cues (head turns, gestures) to regulate conversation flow.",Post-Experiment Interview,"Body Analysis,  Correlated Speech With Proximity and Group Dynamics,  Head Analysis,  Interpersonal Distance",,,"Questionnaires,  Behavioural"
ID056,Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments,"Ogawa  Nami,  Narumi  Takuji,  Hideaki Kuzuoka,  Hirose  Michitaka",,10.1145/3313831.3376562,2020,Conference on Human Factors in Computing Systems,VR,Embodiment Avatars & Social Presence,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"This study investigates how the appearance of self-avatars—manipulated by anthropomorphism (realistic vs. abstract) and visibility (full-body vs. hand-only)—affects user behavior, presence, and embodiment in room-scale VR environments. Through a well-controlled experiment (N=92), participants engaged in a series of navigation and task-based scenarios, which at times incentivized them to pass through virtual walls. Results show that users with realistic full-body avatars were significantly less likely and slower to walk through walls, indicating more realistic behavioral responses and stronger presence. However, the study found no significant difference in self-reported presence or physiological responses (SCR) across avatar conditions, though behavioral data aligned with stronger immersion for high-fidelity avatars.","Summary of the Experiments in the Paper
1) Participants
• Number: 92 participants (64 male, 28 female).
• Age: Mean age 30.48 ± 9.29 years.
• VR Experience:
    ◦ 30 had no prior VR experience.
    ◦ 44 had minimal experience.
    ◦ 18 had moderate to frequent VR use.
• Recruitment: Publicly recruited via social media.
2) Study Design
• Type: Between-subjects experiment with two factors:
    1. Anthropomorphism (realistic vs. abstract avatars).
    2. Visibility (full-body vs. hand-only avatars).
• Conditions: Four avatar types (see Figure 2 in the paper):
    ◦ Full-body Human (realistic, gender-matched).
    ◦ Robot (abstract, full-body).
    ◦ Human Hand (realistic, hand-only).
    ◦ Controller (abstract, hand-only).
• Environment: Room-scale VR (4×4m virtual rooms).
3) Procedure
1. Setup:
    ◦ Participants wore an HTC Vive Pro HMD, Vive Trackers (on shoes and waist), and SCR sensors (for physiological measurement).
    ◦ A virtual mirror was shown to enhance body ownership before the experiment.
2. Task Introduction:
    ◦ Participants were instructed to navigate four virtual rooms, each with increasing incentives to walk through walls.
    ◦ No explicit rules were given about wall penetration—participants had to decide whether to avoid or pass through walls.
3. Room Sequence:
    ◦ Room 1: Low incentive (hand penetration possible but not required).
    ◦ Room 2: Moderate incentive (shortcut possible by walking through walls).
    ◦ Room 3: High incentive (walls must be passed to complete the task).
    ◦ Room 4: Impossible to complete without wall penetration + threatening stimuli (lightning, needles) to measure SCR.
4. Post-Experiment:
    ◦ Questionnaires (presence, body ownership, response to threat).
    ◦ Semi-structured interviews on decision-making.
4) Task
• Primary Task: Navigate through four rooms, pushing buttons in sequence to activate a teleporter.
• Key Manipulation:
    ◦ Walls were penetrable, but participants were not explicitly told they could walk through them.
    ◦ Incentives increased across rooms to encourage wall penetration.
    ◦ Room 4 included threats (lightning, needles) to measure physiological responses.
5) Metrics CollectedMetric TypeSpecific MeasuresPurposeBehavioral- Whether participants walked through walls.- Time until first wall penetration.- Movement trajectories (head/hand tracking).Measure reluctance to violate virtual boundaries.Physiological (SCR)- Skin conductance response to virtual threats (lightning, needles).Assess arousal and presence.Questionnaires- Slater-Usoh-Steed Presence Questionnaire.- Gonzalez-Franco & Peck’s Embodiment Questionnaire (body ownership, response to threat).Subjective sense of presence and embodiment.Performance- Task completion time.- Button-pressing success.Assess efficiency and task adherence.
Key Findings
• Realistic full-body avatars discouraged wall penetration the most.
• Lower presence correlated with faster wall penetration.
• SCR responses were marginally stronger for full-body avatars when facing threats.
• Questionnaires showed higher body ownership for abstract avatars (contrary to hypotheses).
This study provides validated behavioral, physiological, and questionnaire-based metrics for assessing presence and realism in VR, making it highly relevant for XR user experience research.","Gonzalez-Franco & Peck's Embodiment Questionnaire,  Presence – General",Movement Trajectories,EDA / GSR (Skin Conductance),Completion Time,"Questionnaires,  Behavioural,  Physiological,  Performance"
ID057,Does Voice Matter? The Effect of Verbal Communication and Asymmetry on the Experience of Collaborative Social XR,"Merz  Christian,  Wienrich  Carolin,  Latoschik  Marc Erich",,10.1109/ISMAR62088.2024.00129,2024,International Symposium on Mixed and Augmented Reality (ISMAR),"2D monitor only, VR",Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Social VR,"This paper investigates how verbal communication and asymmetric device configurations influence user experience in collaborative social XR. In a between-subjects study with 52 participants (combined with data from a prior no-voice study), dyads collaborated on an object-sorting task while interacting either symmetrically (both in VR) or asymmetrically (one VR user and one desktop user), with or without verbal communication. The authors measured self-perception (presence, embodiment), other-perception (co-presence, social presence, plausibility, humanness), and task-perception (task load, usability, enjoyment) using validated scales. Results show that immersion strongly increases presence and embodiment; verbal communication boosts presence and involvement; and social presence improves only when both verbal communication and high immersion are available. Asymmetric collaboration maintains good interaction quality for VR users but reduces perceptual quality for desktop users. The study offers generalizable insights into how device asymmetry and social cues shape the quality of collaborative XR experiences.","Participants
• 52 new participants (paired in dyads), plus 53 participants from earlier no-voice dataset
• Mean age ≈ 20; mostly female; all studying media/computer science
• VR experience balanced across groups
Procedure
• Between-subjects:
    ◦ Symmetric VR–VR
    ◦ Asymmetric VR–Desktop
    ◦ Voice vs. No-Voice (merged prior dataset)
• Pre-study questionnaires: SSQ, ITQ
• Avatar customization
• Collaborative sorting task in shared virtual environment
• Post-study validated UX questionnaires
Task
• Dyadic cube-sorting task
• VR users interact via controllers; desktop users via mouse
• Interaction uses button-based mechanics to ensure equal affordances
• Voice allowed in VC conditions; no speaking allowed in nVC dataset
Data Collected (Validated User Metrics)
• Presence (IPQ: Spatial, Involvement, Realism)
• Embodiment (VEQ: Ownership, Agency, Change)
• Co-presence & Social Presence (NMM)
• Plausibility (VHP: ABP, MVE)
• Humanness (single item)
• Task load (NASA Raw TLX)
• Task enjoyment (single item)
• Usability (single item)
• Control metrics: SSQ, ITQ","IPQ (Igroup Presence Questionnaire),  NASA-TLX,  Networked Minds Questionnaire,  Presence – General,  SSQ (Simulator Sickness Questionnaire),  VEQ,  VHPQ",,,,Questionnaires
ID058,EEG Features of the Interaction between Sense of Agency and Body Ownership: A Motor Imagery BCI Case Study,"Arpaia  Pasquale,  D'Angelo  Mariano,  D'Errico  Giovanni,  De Paolis  Lucio Tommaso,  Esposito  Antonio,  Grassini  Sabrina,  Moccaldi  Nicola,  Natalizio  Angela,  Nuzzo  Benito Luigi",,10.1109/metroxraine54828.2022.9967507,2022,IEEE International Conference on Metrology for Extended Reality Artificial Intelligence and Neural Engineering,VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Healthcare and Medical Training,"This pilot study investigates the neurophysiological distinction between sense of agency and sense of body ownershipin immersive VR using an EEG-based motor imagery brain-computer interface (BCI). Participants controlled virtual arms in a VR football task either from a first-person view (Body Ownership – BO) or a rotated third-person view (No Body Ownership – NBO). EEG signal decoding accuracy and subjective ratings were collected. Results show that while agency was preserved in both conditions, ownership was present only in BO, suggesting a dissociation between agency and body ownership. EEG classification accuracy was slightly higher in the BO condition, but not significantly so. The study proposes that motor control accuracy and agency do not depend on body ownership, and offers early-stage neurophysiological evidence for ""disembodied agency"" in XR.","1) Participants
• N = 4 (pilot study, 2 women)
• Mean age: 32.75
• Normal or corrected-to-normal vision
• Provided informed consent; study approved by ethics board
• Final target sample: 34 participants (ongoing study)
2) Study Design
• Within-subjects repeated measures design
• Each participant experienced:
    ◦ Two conditions:
        ▪ BO (Body Ownership): Virtual arms in first-person perspective
        ▪ NBO (Non-Body Ownership): Virtual arms in third-person (rotated) perspective
• Randomized order of BO and NBO sessions across two days
• Each session = Calibration phase + Neurofeedback (task) phase
3) Procedure
1. Day 1 & 2: Each participant completed one of the two conditions (BO or NBO)
2. Each session (∼30 minutes) included:
    ◦ 3 calibration blocks (30 trials each)
    ◦ 3 feedback blocks in VR (30 trials each)
3. Break of 10 minutes between calibration and task phases
4. After the session: Participants completed a questionnaire on agency and ownership
4) Task
• VR Football Game (Motor Imagery Task):
    ◦ Participants controlled virtual arms using EEG signals derived from imagined movements (left or right arm)
    ◦ Goal: Tilt the table by raising the correct arm to score goals with a virtual ball
    ◦ Task difficulty adjusted with virtual ""bumps"" on the field
    ◦ Game feedback included sound and visual cues for scoring
• Calibration task (prior to VR):
    ◦ Participants saw directional cues (left/right) and imagined the corresponding arm movement for BCI training
5) Metrics Collected
🧠 Physiological Metrics (EEG):
• EEG recorded using FlexEEG headset
• Signal processing pipeline:
    ◦ Bandpass filtering (4–40Hz)
    ◦ Common Spatial Patterns (CSP)
    ◦ Mutual Information-Based Feature Selection (MIBIF)
    ◦ Naive Bayesian Parzen Window (NBPW) classifier
• Accuracy of motor imagery classification measured per condition
📋 Questionnaire Metrics:
• 12 statements, rated on a 7-point Likert scale (-3 to +3)
    ◦ Ownership (3 statements)
    ◦ Agency (3 statements)
    ◦ Control items (3 each for ownership and agency)
    ◦ Adapted from previous validated studies
🎯 Performance Metrics:
• Number of goals scored in the VR game
• BCI classification accuracy (calibration vs feedback phases, BO vs NBO)","Sense of Agency,  Sense of Ownership",,EEG,Accuracy,"Questionnaires,  Physiological,  Performance"
ID059,EEG-based Evaluation on Intuitive Gesture Interaction in Virtual Environment,"Lee  Jen-Tun,  Rajapakse  R.P.C. Janaka,  Miyata  Kazunori",,10.1109/cw55638.2022.00050,2022,International Conference on Cyberworlds (CW),VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This study evaluates the intuitiveness and cognitive workload of four different VR input devices (Vive Wands, Valve Index, LeapMotion, and a custom VR Glove) using EEG-based physiological signals. Participants performed basic bimanual gestures (grasp, rotate, scale) in a VR puzzle game while their prefrontal EEG activity was recorded using Looxid Link sensors embedded in an HTC Vive Pro. The results show that Vive Wands induced the most relaxed and intuitive interaction overall, while LeapMotion produced the strongest alpha rhythms during more cognitively demanding tasks, suggesting greater immersive potential. The study provides a comparative physiological evaluation of commonly used VR interaction methods and proposes EEG as a valid objective measure of user state in immersive gesture-based systems.","1) Participants
• Total: 15 healthy participants (10 males, 5 females).
• Age Range: 23–27 years (university students).
• Experience: Limited prior experience with LeapMotion and VR gloves; familiarity with VR controllers (Vive wands, ValveIndex) not specified.
2) Study Design
• Within-Subjects Design: All  used four input devices:
    ◦ Vive wands, ValveIndex controllers, LeapMotion, and a custom VR glove.
• Independent Variable: Input device type.
• Dependent Variables:
    ◦ Physiological Metrics: EEG signals (theta/alpha bands) to measure cognitive workload and emotional states (relaxation, intuition).
    ◦ Performance Metrics: Task success in gesture interactions (grasp, rotation, scaling).
3) Procedure
1. Setup:
    ◦ Hardware: HTC Vive Pro headset with integrated Looxid Link EEG sensors (6 electrodes: Fp1, Fp2, AF3, AF4, AF7, AF8).
    ◦ Software: Custom Unity3D application (puzzle-box game).
2. Training:
    ◦ Participants received uniform verbal instructions and practiced with all devices.
3. Task Execution:
    ◦ Performed three gestures (grasp, rotate, scale) to manipulate virtual objects to match target positions.
    ◦ EEG data recorded synchronously with gesture events.
4. Data Collection:
    ◦ EEG signals filtered (0.01–120 Hz band-pass, 60 Hz stop filter) and annotated in real-time.
4) Task
• Primary Task: Solve a geometry puzzle by:
    1. Grasping: Moving objects to target locations.
    2. Rotating: Adjusting object angles.
    3. Scaling: Resizing objects to fit targets.
• Goal: Compare intuitive interaction and cognitive workload across devices.
5) Metrics Collected
• Physiological Metrics:
    ◦ EEG Band Power: Theta (4–7 Hz) for intuition/creativity; alpha (8–12 Hz) for relaxation.
    ◦ Density Values: Normalized power spectral density (PSD) for each band (e.g., Vive wands: alpha = 0.108, theta = 0.252).
• Performance Metrics:
    ◦ Implicitly assessed via task completion (no explicit success rates reported, but EEG correlated with gesture precision).
• Behavioral Metrics:
    ◦ Gesture execution patterns inferred from EEG event markers (e.g., time-stamped interactions).
Key Findings
• Vive Wands: Highest relaxation (alpha) and intuition (theta) during simple tasks (grasping).
• LeapMotion: Highest immersion potential (elevated alpha during complex tasks like scaling/rotation).
• ValveIndex: Balanced performance but less intuitive than Vive wands.
• EEG Validation: Demonstrated feasibility of real-time cognitive workload monitoring in VR.
Relevance to Scoping Review
• RQ1/RQ2: Validates EEG as a physiological metric for quantifying UX (intuition, relaxation, immersion).
• RQ3: Outcomes inform device selection for VR design (trade-offs between immersion and usability).
• Inclusion Criteria:
    ◦ IC1: Generalizable insights into EEG-based UX evaluation.
    ◦ IC2: Novel EEG integration method with VR headsets.
• Exclusion Criteria: None violated (peer-reviewed, human-focused, broader UX implications).
Conclusion: Include for its innovative use of EEG to objectively compare VR interaction paradigms.",,,EEG,Task Success / Completion,"Physiological,  Performance"
ID060,"Effect of Render Resolution on Gameplay Experience, Performance, and Simulator Sickness in Virtual Reality Games","Wang  Jialin,  Shi  Rongkai,  Xiao  Zehui,  Qin  Xueying,  Hai-Ning Liang",,10.1145/3522610,2022,Computer Graphics and Interactive Techniques,VR,Content & System Design,User states: Cognitive & Affective Experience.,Entertainment and Gaming,"This study investigates how different render resolutions (1K to 4K) affect gameplay experience, simulator sickness (SS), and performance in a VR first-person shooter (FPS) game. Using Half-Life: Alyx as the testbed, 16 participants completed combat and navigation tasks under four resolution settings. Results showed that 2K resolution represents a key threshold: it significantly improves flow, positive affect, and reduces simulator sickness compared to 1K. However, no significant performance differences were observed across resolutions. The findings highlight the tradeoff between resolution and system resource usage, supporting 2K as an optimal resolution for balancing quality and usability in VR gaming.","
1. Participants:
    ◦ Number: 16 volunteers (6 females, 10 males).
    ◦ Age Range: 18 to 22 years (Mean = 19.88, SD = 1.11).
    ◦ Experience:
        ▪ 14 participants had experience with 3D video games.
        ▪ 13 participants had experience with non-VR FPS games.
        ▪ 5 participants had some experience with VR games.
    ◦ Vision: All participants had normal or corrected-to-normal vision, no history of color blindness, and no declared mental or physical health issues.
2. Study Design:
    ◦ Type: Within-subjects design.
    ◦ Independent Variable: Render resolution (1K, 2K, 3K, 4K).
    ◦ Dependent Variables:
        ▪ Simulator Sickness (SSQ scores).
        ▪ Gameplay Experience (GEQ scores).
        ▪ Player Performance (ammo fired, hits received).
    ◦ Order of Conditions: Balanced 4×4 Latin-Square design to mitigate carry-over effects.
3. Procedure:
    ◦ Pre-Experiment:
        ▪ Participants filled out a demographic questionnaire and provided information on their gaming and VR experience.
        ▪ Watched a 5-minute tutorial video of the game (Half-Life: Alyx).
        ▪ Adjusted the IPD (Interpupillary Distance) of the VR headset.
        ▪ Completed a 5-minute training session to learn how to use VR controllers for movement, aiming, shooting, and reloading.
    ◦ Experiment:
        ▪ Participants played the game at four different resolutions (1K, 2K, 3K, 4K) in a randomized order.
        ▪ Each session lasted 5 minutes, during which participants had to defeat 5 enemies in a predefined route.
        ▪ After each session, participants filled out the Simulator Sickness Questionnaire (SSQ) and the Game Experience Questionnaire (GEQ).
        ▪ Gameplay performance data (ammo fired, hits received) was recorded.
        ▪ Participants were given enough rest between conditions to prevent the accumulation of simulator sickness.
    ◦ Post-Experiment:
        ▪ A semi-structured interview was conducted to gather participants' general feedback on their experience with different resolutions.
4. Task:
    ◦ Game Environment: A modified level from Half-Life: Alyx, featuring both indoor and outdoor scenes with 5 enemy soldiers placed at different locations.
    ◦ Objective: Defeat all 5 enemies within 5 minutes.
    ◦ Movement: Participants used teleportation for locomotion (to reduce simulator sickness) and could physically move for small-range actions like dodging.
    ◦ Weapon: Participants were given a pistol with unlimited ammo (999 rounds) to eliminate bias from different weapon experiences.
    ◦ Difficulty: Enemies were defeated with 2 headshots or 4 body shots.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Simulator Sickness Questionnaire (SSQ): Measured simulator sickness levels after each condition. Sub-scores included Total Severity, Nausea, Oculomotor, and Disorientation.
        ▪ Game Experience Questionnaire (GEQ): Measured gameplay experience. Sub-scores included Flow, Immersion, Competence, Positive Affect, and Negativity.
    ◦ Performance Metrics:
        ▪ Ammo Fired: Number of bullets fired during the task.
        ▪ Hits Received: Number of times the participant was hit by enemies.
    ◦ Behavioral Metrics:
        ▪ Player performance in the game (e.g., accuracy, time to complete the task).
    ◦ General Feedback:
        ▪ Semi-structured interviews provided qualitative insights into participants' experiences with different resolutions.
Summary:
The study involved 16 participants who played a modified level of Half-Life: Alyx at four different resolutions (1K, 2K, 3K, 4K) in a within-subjects design. The task was to defeat 5 enemies within 5 minutes using a pistol with unlimited ammo. Metrics collected included simulator sickness (SSQ), gameplay experience (GEQ), and performance data (ammo fired, hits received). The study aimed to understand the impact of render resolution on gameplay experience, performance, and simulator sickness in VR.","SSQ (Simulator Sickness Questionnaire),  Game Experience Questionnaire (GEQ),  Semi-structured Interviews",,,Gameplay Performance,"Questionnaires,  Performance"
ID061,Effects of Avatar Transparency on Social Presence in Task-Centric Mixed Reality Remote Collaboration,"Yoon  Boram,  Shin  Jae-eun,  Kim  Hyung-il,  Young Oh  Seo,  Kim  Dooyoung,  Woo  Woontack",,10.1109/TVCG.2023.3320258,2023,Transactions on Visualization and Computer Graphics,"AR, MR, VR",Embodiment Avatars & Social Presence,Content & System Design.,Human-Computer Interaction (HCI),"This study explores how avatar transparency (nontransparent, semi-transparent, near-transparent) influences social presence, interpersonal impression, task load, and behavioral patterns during task-centric MR remote collaborationbetween AR and VR users. Results show that while task performance remained consistent across conditions, semi-transparent avatars were preferred, as they provided a balance between visibility and comfort. Near-transparent avatars reduced social presence and co-presence, while nontransparent avatars increased awareness but also discomfort. Device type significantly influenced experience: VR users reported higher social presence and action possibility, whereas AR users experienced higher task load. Findings inform adaptive design strategies for avatar representation based on task and device.","1) Participants
• Total: 54 participants (34 female, 20 male).
• Age range: 20–38 years (M = 26.50, SD = 3.58).
• Pairings: 27 pairs (randomly assigned as AR-VR collaborators).
• Prior XR experience:
    ◦ 40.74% had moderate-to-high HMD experience (7+ uses).
    ◦ 81.49% had little-to-no prior experience with 3D remote collaboration systems.
2) Study Design
• Mixed factorial design:
    ◦ Within-subjects factor: Avatar transparency (3 levels: Nontransparent (0%), Semi-transparent (50%), Near-transparent (85%)).
    ◦ Between-subjects factor: Device type (AR [HoloLens 2] vs. VR [Meta Quest 2]).
• Counterbalancing:
    ◦ Avatar transparency order randomized using Latin Square.
    ◦ Task sets (Mars, Moon, Earth research bases) balanced across conditions.
3) Procedure
1. Pre-study:
    ◦ Participants completed demographic and experience surveys.
    ◦ Brief training on bare-hand interaction with virtual objects.
2. Main experiment (3 sessions, one per transparency level):
    ◦ Greeting: Participants exchanged a virtual high-five to start.
    ◦ Task execution: Collaborated to assemble a space research base (see Task below).
    ◦ Post-task: Completed questionnaires (social presence, task load, etc.).
3. Post-experiment:
    ◦ Semi-structured interviews on avatar perception and collaboration experience.
4) Task
• Goal: Collaboratively assemble a virtual space research base (Mars, Moon, or Earth theme).
• Structure:
    ◦ Each participant received different but interdependent instructions (Type A/B).
    ◦ Required three actions:
        1. Assembly/placement of base components.
        2. Coloring/texturing objects.
        3. Tagging objects with labels.
    ◦ Forced collaboration: Some steps required partner completion (e.g., checking progress).
• Interaction:
    ◦ Bare-hand manipulation of virtual objects.
    ◦ Verbal communication enabled throughout.
    ◦ Mandatory gestures (high-five at start/end).
5) Metrics CollectedCategorySpecific MetricsMeasurement MethodQuestionnaires- Social Presence (Networked Minds, Temple Presence Inventory, Nowak’s Social Presence)- Perceived Action Possibility (SPES)- Interpersonal Impression (emotional reaction, likability)- Task Load (NASA TLX)7-point Likert scales (unless noted)Behavioral- Close encounters (frequency & duration of avatar collisions)- Movement patternsVideo coding & automated trackingPerformance- Task completion time (seconds)System logsQualitativePost-experiment interviews (avatar perception, collaboration experience)Thematic analysis
Key Takeaways from the Experiment
• Avatar transparency significantly impacted social presence (e.g., Near-transparent reduced awareness).
• Device type (AR vs. VR) influenced task load (higher for AR) and social presence (higher for VR).
• Behavioral metrics (close encounters) revealed users avoided opaque avatars but tolerated transparent ones.
• No difference in task performance (time) across transparency levels, suggesting task focus overrides avatar effects.
This experiment provides a robust evaluation of user metrics in asymmetric MR collaboration, aligning well with your scoping review’s focus on XR quality and experience.","Post-Experiment Interview,  NASA-TLX,  Perceived Action Possibility (SPES),  Social Presence,  Temple Presence Inventory",Movement Trajectories,,Completion Time,"Questionnaires,  Behavioural,  Performance"
ID062,Effects of Different Tracker-driven Direction Sources on Continuous Artificial Locomotion in VR,"Lougiakis  Christos,  Mandilaras  Theodoros,  Katifori  Akrivi,  Ganias  Giorgos,  Ioannidis  Ioannis-Panagiotis,  Roussou  Maria",,10.1145/3641825.3687735,2024,ACM Symposium on Virtual Reality Software and Technology,VR,Interaction Techniques & input Modalities,Behavioural Dynamics & Exploration.,Interaction & Navigation Design in VR Games,"This paper examines how different movement direction sources—Head, Hand, Hip, and a new Feet-based method—affect performance, motion sickness, presence, and user preference during continuous artificial locomotion (CAL).
Using 22 participants, the authors conducted a within-subject experiment where each user performed three VR tasks (a zigzag course, corridor task, and colour-matching game) using all four direction sources. Performance was measured through task duration and accuracy; presence was assessed using items adapted from the Presence Questionnaire (PQ); and motion sickness was evaluated with SSQ symptoms.
Main results:
Performance: No significant differences across most tasks, except in the Colour-Game where the Head method was faster than the Hand method.
Motion sickness: Minimal across all conditions; no method differed significantly.
Presence: No substantial differences except that Hand was rated significantly less natural and less realistic than the other methods.
User preference: Hand was consistently the least preferred method; Feet and Hip were perceived as more natural/realistic.
The paper demonstrates that forward-direction source has limited impact on performance or sickness, but strongly influences naturalness, realism, and user preference. Importantly, the authors introduce and empirically evaluate a new Feet-driven locomotion technique, offering useful behavioural and experiential insights relevant to VR locomotion UX.","Participants
• N = 22 (15 male, 7 female)
• Mostly VR novices
• Many had experience with game controllers
• Good physical condition, no reported discomfort prior to participation
Design
• Within-subjects, Latin Square ordering
• 4 direction-source conditions:
    ◦ Head
    ◦ Hand
    ◦ Hip
    ◦ Feet
• Each participant performed 3 tasks per condition
• Direction source shown on a virtual panel, and questionnaires filled in VR
Tasks
1. ZigZag course – 13 sharp turns; measures accuracy (time out of bounds) + duration
2. Corridor task – walk straight and count dots; tests visual awareness
3. Colour-Game – navigation + interaction + grabbing/placing cubes
Apparatus
• HTC Vive Pro HMD
• Valve Index controllers
• 3 Vive trackers for hip + feet
• Unity VE
• 3×3m play area
Metrics collected
Performance Metrics
• Task duration (all tasks)
• ZigZag accuracy (in vs. out-of-path ratio)
• Corridor accuracy (dot counting correctness)
Questionnaires — Validated & Custom
• Presence Questionnaire (PQ) subscales: naturalness, realism, consistency (IQ2, IQ4, etc.)
• SSQ symptom subset (general discomfort, nausea, dizziness, etc.)
• Comparative preference questionnaire (open-ended + forced choice)
Behavioural / Derived Metrics
• Out-of-path counter (ZigZag)
• Task-specific accuracy measures
• Breaks in presence (from open comments)","Presence – General,  SSQ (Simulator Sickness Questionnaire)",,,"Accuracy,  Error Rate,  Completion Time","Questionnaires,  Performance"
ID063,Effects of Egocentric Versus Exocentric Virtual Object Storage Technique on Cognition in Virtual Environments,"Khadka  Rajiv,  Banic  Amy",,10.1109/VRW50115.2020.00044,2020,Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops,VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This study investigates how egocentric vs. exocentric virtual object storage techniques affect cognitive performanceand task efficiency in VR. Using a within-subjects design, participants stored and recalled the position and orientation of virtual objects either relative to their own body (egocentric) or within the environment (exocentric), across same-context and change-of-context conditions. Results showed that egocentric techniques led to significantly higher task accuracy, faster completion time, lower cognitive workload (NASA-TLX), and higher usability (SUS) than exocentric ones. The findings suggest that body-based object association in VR can enhance memory and reduce workload in spatial tasks.","
1) Participants
• A total of 12 participants (1 female, 11 males) from the University of Wyoming.
• Average age: 24.95 years (SD = 5.46).
• All participants passed a stereopsis test and reported not being color blind.
• Participation was voluntary, and no compensation was provided https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=13144,14033, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=5019,5100.
2) Study Design
• The study employed a 2x2 within-subjects experimental design.
• Two main conditions were tested: Egocentric (associating virtual objects with the participant's body) and Exocentric (associating virtual objects with the environment).
• Each condition was further divided into Same Context and Change of Context https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=10669,11519, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=22535,23050.
3) Procedure
• Participants completed a pre-questionnaire and a Butterfly Stereoscopic test to assess stereopsis ability.
• They underwent training in a virtual environment (VE) to familiarize themselves with the tasks.
• Each participant completed five trials for each of the four experimental conditions, assigned using the Latin square method https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=13144,14033, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=20080,20854.
4) Task
• The experiment involved two main tasks:
    1. Memory Task: Participants were exposed to a set of nine virtual objects in random positions and orientations. They had to remember these positions and orientations after a manipulation task.
    2. Manipulation Task: This task served as a 
distraction, where participants moved virtual objects to match the 
position and orientation of corresponding target objects https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=16453,17468, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=19193,20077.
5) Metrics Collected
• Task Completion Time: Measured the time taken to complete the tasks across different conditions.
• Accuracy: Counted the number of virtual objects placed correctly in their original positions.
• Error Rates: Measured positional and rotational errors in object placement.
• NASA TLX: Collected subjective measures of mental workload.
• System Usability Scale (SUS): Assessed usability perception after each condition.
• Memorability Data: Evaluated how well participants felt they remembered the locations of the virtual objects using a Likert scale https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=26714,27445, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68235ed2411e9deb6a1acf47/?range=15537,16452.","NASA-TLX,  Social Presence",,,"Accuracy,  Error Rate,  Completion Time","Questionnaires,  Performance"
ID064,Effects of heart rate feedback on an asymmetric platform using augmented reality and laptop,"Dey  Arindam,  Cao  Yufei,  Dobbins  Chelsea",,,2022,Conference on Virtual Reality and 3D User Interfaces,"AR, laptop",Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This study examines the effect of audio-visual heart rate feedback on user experience in asymmetric collaboration using AR and laptop setups. Participants performed both collaborative and competitive tasks, with and without simulated heart rate feedback from their partner. Results showed that heart rate feedback increased perceived emotional connection and inclusion (IOS) for AR users, though not significantly for laptop users. Physiological responses (GSR) confirmed higher arousal during competition, while subjective ratings (PANAS, IOS, presence) were consistently higher for collaborative tasks. The study validates heart rate feedback as a behavioral and social cue in XR collaboration and suggests design recommendations for future asymmetric multi-user interfaces.","1. Participants
• Total Participants: 20 individuals (10 groups of 2 participants each).
• Gender: 8 females and 12 males.
• Mean Age: 23.7 years (SD = 2.26).
• Inclusion Criteria: All participants had normal or 
corrected-to-normal vision and reported no physical or neurological 
conditions affecting their performance https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=22706,23728 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=23729,24526.
2. Study Design
• Type: Mixed-factorial user study.
• Independent Variables:
    ◦ Heart Rate Feedback: Between-subjects (On, Off).
    ◦ Task: Within-subjects (Collaboration, Competition).
    ◦ Device: Within-subjects (Hololens2 for AR, Laptop) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=16001,16897, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=28626,29317.
3. Procedure
• Participants were welcomed, introduced to the study, and asked to sign a consent form.
• They completed a demographic questionnaire and were fitted with an Empatica E4 wristband for physiological data collection.
• Participants performed both collaborative and competitive tasks in a counterbalanced order.
• After each task, they completed questionnaires and participated in a semi-formal interview about their experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=24529,25307 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=29318,29784.
4. Task
• Collaboration Task: A code decryption treasure hunt
 where one participant (using the laptop) acted as the decoder and the 
other (using the Hololens2) searched for clues in the AR environment.
• Competition Task: A find-the-ball game where both participants competed to find a ball hidden under boxes, with no communication allowed https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=21123,21917, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=20336,21122.
5. Metrics Collected
• Physiological Metrics: Heart rate, Skin Conductance Level (SCL), and Skin Conductance Response (SCR) collected via the Empatica E4 wristband.
• Questionnaires: 
    ◦ Positive and Negative Affect Schedule (PANAS) to measure emotional states.
    ◦ Inclusion of Other in Self Scale (IOS) to assess feelings of connection with the collaborator.
    ◦ Additional subjective questions regarding presence and emotional awareness https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=46179,46749, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67c99e876396dafb364fd585/?range=15080,15998.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, tasks, and metrics collected","Inclusion of Other in Self Scale,  PANAS",,"Heart Rate,  EDA / GSR (Skin Conductance)",,"Questionnaires,  Physiological"
ID065,Effects of Presence and Challenge Variations on Emotional Engagement in Immersive Virtual Environments,"Caldas  Oscar I.,  Aviles  Oscar F.,  Rodriguez-Guerrero  Carlos",,10.1109/TNSRE.2020.2985308,2020,IEEE Transactions on Neural Systems and Rehabilitation Engineering,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study investigates how presence-based (visual realism, social cues, avatar embodiment) and challenge-based (game difficulty) variations in a VR task influence emotional engagement, measured through self-reported emotional state (SAM: valence, arousal, dominance) and physiological signals (ECG, GSR, respiration). Results from 87 participants showed that presence manipulations primarily affected arousal, while challenge manipulations influenced valence and dominance, particularly in cognitively demanding tasks. Physiological signals such as skin conductance level (SCL) and heart rate variability (HRV) were associated with arousal and task stress, confirming the value of multimodal metrics in immersive emotion modeling.","1) Participants
• Total: 87 healthy adults (51 male, 36 female).
• Mean age: 24.64 years (SD = 8.41).
• Exclusions:
    ◦ 1 male removed due to VR sickness (eyestrain/headache).
    ◦ 1 female excluded due to noisy ECG signal.
    ◦ 1 male excluded due to respiration sensor disconnection.
    ◦ 7 participants excluded due to GSR sensor issues.
• Final groups:
    ◦ Group A (Presence-based variations): 38 (ECG), 37 (GSR), 38 (RSP).
    ◦ Group B (Challenge-based variations): 47 (ECG), 42 (GSR), 47 (RSP).
2) Study Design
• Between-subjects design: Participants randomly assigned to Group A (presence variations) or Group B (challenge variations).
• Within-subjects component: Each group experienced 5 trials with different experimental conditions (randomized order).
• Baseline measurement: 1-minute relaxation phase before trials.
• Post-trial assessments: Self-reported emotions (SAM) and physiological signals collected after each trial.
3) Procedure
1. Pre-test:
    ◦ Participants completed the Virtual Reality Sickness Questionnaire (VRSQ) before exposure.
2. Baseline recording:
    ◦ 1-minute relaxation period to establish physiological baselines (ECG, GSR, RSP).
3. Trials:
    ◦ 5 trials per participant, each lasting ~150 seconds.
    ◦ Group A: Variations in presence (social, physical, self-presence).
    ◦ Group B: Variations in challenge (ring size, falling speed, cognitive quiz).
4. Post-trial assessment:
    ◦ After each trial, participants completed the Self-Assessment Manikin (SAM) to report valence, arousal, and dominance.
    ◦ Physiological signals (ECG, GSR, RSP) recorded continuously.
5. Post-test:
    ◦ VRSQ repeated to check for VR sickness.
4) Task
• VR Skydiving Game (Unity3D):
    ◦ First-person perspective: Participants controlled a virtual skydiver using a balance board (tilting to navigate).
    ◦ Objective: Pass through colored rings (correct ring indicated by color) before landing on a target.
    ◦ Scoring:
        ▪ +10 points for correct ring, -10 for wrong rings.
        ▪ +20 points for accurate landing (max score = 100).
    ◦ Group A (Presence Variations):
        ▪ Trial 1: Full presence (social, physical, self-presence).
        ▪ Trials 2–4: One presence factor removed (e.g., no social presence, no body representation).
        ▪ Trial 5: Minimal presence (all factors removed).
    ◦ Group B (Challenge Variations):
        ▪ Trial 1: Easy (big rings, slow falling speed, no quiz).
        ▪ Trials 2–5: Increasing difficulty (smaller rings, faster falling, added trivia quiz).
5) Metrics CollectedCategoryMetricsDescriptionQuestionnairesSAM (Self-Assessment Manikin)9-point scale for valence, arousal, dominance.VRSQ (Virtual Reality Sickness Questionnaire)Assessed VR sickness symptoms (pre/post).PhysiologicalECG (Electrocardiogram)Heart rate (HR), HR variability (HRV), spectral analysis (LF/HF).GSR (Galvanic Skin Response)Skin conductance level (SCL), skin conductance responses (SCR).RSP (Respiration)Breathing rate, depth, variability.PerformanceGame ScorePoints from ring accuracy and landing.BehavioralBalance board movementsTilt angles (pitch/roll) recorded via gyroscope.
Key Takeaways
• The study used a mixed-methods approach, combining self-reports (SAM), physiological signals (ECG, GSR, RSP), and performance metrics (game score).
• Group A tested how presence factors (social, physical, self-presence) affect emotions.
• Group B tested how challenge (difficulty, cognitive load) affects emotions and performance.
• Findings:
    ◦ Arousal was influenced by presence (e.g., higher with low social presence).
    ◦ Valence & dominance were influenced by challenge (e.g., decreased with higher difficulty).
    ◦ Physiological responses (GSR, HRV, respiration) correlated with emotional states.
This experiment provides a strong methodological framework for assessing XR user experience through multimodal metrics.","SAM (Self-Assessment Manikin),  VRSQ (Virtual Reality Sickness Questionnaire)",,"ECG,  EDA / GSR (Skin Conductance),  Respiration",,"Questionnaires,  Physiological"
ID066,Effects of Presence on Human Performance and Workload in Simulated VR-based Telerobotics,"Nenna  Federica,  Zanardi  Davide,  Gamberini  Luciano",,10.1145/3594806.3594856,2023,International Conference on Pervasive Technologies Related to Assistive Environments,VR,User states: Cognitive & Affective Experience,,Robotics,"This study explored the impact of the Sense of Presence (SoP) on user performance and workload in a VR-based simulated telerobotics task. Participants teleoperated a robotic arm in a pick-and-place task using different control methods and task demands. SoP was self-reported and grouped into low, medium, and high. The study found that higher SoP correlated with significantly faster task performance, especially during the place phase. However, SoP had no statistically significant effect on workload, whether measured explicitly (NASA-TLX) or implicitly (via pupil size variations), though some non-significant trends were noted. The work supports the role of presence in enhancing efficiency but calls for further research into its relation with cognitive load.","1. Participants
• Number: 18 (9 females, 9 males).
• Age: Mean = 26.33 years (SD = 2.02).
• Criteria: Normal/corrected-to-normal vision (via contact lenses only), no neurological/psychiatric conditions.
• Compensation: None (voluntary participation).
2. Study Design
• Within-subjects design: All participants completed all tasks.
• Independent Variables:
    ◦ Control Modality:
        ▪ Button-based (controller input).
        ▪ Action-based (physical movements).
    ◦ Task Load:
        ▪ Single-task (pick-and-place only).
        ▪ Dual-task (pick-and-place + concurrent arithmetic task).
• Dependent Variables:
    ◦ Performance (operation times).
    ◦ Explicit workload (NASA-TLX questionnaire).
    ◦ Implicit workload (pupil size variations).
    ◦ Sense of Presence (MEC-SPQ questionnaire).
3. Procedure
1. Pre-Experiment:
    ◦ Participants completed demographic and expertise questionnaires.
    ◦ Training session to familiarize with VR and tasks.
2. Experimental Session:
    ◦ 5 tasks administered in random order:
        1. Arithmetic task (baseline).
        2. Pick-and-place task (button-based, single-task).
        3. Pick-and-place task (button-based, dual-task).
        4. Pick-and-place task (action-based, single-task).
        5. Pick-and-place task (action-based, dual-task).
    ◦ After each task, NASA-TLX was administered to assess workload.
3. Post-Experiment:
    ◦ MEC-SPQ questionnaire to evaluate overall Sense of Presence (SoP) during the VR experience.
4. Task Details
• Primary Task: Pick-and-Place with Industrial Robotic Arm
    ◦ Goal: Guide a virtual robotic arm to pick up an object and place it in a target location.
    ◦ Control Methods:
        ▪ Button-based: Use controller buttons to manipulate the robot.
        ▪ Action-based: Use physical gestures (e.g., arm movements) to control the robot.
    ◦ Dual-Task Condition: Concurrently perform an arithmetic task (e.g., mental calculations) to increase cognitive load.
• Arithmetic Task:
    ◦ Served as a secondary task in dual-task conditions to elevate mental demand.
5. Metrics Collected
1. Performance:
    ◦ Operation Times: Time (seconds) to complete pick and place actions.
2. Explicit Workload:
    ◦ NASA-TLX: 6 subscales (Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, Frustration) rated on a 1–20 scale.
3. Implicit Workload:
    ◦ Pupil Size Variations: Measured via HTC Vive Pro Eye’s integrated eye-tracking system.
4. Sense of Presence (SoP):
    ◦ MEC-SPQ Questionnaire: Rated on a 1–5 Likert scale (later scaled to 0–1), assessing:
        ▪ Self-location, possible actions, involvement, and immersion in VR.
    ◦ Participants grouped into Low (0–0.33), Medium (0.34–0.66), or High (0.67–1) SoP based on scores.
Key Notes on Analysis
• Statistical Models: Generalized Linear Models (GLMs) in R.
• Group Comparisons: SoP groups (Low/Medium/High) compared for performance, workload, and pupil data.
• Limitations:
    ◦ SoP grouping was post hoc (not experimentally manipulated).
    ◦ Small sample size per SoP group (5–7 participants).
This design allowed the researchers to explore how SoP influences efficiency and cognitive load in VR-based telerobotics, with a focus on both behavioral (task times) and physiological (pupil metrics) outcomes.","NASA-TLX,  Social Presence",,Pupil Analysis,Completion Time,"Questionnaires,  Physiological,  Performance"
ID067,Effects of third-person locomotion techniques on sense of embodiment in virtual reality,"Ulrichs  Johanna,  Matviienko  Andrii,  Quintero  Luis",,10.1145/3701571.3701598,2024,International Conference on Mobile and Ubiquitous Multimedia,VR,Embodiment Avatars & Social Presence,Interaction Techniques & input Modalities.,Interaction & Navigation Design in VR Games,"This paper investigates how three third-person VR locomotion techniques — controller joystick, head tilt, and arm swing — affect users’ sense of embodiment (SoE), perceived usability, and VR sickness. In a controlled experiment with 16 participants, the authors systematically compared these techniques using a VR mini-game in which participants navigated an avatar in third-person perspective to collect virtual objects.
Findings show that physically engaging locomotion techniques (arm swing and head tilt) elicit higher embodiment—particularly in the sense of agency—than joystick-based control. However, they also result in higher completion timesand increased oculomotor discomfort. The controller joystick was rated as easiest and least fatiguing, while arm swingwas preferred for engagement and fun despite greater effort. The study demonstrates a trade-off between embodiment and usability, providing insights into how bodily engagement enhances perceived self-agency in non-egocentric (third-person) VR environments.","1) Participants• Number: 16 (9 male, 6 female, 1 non-binary)• Age: 21–42 years (M = 28, SD = 6.35)• Experience: All had prior VR experience; 14 used it frequently for work or leisure.

2) Study Design• Type: Within-subjects design.• Independent Variable: Locomotion technique with three levels: 
1. Controller Joystick – baseline (low bodily engagement). 
2. Head Tilt – movement via head rotation and tilt.. 
3. Arm Swing – movement via arm-swing gesture while pressing triggers.
• Counterbalancing: Full counterbalanced order.
• Dependent Variables: 
– Sense of Embodiment (SoE) 
– Perceived Usability 
– VR Sickness (VRSQ) 
– Task Completion Time

3) Task
• Participants controlled a third-person avatar (construction worker) in a VR warehouse.
• Task: Collect 10 floating tools by navigating the space (same map layout across conditions).
• Each locomotion mode had a 10-minute time limit; trials lasted ~8–10 min.
• The virtual camera followed the avatar (20 units behind, 8 units above) in all conditions.

4) Apparatus
• Hardware: Meta Quest 2 HMD.
• Environment: 3×3 m physical play area.
• Software: Unity 2022.3; circular “orb view” used to limit FOV and minimize sickness.

5) Metrics Collected
• Questionnaires – Sense of Embodiment (Gonzalez-Franco & Peck, 2018):
 – Subscales: Sense of Body Ownership (SBO), Sense of Agency (SA), Sense of Self-Location (SSL)
 – 10 items (7-point Likert scale, –3 to +3).
• Questionnaires – Perceived Usability (McMahan et al., 2012):
 – Subscales: Easiness, Naturalness, Fun, Exhaustion (7-point Likert scale).
• Questionnaires – VR Sickness (VRSQ; Kim et al., 2018):
 – Subscales: Oculomotor (4 items) and Disorientation (5 items).
• Performance Metrics: Task completion time (seconds) recorded automatically.
• Qualitative Metrics: Post-experiment interviews on usability and comfort.

Main Findings
• Embodiment:
 – Head Tilt and Arm Swing → significantly higher total SoE than Joystick (p < .001).
 – Arm Swing > Head Tilt (p = .028) for overall embodiment, mainly driven by Agency.
 – Self-location remained low across conditions (expected for 3PP).
• Usability:
 – Joystick rated easiest, most natural, and least tiring.
 – Arm Swing perceived as more fun and engaging but slower.
 – Head Tilt least usable (unnatural and tiring).
• VR Sickness:
 – Joystick induced the least sickness.
 – Head Tilt and Arm Swing increased oculomotor symptoms (eye strain, fatigue).
 – No significant differences for Disorientation symptoms.
• Completion Time:
 – Joystick fastest (M = 120s), followed by Head Tilt (137s), Arm Swing (167s).
• Qualitative feedback:
 – Participants described Arm Swing as “most engaging” but “physically tiring.”
 – Head Tilt caused navigation difficulties; some suggested its use for secondary actions (e.g., dodging).","Sense of Embodiment,  Usability – Custom / Rating-based,  VRSQ (Virtual Reality Sickness Questionnaire)",,,Completion Time,"Questionnaires,  Performance"
ID068,EGG Objective Characterization of Cybersickness Symptoms towards Navigation Axis,"Tian  Nana,  Achache  Khalil Haroun,  Ben Mustapha  Ali Raed,  Boulic  Ronan",,10.1109/VRW58643.2023.00068,2023,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,,Healthcare and Medical Training,"This paper investigates cybersickness in VR by combining navigation axis design (longitudinal, lateral, yaw) with physiological measurements from Electrogastrogram (EGG) data. It confirms that yaw rotation is the primary factor contributing to cybersickness. Using a full-factorial design across 8 conditions, the study demonstrates strong correlations between EGG-based metrics (tachygastria ratio, normal ratio, mean dominant frequency) and cybersickness severity. These indicators vary based on user susceptibility classification, suggesting EGG as a promising tool for real-time, non-invasive monitoring of VR-induced nausea.

EGG: Tachygastria ratio, normal gastric ratio, bradygastria ratio, normal-to-tachy ratio, mean dominant frequency","
1. Participants:
    ◦ Total: 26 participants (10 female)
    ◦ Age Range: 18–25 years (Mean = 21, SD = 1.9)
    ◦ Recruitment: Through EPFL intranet
    ◦ Inclusion Criteria:
        ▪ No alcohol, motion-sickness medication, or similar substances 12 hours before the experiment
        ▪ Last meal at least 2 hours before the session
    ◦ Compensation: Cash payment
    ◦ Ethics: Approved by an institutional review board
2. Study Design:
    ◦ Full factorial 2×2×2 within-subjects design with three navigation factors:
        ▪ Longitudinal axis (X): No movement vs. Translational movement
        ▪ Lateral axis (Z): No movement vs. Translational movement
        ▪ Yaw axis (R): No rotation vs. Rotational movement
    ◦ 8 experimental conditions:
        ▪ ooo, xoo, ozo, xzo, oor, xor, ozr, xzr (combinations of the three factors)
    ◦ Session structure:
        ▪ 8 experimental sessions per participant
        ▪ Minimum 1-day gap (up to 3 days) between sessions
        ▪ Counterbalancing: Always started with the ""ooo"" (no motion) condition, rest randomized with a Latin-Square design
    ◦ Setting: Conducted in a sound-attenuated room
3. Procedure:
    ◦ Session flow:
        1. Demographics questionnaire (collected once before the experiment)
        2. Pre-experiment health check using the Simulator Sickness Questionnaire (SSQ)
        3. VR exposure: 20 minutes per condition
        4. Continuous physiological recording (Electrogastrogram (EGG), Electrocardiogram (ECG))
        5. Fast Motion Sickness Questionnaire (FMS) recorded every minute
        6. Post-experiment SSQ and participant interview
    ◦ Hardware used:
        ▪ VR Headset: HTC Vive Pro Eye (120Hz eye-tracking, 90Hz refresh rate)
        ▪ EGG & ECG recording system: Smart EGG100D Electrogastrogram Amplifier and BioPac MP160
        ▪ Software: Acknowledge 5.0 for physiological data monitoring
4. Task:
    ◦ Pre-programmed VR navigation task in a game environment to control movement exposure
    ◦ Participants were seated and used controllers to perform a minimal interaction task:
        ▪ Fire extinguishing task: Aim at fires on buildings while being moved automatically in VR
    ◦ Motion conditions varied (longitudinal, lateral, yaw rotation) to induce cybersickness
5. Metrics Collected:
    ◦ Self-reported Questionnaires:
        ▪ Simulator Sickness Questionnaire (SSQ) (before and after each session)
        ▪ Fast Motion Sickness Questionnaire (FMS) (every minute during VR exposure)
        ▪ Motion Sickness Susceptibility Questionnaire (MSSQ) (collected once before experiment)
    ◦ Physiological Metrics:
        ▪ Electrogastrogram (EGG): Gastric myoelectrical activity (stomach responses to cybersickness)
        ▪ Electrocardiogram (ECG): Used to extract breathing rate for EGG signal correction
    ◦ Computed EGG Parameters:
        ▪ Mean Dominant Frequency (higher = greater sickness)
        ▪ Normal Gastric Ratio (lower = greater sickness)
        ▪ Tachy Gastric Ratio (higher = nausea indicator)
        ▪ Normal to Tachy Gastric Ratio (lower = greater sickness)
    ◦ Behavioral Metrics:
        ▪ Eye-tracking data (collected but not analyzed in detail)
Conclusion:
The study systematically examined cybersickness in XR using a controlled factorial experiment. It collected self-reported, physiological, and behavioral data to evaluate user experience and sickness susceptibility across different navigation conditions.","Fast Motion Sickness Scale,  SSQ (Simulator Sickness Questionnaire)",,EEG,,"Questionnaires,  Physiological"
ID069,Enhancing Positive Emotions through Interactive Virtual Reality Experiences: An EEG-Based Investigation,"Cheng  Shiwei,  Sheng  Danyi,  Gao  Yuefan,  Dong  Zhanxun,  Han  Ting",,10.1109/VR58804.2024.00042,2024,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This study investigates how interactive behaviors in a virtual museum environment influence users’ positive emotions, using EEG-based emotion recognition. It introduces an individualized EEG emotion classification model that achieved over 83% accuracy and uses this model to assess the emotional impact of four VR interaction tasks, varying in user autonomy and interaction type (e.g., no interaction, passive, active, and extensive). The results show that active and extensive (bidirectional) interactions significantly enhance positive affect and well-being, while passive or non-interactive tasks do not. These findings suggest that user autonomy and creative engagement are key to promoting positive emotional experiences in VR and provide actionable guidance for designing emotionally beneficial XR environments.","1) Participants
• Total Participants: 22 (13 male, 9 female)
• Age Range: 18 to 28 years (M = 23.0, SD = 3.45)
• VR Experience: 80% (N = 18) reported not having used VR before.
• Health Status: None had a history of mental disorders, and all had normal or corrected vision https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=19062,19643, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=18314,19061.
2) Study Design
• Type: Mixed design with two main factors:
    ◦ Degree of Autonomy: Restricted-Roaming Tasks vs. Free-Roaming Tasks.
    ◦ Interaction Behavior: Four types of tasks were designed:
        ▪ No Interaction (NI)
        ▪ Passive Interaction (PI)
        ▪ Active Interaction (AI)
        ▪ Extensive Interaction (EI) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=24970,25955, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=22972,23980.
3) Procedure
• Pre-Study: Collected EEG signals from participants 
in three discrete emotional states (positive, neutral, negative) to 
develop an individual emotion model https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=16638,17551, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=27582,28431.
• Experiment Setup: Participants were placed in a neutral scene for baseline EEG data collection before engaging in tasks within a virtual museum https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=19062,19643, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=789,1567.
• Task Execution: Participants were randomly assigned
 to either the Restricted-Roaming or Free-Roaming group, completing all 
four subtasks in randomized order https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=25956,26741, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=22016,22971.
• Post-Task Assessment: After each subtask, 
participants filled out the Positive and Negative Affect Schedule 
(PANAS) questionnaire and the Oxford Happiness Questionnaire https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=26742,27581, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=37665,38458.
4) Task
• Tasks: Each participant engaged in four subtasks categorized by interaction type and autonomy level:
    ◦ No Interaction (NI): Simple exploration without interaction.
    ◦ Passive Interaction (PI): Participants received guide tips while exploring.
    ◦ Active Interaction (AI): Participants could engage with the environment actively (e.g., doodles, emoticons).
    ◦ Extensive Interaction (EI): A combination of passive and active interactions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=24970,25955, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=22972,23980.
5) Metrics Collected
• EEG Data: Continuous recording of EEG data throughout the experiment to assess emotional responses https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=16638,17551, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=4423,5364.
• Questionnaires: 
    ◦ PANAS: Measured positive and negative affect after each task.
    ◦ Oxford Happiness Questionnaire: Assessed overall happiness before and after the experiment https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=55882,56476, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=26742,27581.
• Interviews: Conducted post-experiment to gather qualitative data on participants' emotional experiences and reflections https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=27582,28431, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681393bf7cb155bf8be8616b/?range=39095,39970.
This structured approach allowed the researchers to explore the 
relationship between interactive behaviors in virtual environments and 
their impact on participants' emotional states.","Oxford Happiness Questionnaire,  PANAS",,EEG,,"Questionnaires,  Physiological"
ID070,Entropy and Speed: Effects of Obstacle Motion Properties on Avoidance Behavior in Virtual Environment,"Patotskaya  Yuliya,  Hoyet  Ludovic,  Zibrek  Katja,  Pettre  Julien",,10.1145/3675231.3675236,2024,Symposium on Applied Perception,VR,Behavioural Dynamics & Exploration,,"Locomotion & Collision Avoidance, Navigation Behaviour","This study investigates how obstacle motion properties—speed and predictability (entropy)—affect users’ avoidance behaviour and trajectory adaptations during locomotion in virtual reality. Twenty-one participants walked across a virtual room, avoiding a moving cylindrical obstacle with varying motion characteristics (periodic vs. stochastic trajectories, with different speeds and levels of entropy).
Using Sample Entropy (SampEn) as a quantitative measure of obstacle motion predictability, the study introduces an entropy-based approach to characterize dynamic motion stimuli and relates it to proxemics and locomotor responses in VR.
Results show that both higher speed and higher entropy led participants to maintain larger interpersonal distances and more curved avoidance paths. Regression analysis confirmed a strong correlation between obstacle entropy and avoidance proximity (R² ≈ 0.87). Two distinct behavioural strategies were identified:
1. Avoiders — maintaining larger distances with increasing obstacle speed.
2. Anticipators — modulating movement timing based on perceived predictability.
This research proposes objective behavioural metrics for quantifying users’ perception of dynamic motion predictability and introduces entropy-based motion analysis as a tool to evaluate user adaptation and safety margins in VR navigation.","1) Participants• N = 21 (9 male, 12 female; mean age = 28 ± 7 years).• VR familiarity: 8 high, 7 medium, 6 none.• One excluded for misunderstanding the task.

2) Study Design• Type: Within-subjects repeated-measures design.
• Conditions: 12 total — 1 empty scene, 1 static obstacle, 10 moving obstacle conditions varying in speed (1.2–4.9 m/s)and entropy (periodic vs. stochastic).
• Repetitions: 3 per condition, randomized order.
• Task: Walk from point A to B (6 m apart) while avoiding the moving obstacle.

3) Apparatus & Stimuli
• Hardware: HTC Vive Pro with wireless adapter.
• Software: Unreal Engine 5.0.1.
• Stimuli: Cylindrical obstacle (0.5 m diameter) with motion trajectories generated using a Wiener process (Geometric Brownian Motion) to control entropy.
• Motion categories:
– Periodic (low/high frequency)
– Stochastic (low/high entropy)
– Noise-perturbed sinusoidal patterns
• Trajectory parameters: Sampled at 40 Hz, displacement limited to 1 m radius.

4) Metrics Collected
Behavioural / Trajectory Metrics:
• Relative to Scene Center:
– Average Distance, Clearance Distance, Maximum Lateral Deviation, Side-by-Side Distance.
• Relative to Obstacle:
– Average Distance to obstacle center, Side-by-Side Distance to obstacle (Instantaneous Distance at Crossing).
• General Metrics:
– Travel Time, Trajectory Length.
• Strategy Classification Metrics:
– Anticipators (speed modulation) vs. Avoiders (path deviation).
– Classification based on velocity drop and position thresholds.
Quantitative Motion Metric:
• Sample Entropy (SampEn): measures predictability of obstacle motion (m = 2–100, r = 0.1–0.3).

Main Findings
• Effect of Entropy and Speed:
– Both significantly increased avoidance distance (validated H1, H2).
– Participants maintained greater proximity margins as motion became faster or less predictable.
– Strong correlation between entropy and avoidance metrics (R² = 0.68–0.87).
• Behavioural Strategies:
– Avoiders maintained larger distances with speed.
– Anticipators adjusted timing to unpredictable motions.
– Strategy use shifted with experience — “avoiders” more common after familiarization.
• Entropy Modeling:
– SampEn provided interpretable quantification of motion predictability linked to participant responses.
– Identified optimal SampEn parameters (m = 84, r = 0.18) for predicting proximity metrics.
• Interpretation:
– Predictability affects how humans estimate collision risk.
– Entropy offers a useful measure to model human–object interaction and improve realism of autonomous virtual agents or crowds.
",,"Interpersonal Distance,  Entropy Analysis,  Movement Kinematics,  Movement Trajectories,  Interaction Time",,,Behavioural
ID071,Evaluating Hand-tracking Interaction for Performing Motor-tasks in VR Learning Environments,"Hameed  Asim,  Perkis  Andrew,  Moller  Sebastian",,10.1109/QoMEX51781.2021.9465407,2021,International Conference on Quality of Multimedia Experience,VR,Interaction Techniques & input Modalities,,Entertainment and Gaming,"This study compared hand-tracking vs. controller-based interaction in a VR reach-pick-place task using the Oculus Quest. Thirty-three participants completed the task under four conditions (two manipulation types × two representation levels). The study used game analytics (duration, object picks, clicks) and subjective workload (RSME) to evaluate performance and user experience. Results showed that controllers led to significantly better performance and lower mental workload across all conditions. Representation realism (photorealistic vs. non-photorealistic) had no significant effect. Despite higher immediacy and naturalism, hand-tracking resulted in poorer usability, likely due to system limitations and user familiarity.","Summary of Experiments:
1. Participants:
    ◦ Number of Participants: 33 participants (15 male, 18 female) with an average age of 24.7 years (±2.3).
    ◦ Demographics: All participants were active users of multimedia technologies, and most (18 out of 33) had prior experience with head-mounted displays (HMDs). Only 1 participant had prior experience with hand-tracking interactions.
    ◦ Recruitment: Participants were recruited via mailing lists and online forums over a two-week period.
2. Study Design:
    ◦ Objective: To compare the performance and user experience of two interaction modalities (hand-tracking vs. handheld controllers) in a VR learning environment.
    ◦ Experimental Conditions: The study used a within-subject design with four conditions, combining two Manipulation Types (M) and two Representation Levels (R):
        ▪ Manipulation Types (M):
            • M1: Hand-tracking.
            • M2: Handheld controllers.
        ▪ Representation Levels (R):
            • R1: Photorealistic environment.
            • R2: Non-photorealistic environment.
    ◦ Conditions:
        ▪ M1R1: Hand-tracking in a photorealistic environment.
        ▪ M1R2: Hand-tracking in a non-photorealistic environment.
        ▪ M2R1: Handheld controllers in a photorealistic environment.
        ▪ M2R2: Handheld controllers in a non-photorealistic environment.
3. Procedure:
    ◦ Pre-Experiment: Participants were given an introduction kit, including a consent form, and were briefed on the task. They performed a physical version of the task to familiarize themselves with it.
    ◦ Experiment Setup: Participants sat at a physical table that matched the virtual table in the VR environment. They used the Oculus Quest HMD to perform the task.
    ◦ Task Execution: Each participant performed the task under all four conditions in a randomized order. Each condition was a separate game run, and participants had a 5-7 minute break between runs to prevent fatigue and eye strain.
    ◦ Post-Task: After completing each task, participants rated their perceived mental effort using the Rated Scale Mental Effort (RSME) questionnaire.
4. Task:
    ◦ Task Description: Participants performed a reach-pick-place task in a VR environment. The task involved organizing 15 stationary items on a virtual table:
        ▪ 5 pens into a pen holder.
        ▪ 5 paperclips into a clip saucer.
        ▪ 2 business cards into a card holder.
        ▪ 1 mobile phone into a phone holder.
        ▪ 1 paper weight on top of a stack of paper.
        ▪ 1 coffee mug onto a coaster.
    ◦ Task Flow: Participants pressed a virtual ""start"" button to begin the task, organized the items, and pressed a ""stop"" button to end the task. After stopping, they completed the RSME questionnaire.
5. Metrics Collected:
    ◦ Performance Metrics (In-Game Analytics):
        ▪ Play Duration (AT): Time taken to complete the task (in seconds).
        ▪ Objects Picked (OP): Number of times objects were picked.
        ▪ Right Clicks (RC): Number of right-hand clicks.
        ▪ Left Clicks (LC): Number of left-hand clicks.
    ◦ Subjective Metrics:
        ▪ Perceived Mental Workload: Measured using the Rated Scale Mental Effort (RSME) questionnaire, where participants rated their mental effort on a scale after each task.
    ◦ Additional Data:
        ▪ Demographic information and prior experience with VR were collected via the ITC-SOPI questionnaire.
Summary:
The study involved 33 participants who performed a reach-pick-place task in a VR environment using two interaction modalities (hand-tracking and handheld controllers) across two representation levels (photorealistic and non-photorealistic). The experiment used a within-subject design, and participants performed the task under all four conditions in a randomized order. Performance metrics (play duration, objects picked, clicks) were collected through in-game analytics, and subjective mental workload was measured using the RSME questionnaire. The study aimed to compare the usability and user experience of hand-tracking versus handheld controllers in VR learning environments.",Rating Scale Mental Effort (RSME),,,"Completion Time,  Object Manipulation Performance","Questionnaires,  Performance"
ID072,Evaluating Object Manipulation Interaction Techniques in Mixed Reality: Tangible User Interfaces and Gesture,"Bozgeyikli  Evren,  Bozgeyikli  Lal Lila",,10.1109/VR50410.2021.00105,2021,Conference on Virtual Reality and 3D User Interfaces,MR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This study evaluates three object manipulation techniques in wearable MR—(1) space-multiplexed tangible cube, (2) time-multiplexed controller, and (3) hand gesture—across translation and rotation tasks, involving 42 participants. Results show that both tangible cube and controller methods outperformed hand gestures in terms of usability, efficiency, presence, and discoverability. The tangible cube and controller yielded similar performance, though each had context-specific trade-offs (e.g., physical effort vs. wrist constraint). The findings suggest that TUI-based interactions, especially tangible interfaces with physical affordances, enhance MR usability but also highlight challenges with tracking precision in high-fidelity tangible interfaces.","Summary of Experiments Conducted in the Paper
1) Participants
• Total participants: 42 (after excluding one due to task comprehension issues)
• Demographics:
    ◦ Age: 18–35 years (Mean = 23.55, SD = 4.915)
    ◦ Gender: 25 males, 17 females
    ◦ Mostly undergraduate and graduate students
• Inclusion criteria:
    ◦ Right-handed
    ◦ Hand span >6 inches (to hold the tangible cube comfortably)
    ◦ No eyeglasses, disabilities, or pregnancy
    ◦ English proficiency
    ◦ Minimal prior MR experience (<1 hour in total)
2) Study Design
• Within-subjects design (each participant tested all three interaction methods)
• Three interaction techniques evaluated:
    1. Tangible Cube (space-multiplexed TUI) – A physical cube with real-time projection mapping
    2. Controller (time-multiplexed TUI) – Magic Leap One controller used for object manipulation
    3. Hand Gesture – Mid-air pinch gestures for object manipulation
• Two object manipulation tasks:
    1. Translation task – Move a virtual cube into a target ring projection
    2. Rotation task – Align a specific face of the virtual cube with a colored target ring
• Additional real-world task:
    ◦ After each MR task, participants completed a writing task where they wrote a word on paper
3) Procedure
1. Participants arrived at the lab, signed consent forms, and completed a pre-experiment questionnaire (demographics, prior MR experience, motion sickness levels).
2. They were measured for eye height and assigned foot placement markers to ensure a fixed viewing distance.
3. A tutorial (including minimal instructions and pictographs) introduced each interaction method.
4. Participants performed object manipulation tasks (translation & rotation) using each interaction method in randomized order (Latin square design).
5. After each method, they completed an in-experiment user experience questionnaire.
6. After completing all interaction methods, they filled out a post-experiment questionnaire about overall experience and preference rankings.
4) Task
• Translation Task: Move the virtual cube inside a target ring projection.
• Rotation Task: Rotate the cube so that a specific face matches the color of the target ring.
• Writing Task: After completing the MR task, participants wrote a word starting with a letter displayed in MR.
• Submission Mechanism: Participants confirmed task completion via a gaze-based submission system (looking at a confirmation ring for 1.5 seconds).
5) Metrics Collected
✅ Automated Performance Metrics
• Task Completion Time (seconds)
• Placement Distance (accuracy of cube placement within target ring)
• Writing Time (time to complete the writing task)
• Information Time (time spent looking at tutorial instructions)
✅ User Experience Metrics (via Questionnaire)
• Usability & Efficiency (ease of use, intuitiveness, comfort, frustration)
• Presence (ITC-Sense of Presence Inventory: naturalness, spatial presence, engagement, liveliness)
• Cybersickness (motion sickness questionnaire: nausea, dizziness, fatigue)
• User Preferences (ranked choice of interaction method)
Key Findings
• Tangible cube and controller outperformed gestures in usability and efficiency.
• Gesture-based interaction had the highest completion time due to tracking limitations.
• Tangible cube had higher placement distance errors, likely due to its physical weight.
• Gesture-based interaction led to frequent tracking issues, reducing presence.
• Users preferred TUIs (tangible cube & controller) over gestures for MR interaction.
Conclusion
The study provides valuable insights into interaction techniques for MR, highlighting the advantages of tangible interfaces over mid-air gestures for object manipulation tasks.","Motion sickness questionnaire,  Presence – General",,,"Accuracy,  Completion Time","Questionnaires,  Performance"
ID073,Evaluating QoE in Collaborative Co-Located Training Using Heterogeneous AR Devices,"Moslavac  Mirta,  Vlahović  Sara,  Skorin-Kapov  Lea",,10.1109/QoMEX61742.2024.10598270,2024,International Conference on Quality of Multimedia Experience,AR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Telecommunications and Collaboration,"This paper evaluates Quality of Experience (QoE) and User Experience (UX) in a collaborative co-located AR cybersecurity training application used simultaneously on two heterogeneous AR devices:
• HMD (HoloLens 2 – OST)
• HHD (Android smartphone)
36 participants (18 pairs) completed two collaborative cybersecurity tasks (password cracking + password evaluation). Each pair used different devices at the same time, enabling the authors to compare UX across modalities.
Main results:
• HMD users reported significantly higher 3D perception and sense of physical presence.
• HHD users found button presses more intuitive, while
• HMD users found object translation more intuitive.
• Both devices: similar enjoyment, similar collaboration satisfaction, and similar shared-environment perception.
• Gesture use (pointing) significantly higher in HMD group → higher embodiment in OST AR.
• Visual distractions differed across devices: narrow FOV was a common limitation; rendering quality an issue for HMD (due to transparency).
Contribution:
The paper provides generalizable insights on how heterogeneous AR devices influence:
• interaction intuitiveness
• 3D object perception
• collaborative experience
• QoE factors across mixed AR hardware
This is strong, generalizable XR UX research, not application-bound.","Participants
• N = 36 (18 pairs, each pair uses two different devices)
• Age 20–30 (M = 23.47)
• 1/3 with no AR experience, 1/3 minimal, 1/3 moderate/expert
• Balanced gender split
Study Design
• Between-devices, within-pair
• Each pair performs two scenarios:
    ◦ S1: Password cracking
    ◦ S2: Password generation + rule evaluation
• Device assignment: one person HMD, one HHD
• Pre-experiment questionnaires: demographics, cybersecurity awareness
• Post-experiment: UX, collaboration, AR system comparison
Procedure
1. Introduction + consent
2. Assign devices
3. Training scenario (S0)
4. Scenario S1 (co-located collaborative task)
5. Scenario S2 (collaborative + object manipulation)
6. Post-study questionnaires
Apparatus
• HMD: HoloLens 2 (OST, gesture-based interaction)
• HHD: Huawei Mate 20 X (screen tap interactions)
• Platform: Unity + Photon networking + Azure Spatial Anchors
Metrics collected
Questionnaires – Custom + Adapted (78 items total, only subset analyzed)
• Mixed Likert, multiple choice, binary
• Cover:
    ◦ Virtual environment realism
    ◦ Perceived direct contact with virtual objects
    ◦ Perceived 3D dimensionality
    ◦ Device-specific visual distractions
    ◦ Intuitiveness of interactions (7 types)
    ◦ Collaboration ease (per scenario)
    ◦ Shared environment perception
    ◦ Enjoyment (QoE)
    ◦ Device-homogeneity perception
    ◦ Gesture usage (pointing)
(No standardized presence or usability questionnaires used, but structured and systematically adapted items.)
Performance Metrics
• None directly (focus is UX, QoE)
Qualitative Metrics
• User comments on collaboration, device homogeneity
• Free-text about distractions, use of gestures",Custom made,,,,Questionnaires
ID074,Evaluating Remote Virtual Hands Models on Social Presence in Hand-based 3D Remote Collaboration,"Yoon  Boram,  Kim  Hyung-il,  Oh  Seo Young,  Woo  Woontack",,10.1109/ISMAR50242.2020.00080,2020,International Symposium on Mixed and Augmented Reality (ISMAR),"AR, VR",Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,Entertainment and Gaming,"This study examines user experience and task performance in remote asymmetric XR collaboration, comparing three interface types: AR on tablet (ARTab), VR on HMD (VRHMD), and a hybrid Mixed condition where participants use both. Thirty participants completed a collaborative furniture layout task in dyads using different role configurations. Results show that the Mixed condition led to better usability and task performance, while AR-only setups caused higher workload and reduced presence. The study concludes that asymmetric XR setups can offer enhanced remote collaboration if interaction modalities are thoughtfully combined.","Summary of the Experiments Conducted:
1. Participants:
    ◦ Total Participants: 48 participants (22 females, 26 males).
    ◦ Age Range: 19 to 36 years (Mean age = 24.58, SD = 3.65).
    ◦ Recruitment: Participants were recruited in pairs (10 male-male, 8 female-female, and 6 female-male pairs) through a university online community. Pairs were selected to have a close relationship (e.g., friends, family, or colleagues) to avoid negative feelings between strangers.
    ◦ Familiarity with XR: Participants had limited experience with VR/AR (Mean = 2.56, SD = 1.64) and hand-gesture interaction in VR/AR (Mean = 2.15, SD = 1.64).
2. Study Design:
    ◦ Design: Mixed factorial design with a within-subject variable (Virtual Hands Type) and a between-subject variable (3D Environment Type).
    ◦ Virtual Hands Type: Three levels (Skeleton, Low Polygon, Realistic).
    ◦ 3D Environment Type: Two levels (AR and VR).
    ◦ Conditions: Six experimental conditions derived from the combination of the two factors (3 hands types × 2 environments).
    ◦ Pairing: Each pair of participants consisted of one participant in the AR condition and the other in the VR condition.
3. Procedure:
    ◦ Pre-Experiment: Participants filled out a demographic questionnaire and were briefed on the study's purpose and tasks. They were trained in American Sign Language (ASL) and practiced the task face-to-face.
    ◦ Main Experiment: Participants were randomly assigned to either the AR or VR condition. The AR participant wore a Meta 2 Optical See-Through (OST) HMD, while the VR participant wore an HTC Vive HMD. Both participants used Leap Motion for hand tracking.
    ◦ Task Execution: The main task involved solving ASL word quizzes collaboratively. Participants alternated roles as the quizzer (giving the quiz) and solver (guessing the answer). Each pair completed 30 quizzes (10 per virtual hands condition).
    ◦ Post-Task: After each condition, participants completed questionnaires measuring social presence, presence, trust, likability, and mental effort. A post-experiment interview was conducted after all conditions were completed.
4. Task:
    ◦ Main Task: ASL Word Quiz Solving. Participants collaborated to solve word quizzes using ASL hand gestures. The quizzer posed ASL letters and numbers, and the solver guessed the word based on the hand gestures.
    ◦ Sub-Tasks: Handshake, rock-paper-scissors, and high-five were included at the beginning and end of the main task to encourage hand interaction.
    ◦ Task Structure: Each pair completed 10 quizzes per virtual hands condition (30 quizzes in total). The order of conditions was randomized using a Latin Square design to avoid ordering effects.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Social Presence: Measured using Harms and Biocca's Social Presence (HSP), Bailenson's Social Presence (BSP), and Nowak's Social Presence (NSP) questionnaires.
        ▪ Presence: Measured using Witmer and Singer's Presence Questionnaire (PQ).
        ▪ Trust: Measured using a trust questionnaire derived from previous studies.
        ▪ Likability: Measured using a single question about the appearance of the remote partner's virtual hands.
        ▪ Mental Effort: Measured using the Subjective Mental Effort Questionnaire (SMEQ).
    ◦ Performance Metrics:
        ▪ Task Completion Time: The time taken to complete each quiz was recorded as an objective measure of task performance.
    ◦ Qualitative Data:
        ▪ Post-Experiment Interview: Participants provided feedback on their experience with each virtual hands model, including social presence, ease of understanding, and preference.
Summary:
The study involved 48 participants in pairs, with one in AR and the other in VR, performing collaborative ASL word quizzes using three different virtual hand models (skeleton, low polygon, realistic). The study collected both subjective (questionnaires) and objective (task completion time) metrics to evaluate the impact of virtual hand representations on social presence, trust, and task performance. The findings provide insights into how different levels of realism in virtual hands affect user experience in MR remote collaboration.","Post-Experiment Interview,  Presence – General,  Social Presence,  Subjective Mental Effort Questionnaire",,,Completion Time,"Questionnaires,  Performance"
ID075,Evaluating the Effect of Binaural Auralization on Audiovisual Plausibility and Communication Behavior in Virtual Reality,"Immohr  Felix,  Rendle  Gareth,  Lammert  Anton,  Neidhardt  Annika,  Heyde  Victoria Meyer Zur,  Froehlich  Bernd,  Raake  Alexander",,10.1109/VR58804.2024.00104,2024,Conference on Virtual Reality and 3D User Interfaces,VR,Embodiment Avatars & Social Presence,Content & System Design.,Telecommunications and Collaboration,"This paper investigates the effect of spatial vs. non-spatial (diotic) audio on social presence, plausibility, communication behavior, and task performance in a collaborative VR task. Participants (in dyads) performed a shape-comparison task under different audio and spatial arrangement conditions. Results showed no significant effects of auralization method on presence or performance in the communication task, though users preferred spatial audio in a dedicated active listening test. Scene layout, rather than audio method, impacted communication structure and behavior (e.g., mutual silence states and movement patterns). The study contributes a novel VR conversation paradigm, a custom plausibility questionnaire, and insights into when spatial audio matters in social VR.","
1. Participants:
    ◦ Pre-test: 9 dyads (18 participants) were used to verify the setup, testing approach, and duration. These participants were excluded from the main study.
    ◦ Main test: 16 dyads (32 participants) participated in the main study. The participants were aged 21-39 (M=26.94, SD=4.02), with 23 males and 9 females. Nine dyads were mixed-gender, while seven consisted of two males. Participants were recruited from the university body, and some were familiar with each other (classmates, colleagues, or friends).
2. Study Design:
    ◦ The study followed a two-by-two factorial within-subject design, with two independent variables:
        ▪ Auralization method: Binaural spatial audio (SPATIAL) vs. diotic audio (DIOTIC).
        ▪ Scene arrangement: Distributed (DIST) vs. shadowing (SHAD).
    ◦ This resulted in four conditions: SPATIAL_DIST, DIOTIC_DIST, SPATIAL_SHAD, and DIOTIC_SHAD.
    ◦ The study was divided into two parts:
        ▪ Conversation test: Participants performed a collaborative task in VR under the four conditions.
        ▪ Active listening preference test: Participants directly compared the two auralization methods (SPATIAL vs. DIOTIC) to indicate their preference.
3. Procedure:
    ◦ Pre-test: Participants filled out a consent form and demographic survey, including questions about familiarity with their conversation partner, prior VR experience, and hearing abilities. Visual acuity and color vision were tested using Snellen and Ishihara charts.
    ◦ Training phase: Participants explored the virtual environment and performed a simplified version of the task to familiarize themselves with the setup.
    ◦ Conversation test: Participants performed the task in four trials, each corresponding to one of the four conditions. After each trial, they completed a post-trial questionnaire.
    ◦ Active listening preference test: Participants were placed in the same virtual environment and assigned roles (describer or listener). The listener compared the two auralization methods and indicated their preference.
    ◦ Post-test interview: An informal interview was conducted after the active listening test.
4. Task:
    ◦ Conversation test: Participants performed a collaborative task adapted from the Leavitt task, originally designed for telemeeting assessment. The task required participants to identify differences in shapes displayed on cubes distributed throughout the virtual room. Each participant could only see their own set of shapes and had to communicate with their partner to find differences in color, orientation, or form. Participants marked the differing shapes by touching them with their virtual hand. The task encouraged movement and spatial awareness, as participants had to navigate the room to compare shapes.
    ◦ Active listening preference test: Participants compared the two auralization methods (SPATIAL vs. DIOTIC) while one participant described their set of shapes, and the other listened and moved freely. The listener indicated their preferred auralization method.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Social presence: Measured using the Networked Minds Social Presence Inventory (NM-SPI), which included subscales for co-presence, perceived message understanding, and mutual assistance.
        ▪ Plausibility: A custom questionnaire with 21 items assessed task difficulty, enjoyment, interaction, audiovisual quality, and coherence.
    ◦ Behavioral Metrics:
        ▪ Movement patterns: Head rotation and translation speed were analyzed to understand exploration behavior.
        ▪ Gestural behavior: Hand movement speed was analyzed to assess non-verbal communication.
    ◦ Performance Metrics:
        ▪ Task completion time: The time taken to complete the task in each trial was recorded and analyzed.
    ◦ Conversation Analysis:
        ▪ Conversational structure: Parametric conversation analysis was used to analyze turn-taking patterns, including states such as silence, single talk, and double talk.
    ◦ Active Listening Preference:
        ▪ Participants indicated their preferred auralization method (SPATIAL or DIOTIC) in the active listening test.
Summary:
The study involved 32 participants in a within-subject design with four conditions, combining different auralization methods and scene arrangements. Participants performed a collaborative task in VR, followed by an active listening preference test. Metrics included questionnaires (social presence, plausibility), behavioral metrics (movement, gestures), performance metrics (task completion time), and conversational analysis. The study aimed to understand the impact of spatial audio on user experience and communication behavior in VR.","Custom made - Plausibility,  Social Presence","Hand Movement,  Movement Trajectories,  Communication Analysis",,Completion Time,"Questionnaires,  Behavioural,  Performance"
ID076,Evaluating the User Experience of a Photorealistic Social VR Movie,"Li  Jie,  Subramanyam  Shishir,  Jansen  Jack,  Mei  Yanni,  Reimat  Ignacio,  Lawicka  Kinga,  Cesar  Pablo",,10.1109/ISMAR52148.2021.00044,2021,International Symposium on Mixed and Augmented Reality (ISMAR),VR,Embodiment Avatars & Social Presence,Content & System Design.,Entertainment and Gaming,"This paper investigates user experience in a photorealistic social VR movie by comparing user responses between HMD-based and 2D screen-based setups. Through a 48-user study and two expert focus groups, it evaluates presence, social interaction, workload, visual quality, and immersion. The study finds that HMD users report significantly higher presence and immersion but also experience higher task load and restricted exploration. Screen users benefit from greater freedom of navigation but lower engagement. Both user groups and experts highlight the impact of photorealistic point cloud representations on co-presence and express interest in improved narrative interaction and system fidelity. The paper makes a generalizable contribution by offering metrics and qualitative insights into immersive co-watching experiences.","The virtual movie invites four users to join in simultaneously, who form the Civilian Oversight
Committee as the witnesses of the crime solving process, and allows
interaction with the movie characters to help with the process. Users can access to the virtual movie either by using an HMD or
using a screen with a game controller.","NASA-TLX,  Presence – General,  SSQ (Simulator Sickness Questionnaire),  Social VR,  Visual Quality Rating,  Unstructured Interviews",Movement Trajectories,,,"Questionnaires,  Behavioural"
ID077,Evaluating the User in a Sound Localisation Task in a Virtual Reality Application,"Moraes  Adrielle Nazar,  Flynn  Ronan,  Hines  Andrew,  Murray  Niall",,10.1109/QoMEX48832.2020.9123136,2020,International Conference on Quality of Multimedia Experience,VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"The study presents a VR-based sound localization task inspired by the LISN test and evaluates two interaction modalities: eye gaze pointing and wand-based selection. Participants completed 72 trials under three auditory conditions (target only, co-located distractor, and spatially separated distractor). Key findings show both methods yielded comparable usability and performance, though gaze pointing led to lower perceived time pressure. Participants reported high presence and low frustration. Physiological arousal (EDA) increased across phases, indicating engagement and immersion. The system has implications for healthcare assessments such as Central Auditory Processing Disorder (CAPD) testing.","1) Participants
• Number: 20 adults (12 male, 8 female)
• Age: Average 29 years old
• VR Experience: 6 participants were VR-naïve
• Screening: Hearing/vision tests conducted beforehand
2) Study Design
• Between-subjects comparison:
    ◦ 10 participants used eye gaze pointing (GP)
    ◦ 10 used wand pointing (WP)
• Three test phases of increasing difficulty:
    1. Target sound only
    2. Target + same-location distractor
    3. Target + 90°-offset distractor
• 24 trials per phase (72 total per participant)
3) Procedure
1. Pre-test screening (hearing/vision)
2. 5-minute baseline physiological measurement
3. VR training session
4. Main experiment (three phases)
5. Post-test questionnaires:
    ◦ NASA-TLX (workload assessment)
    ◦ Custom QoE questionnaire
4) Task
• Primary task: Identify which of 24 equally-spaced spheres (15° apart) emitted a 2.5s white noise sound (55 dB SPL)
• Interaction methods:
    ◦ GP: Select sphere by looking at it + controller click
    ◦ WP: Select with controller laser pointer + click
• Distractors: Low-frequency tones in phases 2-3
5) Metrics Collected
Explicit (Questionnaires):
• NASA-TLX (6 workload dimensions)
• Custom QoE survey (usability, presence, interaction)
Behavioral:
• Head movement (HMD yaw rotation)
• Gaze direction (Tobii eye tracking)
• Selection accuracy
• Task completion time
Physiological:
• Electrodermal activity (EDA)
• Heart rate (via Empatica E4 wristband)
Performance:
• Localization error angle (degrees)
• Front-back confusion rate
• Learning effects over trials
Key features of the experimental design include its focus on comparing interaction methods while systematically increasing task difficulty, and its comprehensive multimodal data collection approach combining traditional questionnaires with behavioral and physiological metrics. The study maintained ecological validity by using a wireless VR setup that allowed natural head movements.","Custom made,  Interaction,  NASA-TLX,  Presence – General,  Usability – Custom / Rating-based",Head Analysis,"EDA / GSR (Skin Conductance),  Eye-Tracking",Completion Time,"Questionnaires,  Behavioural,  Physiological,  Performance"
ID078,"Evaluating User Performance, Workload, and Presence of Virtual Reality Questionnaires Using Joystick and Raycasting Selection Techniques","Wei  Xingbo,  Li  Yue",,10.1145/3603421.3603426,2023,International Conference on Virtual and Augmented Reality Simulations,VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This paper evaluates how different VR selection techniques (joystick vs. raycasting) influence user performance, workload, and presence when completing questionnaires within VR. Using a controlled study with 16 participants, it finds that while VR-based questionnaires reduce break in presence, PC-based completion yields better performance and lower workload. Raycasting slightly outperforms joystick in workload for simple tasks, but joystick is preferred for precise slider inputs. Results suggest no one-size-fits-all technique, but provide empirical design recommendations for in-VR questionnaires, especially in balancing precision, effort, and engagement.","
1. Participants:
    ◦ Number: 16 participants (8 males, 8 females).
    ◦ Age: Mean age of 22.25 years (SD = 1.25).
    ◦ Familiarity with VR: Slightly familiar with VR (M = 2.44 on a 5-point Likert scale, SD = 1.257).
    ◦ Familiarity with 3D technology: Familiar (M = 3.611 on a 5-point Likert scale, SD = 1.11).
    ◦ Vision: All participants had normal or corrected-to-normal vision.
    ◦ Simulator Sickness: No participants reported simulator sickness during the experiments.
2. Study Design:
    ◦ Design: Within-subjects design.
    ◦ Conditions: Three selection techniques were evaluated:
        ▪ Joystick selection in VR.
        ▪ Raycasting selection in VR.
        ▪ Point & select technique on a PC.
    ◦ Question Types: Three question types were used:
        ▪ Radio questions (5-point Likert scale).
        ▪ Block questions (multiple-choice with 5 options).
        ▪ Slider questions (numerical scale from 0 to 100).
    ◦ Balancing: A Latin square design was used to balance the order of conditions across participants.
3. Procedure:
    ◦ Introduction: Participants were welcomed, provided consent, and given a brief introduction to VR technology and the Meta Quest 2 device.
    ◦ Tutorial: Participants were given a tutorial to familiarize themselves with the joystick and raycasting selection techniques in VR (see Figure 2 in the paper).
    ◦ Experimental Sessions: Each participant attended three sessions, one for each selection technique (joystick, raycasting, and PC). In each session, participants completed:
        ▪ 30 Simple Selection Tasks (SSTs): 10 trials for each question type (radio, block, and slider).
        ▪ Workload Assessment: After completing the SSTs, participants filled out the NASA Task Load Index (NASA-TLX) in VR to assess their perceived workload.
        ▪ Presence Assessment: Participants also completed the Slater, Usoh, and Steed (SUS) presence questionnaire to measure their sense of presence in VR.
    ◦ Interview: At the end of the experiment, participants were interviewed to gather subjective preferences and feedback on the selection techniques and question types.
    ◦ Duration: The experiment took approximately 35 minutes on average.
4. Task:
    ◦ Simple Selection Tasks (SSTs): Participants completed a series of simple selection tasks, which included:
        ▪ Find and select (number): Choose a specific number from a list.
        ▪ Find and select (letter): Identify a letter contained in a word.
        ▪ Find and select (word): Identify the color of an object (e.g., ""What is the color of a banana?"").
        ▪ Count and select: Count the number of countries mentioned in a short text.
        ▪ Calculate and select: Solve a simple arithmetic problem (e.g., ""45 + 29 = ?"").
    ◦ Question Types: Each SST was presented in three formats:
        ▪ Radio questions: 5-point Likert scale.
        ▪ Block questions: Multiple-choice with 5 options.
        ▪ Slider questions: Numerical scale from 0 to 100.
5. Metrics Collected:
    ◦ Task Performance:
        ▪ Task Completion Time: The time taken to complete each SST was recorded using a built-in C# script.
    ◦ Workload:
        ▪ NASA-TLX: Participants filled out the NASA Task Load Index (NASA-TLX) in VR after completing the SSTs. The NASA-TLX measures six dimensions of workload: mental demand, physical demand, temporal demand, performance, effort, and frustration.
    ◦ Presence:
        ▪ SUS Questionnaire: Participants completed the Slater, Usoh, and Steed (SUS) presence questionnaire to measure their sense of presence in VR. The SUS consists of 6 items on a 7-point Likert scale.
    ◦ Subjective Preferences:
        ▪ Interview: Participants were asked about their preferences for completing questionnaires (VR vs. PC) and their preferred selection technique for each question type (joystick vs. raycasting).
Summary:
The study involved 16 participants who completed a series of simple selection tasks (SSTs) using three different selection techniques (joystick, raycasting, and PC) and three question types (radio, block, and slider). Task performance was measured by completion time, while workload and presence were assessed using the NASA-TLX and SUS questionnaires, respectively. The study aimed to compare user performance, workload, and presence across different selection techniques and question types in VR, with a focus on understanding how these factors influence user experience in VR questionnaires.","NASA-TLX,  Presence – General,  Semi-structured Interviews",,,Task Success / Completion,"Questionnaires,  Performance"
ID079,Evaluating Visual Attention and QoE for 360° Videos with Non-Spatial and Spatial Audio,"Hirway  Amit,  Qiao  Yuansong,  Murray  Niall",,10.1145/3625468.3652916,2024,Multimedia Systems Conference (MMSys),360 video,Content & System Design,Behavioural Dynamics & Exploration.,Entertainment and Gaming,"This study evaluates how different audio spatialization techniques (stereo, first-order, third-order ambisonics) affect QoE and visual attention in immersive 360° videos. Using a between-subjects design (n=73), the authors collect subjective data (via a 20-item questionnaire using ACR) and objective data (eye tracking, head pose, heart rate, pupil dilation). Preliminary findings indicate that third-order ambisonics significantly enhance immersion, emotional engagement, and attention guidance. Spatial audio influenced gaze distribution and physiological arousal, suggesting that complex soundscapes can be used to direct attention and improve immersive media delivery.","1) Participants
• Total: 80 participants initially recruited; 7 excluded (visual or auditory issues), leaving 73 participants for analysis.
• Gender Distribution: 48 men, 32 women.
• Age Range: 14 to 62 years (Mean = 28.54 years).
• VR Experience:
    ◦ 26 had prior VR experience.
    ◦ 26 were newcomers.
    ◦ 21 unspecified.
• Sampling Method: Convenience sampling from a university campus.
• Participant Groups:
    ◦ 4 different audio conditions (no sound, first-order ambisonics, third-order ambisonics, stereo sound).
    ◦ ~20 participants per audio condition.
2) Study Design
• Between-subjects design: Each participant was assigned to one of four audio conditions (No sound, Stereo, First-order Ambisonics, Third-order Ambisonics).
• Controlled lab experiment conducted in a 5m × 6m VR-equipped room.
• Diverse 360° video content used to ensure ecological validity.
• Mixed-methods approach:
    ◦ Subjective questionnaires (self-reported experience).
    ◦ Objective physiological and behavioral tracking (eye-tracking, gaze analysis, heart rate, pupil dilation).
3) Procedure
• Total Duration: 40–50 minutes per participant.
• Conducted in five phases:
1. Information Phase (10 min):
    ◦ Participants received details about the study.
    ◦ Consent obtained.
2. Screening Phase (10 min):
    ◦ Visual and auditory acuity tests (Snellen eye chart, Ishihara color perception test, standard auditory test).
3. Training Phase (5 min):
    ◦ VR environment introduction via a 60-second 360° video.
    ◦ Calibration of gaze-tracking system.
4. Testing Phase (10 min):
    ◦ Participants watched two 300-second (5-minute) 360° videos (one indoor, one outdoor).
    ◦ Each video was played with one of four audio conditions (no sound, stereo, first-order ambisonics, third-order ambisonics).
    ◦ Participants sat on a swivel chair to allow full 360° exploration.
5. Questionnaire Phase (5–10 min):
    ◦ Participants rated their experience using a subjective questionnaire on immersion, presence, and sound spatiality.
4) Task
• Task: Watch two 360° videos (one indoor, one outdoor) while their visual attention and physiological responses were recorded.
• 360° Video Content:
    ◦ Indoor videos: Opera performance with actors and an orchestra.
    ◦ Outdoor videos: Exploration-based scenes with ambient sounds (clock towers, musicians, pedestrians, dogs, ducks).
    ◦ Technical Specs: 29.970 FPS, YUV color space, 8-bit depth, 4:2:0 chroma subsampling.
    ◦ Audio Types Tested:
        ▪ No sound (baseline).
        ▪ Stereo sound (2 channels).
        ▪ First-order ambisonics (4 channels).
        ▪ Third-order ambisonics (16 channels, most immersive).
5) Metrics Collected
(A) Subjective Measures
• Self-reported questionnaire using a five-point Likert scale.
• Measured factors:
    ◦ Quality of Experience (QoE) (immersion, engagement, satisfaction).
    ◦ Presence and spatial audio perception.
    ◦ Emotional response to different audio conditions.
(B) Objective Measures
• Behavioral Metrics:
    ◦ Eye-tracking & gaze analysis → Measures where participants focus their attention.
    ◦ Head pose tracking → Captures movement patterns in the 360° environment.
• Physiological Metrics:
    ◦ Heart rate (HR): Indicates arousal and emotional engagement.
    ◦ Pupil dilation: Measures cognitive load and immersion levels.
(C) Performance Metrics
• Engagement levels: Analyzed based on gaze duration and attention shifts under different audio conditions.
Key Findings from the Experiment
• Spatial audio (third-order ambisonics) significantly enhanced immersion.
• Participants with spatial audio showed higher heart rates and pupil dilation, indicating stronger engagement.
• Spatial audio encouraged more diverse gaze fixations, meaning it helped direct attention better than non-spatial audio.
• Higher sound complexity correlated with increased arousal (third-order ambisonics induced the highest heart rate).
• Subjective reports confirmed that participants found spatial audio more immersive and natural.
This study provides strong empirical evidence that spatial audio enhances immersive user experience in XR, supporting visual attention guidance, emotional engagement, and overall QoE.","Custom made,  Presence – General,  QoE - ACR",Head Analysis,"Eye-Tracking,  Heart Rate,  Pupil Analysis",,"Questionnaires,  Behavioural,  Physiological"
ID080,Event Related Brain Responses Reveal the Impact of Spatial Augmented Reality Predictive Cues on Mental Effort,"Volmer  Benjamin,  Baumeister  James,  Von Itzstein  Stewart,  Schlesewsky  Matthias,  Bornkessel-Schlesewsky  Ina,  Thomas  Bruce H.",,10.1109/TVCG.2022.3197810,2023,Transactions on Visualization and Computer Graphics,AR,Visualization Techniques,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This study explores the cognitive effects of spatial augmented reality (SAR) cues in a predictive maintenance task. Participants were guided through different machine inspection conditions (no cue, text-only, SAR cue) while EEG was recorded. Behavioral (task performance) and neurophysiological (ERP and frontal theta) responses showed that SAR cues significantly reduced mental effort compared to no cue and outperformed text-only cues. Notably, higher P3 amplitudes and lower frontal theta power were associated with SAR, indicating more efficient cognitive processing. The study demonstrates that EEG-based metrics can reveal subtle differences in user workload not captured by behavior alone.","1. Participants
• Sample Size: 23 participants (2 female, 21 male).
• Age Range: 21 to 39 years (M = 26.87, SD = 5.32).
• Inclusion Criteria: Right-handed, healthy adults 
aged 18-40, normal (or corrected-to-normal) vision, no psychiatric or 
hearing impairments, no intellectual or physical impairments, no history
 of dyslexia or epilepsy, and no recreational drug use in the past six 
months https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=25118,25922 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=24215,25117.
2. Study Design
• The study employed a repeated-measures design with three main tasks:
    1. N-back Task: A cognitive task to assess memory load.
    2. Procedural Task: Comparison of instructions presented via a traditional monitor (MON) versus Spatial Augmented Reality (SAR).
    3. Predictive Cues Task: Evaluation of different SAR predictive cues https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=5667,6514, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=66165,66984.
3. Procedure
• Participants underwent an EEG setup before the experiments began. 
They completed resting-state EEG recordings, followed by the tasks in a 
randomized order. Each task included a training session to familiarize 
participants with the conditions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=25923,26631, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=3391,4282.
• The experiment adhered to strict COVID-19 protocols, including mask-wearing and social distancing https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=4283,4929 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=31244,32047.
4. Task
• N-back Task: Participants responded to a sequence 
of letters, indicating whether the current letter matched the one 
presented 'n' letters ago (1-back, 2-back, or 3-back conditions).
• Procedural Task: Participants pressed buttons on a 
dome-shaped device based on instructions displayed either on a monitor 
or projected onto the dome.
• Predictive Cues Task: Participants performed the 
same procedural task but with different types of predictive cues (e.g., 
ARC, ARROW, LINE, BLINK, COLOR) to guide their actions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=66165,66984 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=60332,60907.
5. Metrics Collected
• Behavioral Metrics: Response times, errors, and self-assessed mental effort (rated on a scale from 1 to 9) were recorded for each task.
• Physiological Metrics: EEG data were collected to 
measure cognitive load through event-related potentials (ERPs), 
specifically the mismatch negativity (MMN) and P3 components, in 
response to an auditory oddball task https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=20759,21575, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d940e93f6d2e0d50b7caa6/?range=3391,4282.
This structured approach allowed the researchers to investigate the 
cognitive load associated with different instructional methods in XR 
environments, providing valuable insights into user experience and 
performance.",Self-assessed Mental Effort,,EEG,"Error Rate,  Response Time","Questionnaires,  Physiological,  Performance"
ID081,"Examining Pair Dynamics in Shared, Co-located Augmented Reality Narratives","Connor  Cherelle,  Schoenborn  Eric Cade,  Hu  Sathaporn,  Porcino  Thiago Malheiros,  Moore  Cameron,  Reilly  Derek,  Lages  Wallace S",,10.1145/3677386.3682091,2024,Symposium on Spatial User Interaction,AR,Embodiment Avatars & Social Presence,,Entertainment and Gaming,"This paper investigates how pairs of users experience shared, co-located augmented reality (AR) narratives using Microsoft HoloLens 2. The authors designed three original AR stories—Phenomenal Things, Sentiments, and Spill—spanning comedy, drama, and fantasy, each with different interaction styles, spatial layouts, and levels of complexity. Forty-two participants experienced all three narratives either alone or in pairs, and the study collected both quantitative measures (an adapted Immersive Experience Questionnaire and a custom Shared Narrative Questionnaire) and qualitative interview data to understand how people coordinate, communicate, and move together during shared AR storytelling. The findings reveal how pairs negotiate physical and virtual space, how interaction complexity influences communication, how spatial audio and visuals shape orientation, and how narrative structure affects collaborative engagement. The paper identifies five themes describing pair dynamics and proposes design recommendations for multi-user AR narrative experiences.","1. Participants
• A total of 42 participants took part in the study.
• 34 participants participated in pairs (17 pairs).
• 8 participants completed the study individually due to scheduling or COVID-19 issues.
• Participants had varying levels of AR familiarity, from novice to expert; 6 participants rated their AR expertise as average or above.
• 8 pairs already knew each other.
• Participants were recruited via the university’s mailing list at Virginia Tech.

2. Study Design
• The study followed a within-subjects design.
• All participants experienced three different AR narratives (“Phenomenal Things,” “Sentiments,” “Spill”).
• The narratives varied in:
    ◦ Genre (comedy, drama, fantasy)
    ◦ Interaction complexity (low, high, medium)
    ◦ Spatial presentation (characters around edges, partial scene renderings, full room occlusion)
• The within-subject design was chosen to compare participants’ responses across diverse narrative styles.

3. Procedure
1. Arrival & Setup
    ◦ Participants received a verbal description of the study and provided informed consent.
    ◦ Pairs were asked whether they knew each other.
    ◦ HoloLens 2 headsets were fitted and calibrated (visual markers + Azure spatial anchors).
2. AR Narrative Sessions
    ◦ Participants experienced all three AR narratives in a large 40×32 ft theater space.
    ◦ Order was not fully counterbalanced, but most participants progressed from low → high interaction complexity:
        1. Phenomenal Things
        2. Sentiments
        3. Spill
    ◦ After each narrative, participants moved to a quiet area to complete questionnaires.
3. Post-Narrative Measures
    ◦ After each story, participants completed:
        ▪ The adapted Immersive Experience Questionnaire (IEQ)
        ▪ The Shared Narrative Questionnaire
    ◦ After finishing all three stories, semi-structured interviews were conducted with each pair (or individual for solo participants).

4. Tasks
Participants engaged in interactive AR storytelling sessions where the tasks depended on narrative structure:
Phenomenal Things (Comedy)
• Main mechanic: proxemic interaction (“befriending” characters by moving close).
• Characters arranged around the edges of the space.
• Movement optional; story progresses even if players do not interact.
Sentiments (Drama)
• Interaction-heavy narrative with object manipulation (e.g., turning off alarm, cutting tomatoes, picking up keys).
• Scenes presented in different areas of the space, requiring reorientation.
• Some scenes required sequential actions (pick keys → open door).
Spill (Fantasy)
• Task involved eavesdropping, detecting keywords, and progressing via voice commands.
• Required interactions included:
    ◦ Picking up a lockpick
    ◦ Opening a door
    ◦ Speaking a code phrase in the correct location
• Numerous optional interactable objects created a party-like environment.

5. Metrics Collected
Questionnaires
• Immersive Experience Questionnaire (IEQ), adapted
    ◦ Measures: emotional involvement, cognitive involvement, real-world dissociation, challenge, control.
• Shared Narrative Questionnaire – Custom made
    ◦ Dimensions:
        ▪ Narrative perception: engagement with characters, story comprehension
        ▪ Interaction understanding: clarity of interaction mechanics
        ▪ Shared experience: communication with partner, consistency, ease of sharing
Qualitative Measures
• Semi-structured interviews including questions on:
    ◦ Understanding of each story
    ◦ Awareness of partner
    ◦ Communication behaviour
    ◦ Distractibility in shared AR
Behavioural Observations (qualitative only)
• Spatial movement patterns
• Avoidance of virtual walls and characters
• Partner avoidance and collision prevention
• Orientation based on spatial audio","shared narrative,  IEQ (Immersion Experience Questionnaire),  Semi-structured Interviews",,,,Questionnaires
ID082,Exploiting Object-of-Interest Information to Understand Attention in VR Classrooms,"Bozkir  Efe,  Stark  Philipp,  Gao  Hong,  Hasenbein  Lisa,  Hahn  Jens-Uwe,  Kasneci  Enkelejda,  Gollner  Richard",,10.1109/VR50410.2021.00085,2021,Conference on Virtual Reality and 3D User Interfaces,VR,Behavioural Dynamics & Exploration,Visualization Techniques.,Education and Training,"This paper investigates how different virtual classroom design manipulations affect students' visual attention during VR-based lectures. The study involved 280 children aged 10–13 using HMDs with eye tracking. Three factors were varied: seating position (front/back), avatar visualization style (cartoon/realistic), and the percentage of virtual peers raising hands (20–80%). Eye-tracking data was processed with ray-casting and a minimum attention threshold to compute object-of-interest (OOI) attention times. Results show that avatar style, seat position, and social behavior cues significantly affect attention allocation to peer-learners, instructors, and content. These findings highlight how virtual classroom designs can be optimized to modulate attention and enhance learning experience.","
1. Participants
• Total Participants: 381 sixth-grade students (179 female, 202 male), aged between 10 to 13 years (M = 11.5, SD = 0.6).
• Final Sample: Data from 101 participants were 
removed due to hardware issues, resulting in a final sample of 280 
participants (140 female, 140 male) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=13207,14013 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=41563,42497.
2. Study Design
• Design Type: Between-subjects design.
• Factors Manipulated:
    ◦ Sitting positions (front vs. back of the classroom).
    ◦ Visualization styles of virtual avatars (cartoon vs. realistic).
    ◦ Hand-raising percentages of virtual peer-learners (20%, 35%, 65%, and 80%) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=6448,7286, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=25522,26375.
3. Procedure
• Participants were introduced to the experiment and allowed to familiarize themselves with the VR environment and hardware.
• Eye tracking was calibrated before the experiment began.
• The virtual lecture lasted approximately 15 minutes, divided into 
four phases: Introduction, Knowledge Input, Exercises, and Summary https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=15491,16214, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=14667,15490.
• After the lecture, participants completed questionnaires assessing perceived realism and experienced presence https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=19701,20523, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=22366,23280.
4. Task
• Participants attended a virtual lecture on computational thinking, 
during which they interacted with virtual peer-learners and a virtual 
instructor. The lecture included questions posed by the instructor, with
 varying hand-raising behaviors from the peer-learners https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=5173,5865, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=14667,15490.
5. Metrics Collected
• Eye Tracking Data: Head location, pose, gaze, and 
eye-related data were collected to assess visual attention towards the 
instructor, peer-learners, and the screen https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=20524,21441 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=40537,41562.
• Self-Reported Metrics: Questionnaires measuring perceived realism and experienced presence were collected using 4-point Likert scales https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=27073,27669 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=22366,23280.
• Attention Threshold: An attention threshold of 200 ms was set to determine the objects of interest based on gaze duration https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=21442,22363 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d95af7663842dc4615597e/?range=11561,12408.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, task, and metrics collected.","Presence – General,  Self-Reported",Head Analysis,Eye-Tracking,,"Questionnaires,  Behavioural,  Physiological"
ID083,Exploring and Modeling Directional Effects on Steering Behavior in Virtual Reality,"Wei  Yushi,  Xu  Kemu,  Li  Yue,  Yu  Lingyun,  Liang  Hai-Ning",,10.1109/TVCG.2024.3456166,2024,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,cinematic VR,"This paper investigates how device type (HMD vs. traditional screen) influences presence and immersion during a cinematic virtual reality (CVR) storytelling experience. Using a within-subjects design, sixty participants viewed the 360° narrative film Faoladh (a Viking raid on an Irish village) either via an HTC Vive Pro HMD or a 4K monitor, and completed the Igroup Presence Questionnaire (IPQ) and adapted System Usability Scale (SUS) after each condition.
Results showed that HMD viewing significantly increased spatial presence, involvement, and overall sense of “being there” compared to the screen-based version (p < .001 for spatial presence and involvement). The “experienced realism” subscale showed no significant difference, suggesting that realism depends more on content quality than viewing medium. Qualitative thematic analysis identified three key experiential themes:
1. Engagement and immersion – participants described stronger emotional connection and embodiment through the HMD.
2. Technical influence – lag and low resolution reduced immersion, particularly for screen-based viewing.
3. Sensory and physical interaction – freedom of head movement and environmental awareness enhanced engagement.
The study concludes that spatial presence and involvement are the main psychological factors affected by the display medium, highlighting the importance of spatial design and narrative viewpoint in CVR storytelling.","Participants:
24 university students (12 male + 12 female, mean age ≈ 22.9 years, all right-handed).
Apparatus & Setup:
• Device: Meta Quest Pro HMD using built-in optical hand-tracking.
• Task environment: Unity 2021 scene showing a straight transparent “tube” path suspended in 3D space.
• Input: Users pinched a small virtual sphere and steered it through the tube with their dominant hand—no controller.
• Directions tested: 12 directions spaced every 30° around a circle (0° to 330°).
• Independent variables:
– Path length (3 levels = 160 mm, 240 mm, 320 mm)
– Path width (3 levels = 12 mm, 16 mm, 20 mm)
– Steering direction (12 angles)
→ 3 × 3 × 12 within-subjects design, randomized order.
Dependent variables recorded automatically:
1. Movement Time (MT) – total duration of a successful steering trial.
2. Average Speed (V) = A / MT.
3. Re-entries – number of times the sphere collided with or re-entered the path boundary.
4. Success Rate – % of trials completed without errors.
Procedure:
Each participant performed 324 trials (3 widths × 3 lengths × 12 directions × 3 repetitions).
Short breaks were inserted every 36 trials to minimize fatigue.
Results highlights:
• ANOVA: direction (η² ≈ 0.40 for MT) significant (p < .001).
• Movements along cardinal axes (0°, 90°, 180°, 270°) produced shortest MT and highest speed.
• Downward (270°) fastest — likely ergonomic/gravity advantage.
• Narrower paths and longer distances increased MT as expected.
🧪 Experiment 2 – Model Validation
Goal:
Validate the Sθ model derived from Exp 1 on a new sample and device.
Participants:
12 new participants (no overlap with Exp 1).
Apparatus:
Meta Quest 3 HMD (also optical hand tracking).
Design differences:
• Directions: 8 equally spaced (every 45° from 0°–315°).
• Widths/Lengths: fixed mid-values (W = 16 mm, A = 240 mm).
• 3 repetitions × 8 directions × 2 hands → 48 trials per participant.
Purpose: evaluate predictive accuracy of the proposed Sθ model
MT = a + b·(A/W) + c·sin(4θ − π/2).
Findings:
• Model fit R² = 0.958 (Exp 1 training) and 0.910 (Exp 2 validation).
• Compared to classic Steering Law, ΔR² ≈ +0.15.
• Directional sinusoid captured cyclical ergonomic variation.
• Results consistent across devices → good generalization.",,Hand Movement,,"Movement Time,  Task Success / Completion","Behavioural,  Performance"
ID084,Exploring Implicit and Explicit Stimuli of Virtual Agents with Increased Group Size in VR,"Jung  Sungchul,  Wile  Nicholas,  Jung  Kyung Hun Jay",,10.1109/ISMAR62088.2024.00122,2024,International Symposium on Mixed and Augmented Reality (ISMAR),VR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,"Emotional Response in Crowded VR Environments, Social Interaction","This study explores how implicit vs. explicit behavioral stimuli of virtual agents influence users’ perceptions, emotions, and behaviors in a VR public speaking scenario, and how these effects scale with audience group size. Twenty-four participants were divided into two conditions: agents showing implicit behavior (eye-gaze shifts) and explicit behavior (head turns). Each participant gave speeches to virtual audiences of three sizes (small, medium, large). A psychophysical detection experiment confirmed that eye-gaze shifts were below conscious detection thresholds, validating their implicit nature.
Results show that explicit agent behaviors caused stronger behavioral reactions (increased head movement and higher influence ratings) but evoked negative emotions (fear, worry, panic). In contrast, implicit eye-gaze behaviors did not elicit overt behavioral changes but led to more positive affective states (happy, easygoing, enjoyment). Importantly, the influence of both implicit and explicit stimuli increased with audience size, suggesting that group-scale amplification affects perception and emotion even without conscious awareness.
This study contributes to understanding subliminal social cues and crowd effects in immersive environments and provides validated emotion and behavior metrics to capture implicit user responses to group-level avatar stimuli.","1) Participants
• N = 24 (12 per condition: implicit/eye-gaze vs explicit/head-turn).
• Age: 18–31 (M = 26 ± 3.5).
• Gender: 22 male, 2 female.
• Experience: Mixed VR familiarity; low prior public speaking experience.
• Measured traits: Public speaking anxiety, social anxiety, personality (active/neutral/passive).
2) Study Design
• 2 × 3 mixed factorial design:
– Factor 1 (between-subjects): Behavior type (implicit vs explicit).
– Factor 2 (within-subjects): Group size (small, medium, large).
• Implicit condition: Eye-gaze shifting (20% of agents).
• Explicit condition: Head-turning (20% of agents).
• Control for detection: Psychophysical task confirming implicit/explicit thresholds.
3) Task & Procedure
• Participants delivered three 3-minute speeches (topics generated by GPT) to VR audiences.
• Audiences varied in size:
– Implicit (eye): 5, 10, 15 agents.
– Explicit (head): 10, 25, 50 agents.
• Behavioral animations (gaze or head turns) occurred 6 times per session (2s each).
• After each condition: questionnaires on presence, emotion, and perceived influence.
• Afterward: psychophysical detection task (2AFC) to confirm awareness thresholds.
4) Apparatus
• VR system: HTC Vive Pro Eye with built-in eye tracker (90 Hz).
• Software: Unity 2021.3, Tobii XR SDK, OpenVR plugin.
• Stimuli: Virtual classroom (small → large), animated agents created in Character Creator 4 / iClone 8.
• Sensors: Head position and rotation, gaze direction recorded continuously.
5) Metrics Collected
• Behavioral Metrics: Head rotation, head position, gaze direction (tracking data).
• Questionnaires:
• Social Presence & Co-Presence (Bailenson et al., 2003).
• Engagement (Jung et al., 2022).
• Body Ownership (Peck & González-Franco, 2021).
• Influence (custom items: “Did the audience size/behavior influence your performance?”).
• Discrete Emotion Questionnaire (Harmon-Jones et al., 2016; 32 emotion items).
• Psychophysical Detection Task: % detection of implicit (eye) vs explicit (head) behavior.
Main Findings
Psychophysical validation:
• Eye-gaze movement detection < 50% (implicit threshold).
• Head movement detection ≈ 100% (explicit awareness confirmed).
Behavioral results:
• Head-turning (explicit) → greater head rotation and motion; group size amplified effects.
• Eye-gaze shifting (implicit) → no significant behavioral change.
Perceptual & emotional results:
• Explicit behavior → higher social presence and co-presence, but also stronger negative emotions (fear, panic, worry).
• Implicit behavior → higher positive emotions (happy, easygoing, enjoyment); emotional effects increased with larger audiences.
• No significant differences for body ownership (comparable immersion).
Interpretation:
• Implicit visual cues modulate user affect without conscious detection or overt behavioral change.
• Group size amplifies emotional and perceptual effects for both implicit and explicit agent behaviors.","Co-Presence Scale,  Custom made,  Engagement,  Likert Scale (3-point) Body Ownership,  Social Presence,  Discrete Emotion Questionnaire","Head Analysis,  Gaze Analysis",,,"Questionnaires,  Behavioural"
ID085,Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options,"Christiansen  Frederik Roland,  Hollensberg  Linus Nørgaard,  Jensen  Niko Bach,  Julsgaard  Kristian,  Jespersen  Kristian Nyborg,  Nikolov  Ivan",,10.1145/3641825.3687716,2024,ACM Symposium on Virtual Reality Software and Technology,VR,Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,Entertainment and Gaming,"The paper investigates how two communication modalities — speech recognition vs dialogue options — influence social presence, game experience, and interaction quality when users converse with LLM-driven NPCs inside a VR murder-mystery game.
22 participants interacted with NPCs using either free natural speech or predefined dialogue choices. The authors measured social presence (GEQ-SP), in-game experience (GEQ), task outcome (correctly identifying the murderer), and qualitative feedback.
Main findings:
• No overall significant difference in full GEQ components, but individual items show meaningful differences.
• Speech recognition → higher behavioural involvement, stronger sense that NPCs “paid attention,” perceived as more immersive but overwhelming.
• Dialogue options → higher perceived challenge, higher competence (“I felt skillful”), better understanding of the plot.
• Response time was a major UX barrier in both modalities.
","1) Participants
• N = 22 (after removing 2 for pilot/sickness)
• Ages not specified
• Between-subjects:
    ◦ 11 speech recognition
    ◦ 11 dialogue options
2) Study Design
• Between-subjects, 2 interaction modalities
• 30-minute gameplay session
• GEQ-Social Presence + GEQ-In-Game questionnaires afterward
• Task: identify the murderer
3) Procedure
1. Consent + explanation
2. 15-minute VR gameplay
3. Interaction with 4 NPCs (LLM-driven)
4. Find clues + ask questions
5. Questionnaire:
    ◦ Murderer guess
    ◦ GEQ Social Presence
    ◦ GEQ In-Game
    ◦ Open-ended feedback
4) Apparatus
• Meta Quest 2 + controllers
• Whisper API (speech transcription)
• ChatGPT 4-1106-preview (NPC responses)
• Oculus TTS for NPC voices
5) Metrics collected
Questionnaires – Standard
• GEQ – Social Presence
Components: Empathy, Negative Feelings, Behavioural Involvement
• GEQ – In-Game
Components: Immersion, Flow, Competence, Challenge, Positive/Negative Affect
Performance Metrics
• Task success: correctly guessing the murderer
Behavioural Metrics
• Interaction modality effects (frequency of interactions, response times) — indirectly measured
• NPC attentiveness and user engagement (through GEQ items)
Qualitative Metrics
• Written feedback
• Perceived realism, engagement, confusion, challenge
• Perceived limitations (response delay, hallucinations)",Game Experience Questionnaire (GEQ),,,"Response Time,  Task Success / Completion","Questionnaires,  Performance"
ID086,Exploring the Facets of the Multiplayer VR Gaming Experience,"Vlahovic  Sara,  Slivar  Ivan,  Silic  Matko,  Skorin-kapov  Lea,  Suznjevic  Mirko",,10.1145/3649897,2024,Transactions on Multimedia Computing Communications and Applications,VR,Embodiment Avatars & Social Presence,Content & System Design.,Entertainment and Gaming,"This study explores the multifaceted user experience of multiplayer VR gaming by evaluating the effects of network conditions (4G, 5G, Ethernet, and added latency), social context (friends vs. strangers), and interplayer competition on users’ perceived Quality of Experience (QoE). It found that while 5G and Ethernet yielded similar QoE levels, games relying on realistic shared-object physics (like table tennis) were more sensitive to latency than stylized shooter games. Participants’ prior relationship and gaming expertise influenced feelings of competitiveness and social bonding, but not consistently their overall QoE. The findings emphasize that QoE in VR gaming depends on a complex interplay of technical, social, and gameplay factors, with implications for study design and matchmaking in future research.

QoE was significantly reduced under high-latency (4G) conditions, especially in table tennisFriends and strangers behaved differently in terms of competitiveness and collaborationShooter gameplay was more robust to network issues than precision-based table tennis5G performance matched Ethernet, supporting its feasibility for wireless multiplayer VR","1) Participants
• Total: 32 participants
• Grouped into: 16 pairs of participants
• Each pair played two cooperative multiplayer VR games
• Pairs were either:
    ◦ Friends (pre-existing relationship), or
    ◦ Strangers (no prior relationship)
2) Study Design
• Mixed design with:
    ◦ Within-subjects factors:
        ▪ Network condition: Ethernet, 5G, 4G, and 4G + added latency
        ▪ Game type: VR shooter vs. table tennis
    ◦ Between-subjects factor:
        ▪ Social familiarity: Friends vs. strangers
• Each pair experienced both games, each under all four network conditions
3) Procedure
1. Setup:
    ◦ Pairs were seated ~1.5m apart and used Meta Quest 2 headsets
    ◦ Assigned randomly to friend or stranger conditions
2. Gameplay:
    ◦ Each pair played both games across all network conditions
    ◦ Game order and condition order were counterbalanced
3. Post-game questionnaires:
    ◦ After each round, participants rated:
        ▪ QoE (Quality of Experience)
        ▪ Interaction quality with Gaming Input Quality Scale (GIPS)
    ◦ After all scenarios for a particular game:
        ▪ post-game questionnaire, a modified version of the Core Module of the Game Experience Questionnaire (GEQ)
4. Post-study interviews:
    ◦ Collected open-ended feedback on experience, social dynamics, and gameplay quality
4) Task
• Two VR games with different sensitivity to latency:
    1. Shooter game: Fast-paced interaction with lower motor precision demands
    2. Table tennis game: High precision timing task, more affected by latency
• Both games required close coordination or competitive engagement between participants","Gaming Input Quality Scale,  QoE - ACR,  Game Experience Questionnaire (GEQ)",,,,Questionnaires
ID087,Exploring the Relative Effects of Body Position and Locomotion Method on Presence and Cybersickness When Navigating a Virtual Environment,"Kim  Aelee,  Lee  Jeong-Eun,  Lee  Kyoung-Min",,10.1145/3627706,2023,Transactions on Applied Perception,VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This paper investigates the impact of body posture (sitting vs. standing) and locomotion interface (joystick vs. walking-in-place) on presence and cybersickness in VR. Using a 2x2 within-subjects design with 29 participants, the study measured presence (using the Presence Questionnaire), cybersickness (using SSQ), and simulator time as performance. Key findings include: standing increases presence; joystick locomotion increases cybersickness; and presence is not directly correlated with cybersickness. The authors recommend that locomotion interface and body posture should be carefully selected based on application goals, particularly in balancing immersion and comfort.","
1. Participants:
    ◦ Number: 90 students (54 males and 36 females) from Seoul National University in South Korea.
    ◦ Age Range: Not explicitly mentioned, but participants were university students, typically aged 18–25.
    ◦ Inclusion Criteria: No specific inclusion/exclusion criteria were mentioned beyond being students.
2. Study Design:
    ◦ Design: 2 (body position) × 4 (locomotion method) between-subjects design.
    ◦ Body Positions: Standing vs. sitting.
    ◦ Locomotion Methods:
        ▪ Steering + Embodied Control (EC): Participants steered themselves to change position and turned their bodies to rotate.
        ▪ Steering + Instrumental Control (IC): Participants steered themselves to change position and used a thumbstick to rotate.
        ▪ Teleportation + EC: Participants teleported themselves to change position and turned their bodies to rotate.
        ▪ Teleportation + IC: Participants teleported themselves to change position and used a thumbstick to rotate.
    ◦ Randomization: Participants were randomly assigned to one of the eight conditions (2 body positions × 4 locomotion methods).
3. Procedure:
    ◦ Setup: Participants used an Oculus Rift HMD, two Oculus Touch controllers, and a swivel chair (for sitting conditions). The VR environment was created using the Nature Treks application.
    ◦ Training: Participants were shown a video of the navigation route and practiced navigating the route for about 10 minutes using their assigned body position and locomotion method.
    ◦ Navigation Task: Participants navigated the practiced route for approximately 10–15 minutes. They were instructed to use only forward translational movement (steering or teleportation) and the assigned rotation method (EC or IC).
    ◦ Post-Task Questionnaire: After completing the navigation task, participants filled out questionnaires to measure presence and cybersickness.
4. Task:
    ◦ Navigation Task: Participants were required to navigate a predefined route in a virtual environment (VE) using their assigned locomotion method. The route included various landmarks (e.g., a house, gates, bridges) and geographical features (e.g., hills, streams). The task involved moving through the environment and interacting with the virtual space as naturally as possible.
    ◦ Locomotion Methods:
        ▪ Steering: Continuous linear movement controlled by holding down a trigger on the controller.
        ▪ Teleportation: Discontinuous movement where participants pointed to a location and were instantly transported there.
        ▪ Rotation: Participants either turned their bodies (EC) or used a thumbstick (IC) to rotate.
5. Metrics Collected:
    ◦ Presence: Measured using a questionnaire based on Witmer and Singer’s framework. The questionnaire included 27 items divided into five factors: control, visual sense, attention, spatial presence, and immersion. Responses were scored on a 10-point Likert scale (1 = not at all; 10 = very much).
    ◦ Cybersickness: Measured using the Simulator Sickness Questionnaire (SSQ), which includes 16 symptoms categorized into three subscales: nausea, oculomotor, and disorientation. Participants rated symptoms on a 4-point scale (0 = none; 1 = slight; 2 = moderate; 3 = severe). Total scores were computed to reflect the severity of symptoms.
    ◦ Performance Metrics: Although not explicitly mentioned, the study implicitly collected performance data related to navigation efficiency and accuracy, as participants had to navigate a predefined route using different locomotion methods.
Summary:
The study involved 90 participants who were randomly assigned to one of eight conditions based on body position (standing vs. sitting) and locomotion method (steering + EC, steering + IC, teleportation + EC, teleportation + IC). Participants practiced navigating a virtual environment using their assigned condition and then completed a navigation task. After the task, they filled out questionnaires to measure presence and cybersickness. The study collected data on presence, cybersickness, and implicitly on navigation performance.","Presence – General,  SSQ (Simulator Sickness Questionnaire)",,,,Questionnaires
ID088,Exploring the Use of Olfactory Stimuli Towards Reducing Visually Induced Motion Sickness in Virtual Reality,"Ranasinghe  Nimesha,  Jain  Pravar,  Tolley  David,  Karwita Tailan  Shienny,  Yen  Ching Chiuan,  Do  Ellen Yi-Luen",,10.1145/3385959.3418451,2020,Symposium on Spatial User Interaction,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This study demonstrates that integrating pleasant olfactory stimuli (peppermint scent) during immersive VR experiences can reduce the severity of visually induced motion sickness (VIMS), as measured by the Simulator Sickness Questionnaire (SSQ) and the Fast Motion Sickness Scale (FMS). A moderate-intensity peppermint scent (Med-Olf condition) significantly reduced total SSQ, oculomotor, and disorientation scores compared to the no-olfaction condition, although it had limited impact on nausea subscale scores. The results suggest olfactory intensity is a key parameter, with moderate intensity outperforming both none and high-intensity exposure. These findings highlight the potential of multisensory interventions—particularly smell—for improving user experience in VR by mitigating motion-related discomfort.","1) Participants (Updated):
• Total: 14 participants
• Design: Within-subjects
    ◦ Each participant experienced all three conditions (No-Olf, Med-Olf, High-Olf)
    ◦ The order of conditions was counterbalanced across participants to minimize ordering effects
2) Study Design
• Within-subjects design
    ◦ Each participant experienced three olfactory conditions in counterbalanced order:
        1. No-Olf: No scent
        2. Med-Olf: Medium-intensity peppermint scent
        3. High-Olf: High-intensity peppermint scent
• Independent Variable:
    ◦ Olfactory intensity condition during VR exposure
• Dependent Variables:
    ◦ Simulator sickness severity (SSQ subscales and total score)
    ◦ Motion discomfort over time (FMS)
    ◦ Odor intensity and pleasantness ratings
3) Procedure
1. Pre-exposure (Screening and Baseline):
    ◦ Participants completed the COSS to assess olfactory sensitivity
    ◦ A baseline SSQ was collected before VR exposure began
2. VR Exposure (Repeated Trials):
    ◦ Participants underwent three separate 10-minute VR driving simulations, each paired with a different olfactory condition
    ◦ During each session, they rated their current level of motion discomfort every 1–2 minutes using the Fast Motion Sickness Scale (FMS)
3. Post-exposure (Per Condition):
    ◦ After each session, participants completed the post-SSQ, odor intensity scale, and odor pleasantness scale
    ◦ All three conditions were completed in a single session, with rest periods between exposures
4) Task
• Immersive VR Driving Task:
    ◦ A first-person passive driving simulation designed to induce visually induced motion sickness (VIMS)
    ◦ Participants were instructed to keep their eyes open and continuously observe the environment
    ◦ No user input or active navigation was involved
5) Metrics Collected
Questionnaires (Subjective Metrics):
• Simulator Sickness Questionnaire (SSQ): Pre- and post-condition (Total, Nausea, Oculomotor, Disorientation subscales)
• Fast Motion Sickness Scale (FMS): Repeated in-VR self-report of motion sickness severity
• Olfactic Intensity Rating Scale (OIRS): Post-condition rating of perceived scent strength
• Odor Pleasantness Rating: Subjective rating of scent enjoyment
• Chemical Odor Sensitivity Scale (COSS): Used as a pre-study screening tool","Chemical Odor Sensitivity Scale,  Fast Motion Sickness Scale,  Olfactory Intensity Rating Scale,  SSQ (Simulator Sickness Questionnaire)",,,,Questionnaires
ID089,Exploring the Use of Smartphones as Input Devices for the Mixed Reality Environment,"Pan  Zhigeng,  Pan  Zhipeng,  Luo  Tianren,  Zhang  Mingmin",,10.1145/3574131.3574451,2022,Conference on Virtual-Reality Continuum and its Applications in Industry,"Collaborative Mixed Reality, MR",Interaction Techniques & input Modalities,Visualization Techniques.,Use of Smartphones as Input Devices for the Mixed Reality Environment,"This study compares three MR interaction methods—a hybrid interface (smartphone + HoloLens2), a 3D MR-only interface (HoloLens2), and a 2D tablet-based UI—to evaluate how they affect usability, presence, and user satisfaction. Results from 18 participants show that the hybrid interface scored highest in terms of usability, interaction satisfaction, and user preference, while also delivering strong levels of presence. Although 2D input via tablet was faster for text entry, it offered the least immersion. Users appreciated the naturalness of mid-air gestures and the tactile precision of smartphone input, suggesting that combining 2D and 3D modalities can enhance comfort, efficiency, and immersion in collaborative MR applications. These findings support the future design of hybrid interfaces as a flexible solution for shared MR environments.

The content of the user experience ques-
tionnaire is divided into two categories: interaction satisfaction and
preference. Interaction satisfaction includes the following indica-
tors: easy to learn, natural, easy to perform, comfortable, intuitive,
satisfying, efficient. ","1) Participants
• Total: 18 participants
• Demographics:
    ◦ Balanced gender distribution
    ◦ Age and prior experience with MR not detailed
    ◦ All participants completed all three interface conditions
2) Study Design
• Within-subjects design
    ◦ Each participant experienced all three UI conditions in randomized order:
        1. Hybrid UI: Smartphone + HoloLens2
        2. 3D UI: HoloLens2 mid-air gesture-based interaction
        3. 2D UI: Tablet-based input
• Independent Variable:
    ◦ Type of input interface
• Dependent Variables:
    ◦ Usability (SUS)
    ◦ Presence (SPQ)
    ◦ Interaction satisfaction (custom questionnaire)
3) Procedure
1. Introduction & Training:
    ◦ Participants were given instructions and practiced with each interface
2. Task Execution:
    ◦ Participants completed a shared MR interaction task involving selecting and placing virtual objects
    ◦ Input was provided via smartphone, tablet, or HoloLens gestures depending on the condition
3. Post-condition Evaluation:
    ◦ After each interface, participants completed:
        ▪ SUS
        ▪ SPQ
        ▪ Interaction satisfaction questionnaire
4) Task
• Collaborative Object Manipulation Task in MR:
    ◦ Participants worked together to arrange virtual items in a shared MR space
    ◦ Focus was on evaluating ease of input, comfort, and interface preference
5) Metrics Collected
• System Usability Scale (SUS)
• Sense of Presence (ITC-SPQ)
• Custom satisfaction questionnaire (comfort, learnability, preference)","Custom made,  Presence – General,  System Usability Scale (SUS)",,,,Questionnaires
ID090,Exploring Trajectory Data in Augmented Reality: A Comparative Study of Interaction Modalities,"Joos  Lucas,  Klein  Karsten,  Fischer  Maximilian T.,  Dennig  Frederik L.,  Keim  Daniel A.,  Krone  Michael",,10.1109/ISMAR59233.2023.00094,2023,International Symposium on Mixed and Augmented Reality (ISMAR),AR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This paper explores and compares gaze- and hand-based interaction techniques in AR for analyzing 3D trajectory data. It introduces novel methods to address known gaze-based interaction limitations (e.g., imprecision, Midas Touch) and evaluates these methods through two user studies (pilot and main). Results show that while eye-tracking interaction generally requires more time and induces higher mental workload in precise tasks, it can compete with hand tracking in simpler or selection tasks when supplemented by spatial decluttering or two-step selection techniques. The findings highlight that the effectiveness of interaction modality is task- and user-dependent, suggesting gaze interaction as a viable alternative in immersive analytics with careful design.","Here
 is a summary of the experiments conducted in the paper, including 
details on participants, study design, procedure, tasks, and metrics 
collected:
1. Participants
• Total Participants: 22 participants were invited, but the results of 20 participants were included in the evaluation (10 female, 10 male).
• Age Range: 21 to 52 years (Mean: 25.6, SD: 7.7).
• Vision: All participants had normal or corrected-to-normal vision.
• Experience: No participants had prior experience with trajectory analysis, and only one reported experience with augmented reality (AR) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=5824,6753 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=31679,32267.
2. Study Design
• Type: The study consisted of a pilot study followed by a main user study.
• Pilot Study: Conducted with 6 participants to refine the study setup and interaction techniques.
• Main User Study: Conducted with 20 participants, focusing on comparing eye tracking and hand tracking as interaction modalities https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=55338,56331, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=53343,54402.
3. Procedure
• Setting: Individual one-hour sessions in a university laboratory using the HoloLens 2 HMD.
• Calibration: Participants underwent eye tracking calibration after receiving detailed instructions.
• Tasks: Participants completed four tasks in a fixed order, with conditions randomized using a Latin square design.
• Breaks: After each condition, participants filled out the NASA TLX test to report task load https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=30812,31676 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=47982,48789.
4. Tasks
The study included four abstract tasks designed to evaluate different interaction techniques:
1. Cluster Exploration: Participants identified the cluster with the highest number of members.
2. Trajectory Selection: Participants selected a specific trajectory indicated by color.
3. Time Step Identification (Task 3): Participants identified a time step at a direction change in a single trajectory.
4. Time Step Identification (Task 4): Participants located the time step with the highest value among multiple direction changes https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=28968,29862, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=26419,27177.
5. Metrics Collected
• Performance Metrics: Task accuracy and answer times were measured for each task.
• Task Load Metrics: Collected using the NASA TLX 
test, which assesses mental demand, physical demand, temporal demand, 
performance, effort, and frustration https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=47982,48789 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=32801,33555.
• Qualitative Feedback: Participants provided feedback on their experiences and preferences regarding the interaction modalities used https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=35297,36261, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5057fa5b89ad86dbb51d/?range=46335,47110.
This structured approach allowed the researchers to evaluate the 
effectiveness and user experience of different interaction modalities in
 immersive technology, specifically in the context of trajectory 
exploration.",NASA-TLX,,,"Response Time,  Accuracy","Questionnaires,  Performance"
ID091,Exploring User Behaviour in Asymmetric Collaborative Mixed Reality,"Numan  Nels,  Steed  Anthony",,10.1145/3562939.3565630,2022,ACM Symposium on Virtual Reality Software and Technology,"AR, VR",Behavioural Dynamics & Exploration,Embodiment Avatars & Social Presence.,Telecommunications and Collaboration,"This study investigates user behavior in asymmetric collaborative mixed reality (MR), comparing how users equipped with AR vs. VR headsets interact during a joint word puzzle task. The authors find that while interface asymmetry does not significantly affect subjective leadership ratings, AR users objectively speak more and rotate their heads more, while VR users move more in space. Head rotation velocity was found to correlate with leadership and talkativeness. The paper highlights that combining subjective and behavioral metrics can yield deeper insights into user roles, engagement, and collaboration in MR.","
1. Participants:
    ◦ 30 participants (15 dyads) were recruited for the study.
    ◦ Gender distribution: 15 males and 15 females.
    ◦ Age distribution:
        ▪ 21 participants were aged 18-24.
        ▪ 9 participants were aged 25-34.
    ◦ Educational background: Participants were recruited from a university campus, and most were students or staff.
    ◦ VR/AR experience:
        ▪ 7 participants used VR weekly.
        ▪ 2 participants used social VR weekly.
        ▪ 18 participants did not speak English natively.
    ◦ Prior relationships: 11 of the 15 dyads knew each other before the experiment.
2. Study Design:
    ◦ The study used a within-subjects design, where each dyad performed the task twice, once with one participant using AR (Microsoft HoloLens 2) and the other using VR (Meta Quest 2), and then switching roles for the second trial.
    ◦ The order of conditions (AR or VR first) was counterbalanced using a Latin square to avoid order effects.
    ◦ The study aimed to explore how asymmetric MR interfaces (AR and VR) affect collaboration and user behavior.
3. Procedure:
    ◦ Participants were briefed on the task and given instructions on how to use the AR and VR headsets.
    ◦ Each dyad performed a word puzzle task in a collaborative virtual environment (CVE) for 15 minutes per trial.
    ◦ After each trial, participants completed a post-trial questionnaire to assess their experience (e.g., leadership, talkativeness, presence, co-presence, and accord).
    ◦ An unstructured interview was conducted after each trial to gather qualitative feedback on the experience.
    ◦ Participants were compensated with a £10 voucher for their time.
4. Task:
    ◦ The task involved solving word puzzles collaboratively in a virtual environment.
    ◦ Participants had to identify and order words displayed on virtual posters hung around the room.
    ◦ Each poster contained a list of words preceded by a number, indicating which riddle the words belonged to.
    ◦ Participants could check their solutions by saying them out loud, and the experimenter would confirm if the solution was correct.
    ◦ The task required joint orientation, exploration, and problem-solving, which are relevant to real-world collaborative tasks.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Leadership: Participants rated their own and their partner’s leadership on a scale from 1 to 100.
        ▪ Talkativeness: Participants rated their own and their partner’s talkativeness on a scale from 1 to 100.
        ▪ Presence: Measured using a 7-point Likert scale (e.g., ""I felt like I was in the virtual office"").
        ▪ Co-presence: Measured using a 7-point Likert scale (e.g., ""I felt like I was collaborating with a real person"").
        ▪ Accord (group harmony): Measured using a 7-point Likert scale (e.g., ""I felt in harmony with my partner"").
    ◦ Behavioral Metrics:
        ▪ Number of words spoken: Calculated by transcribing participant speech and counting the words.
        ▪ Distance traveled: Calculated based on the distance each participant’s avatar traveled in the CVE.
        ▪ Head rotation velocity: Calculated based on the median head rotation velocity (yaw, roll, and pitch axes) using data from the HMD sensors.
    ◦ Qualitative Data:
        ▪ Unstructured interviews: Conducted after each trial to gather participant feedback on their experience, preferences, and any issues they encountered.
Key Findings:
• Behavioral Differences:
    ◦ AR users spoke more words than VR users, supporting the hypothesis that AR users might be more talkative.
    ◦ VR users traveled further in the virtual environment, likely due to the ability to use a joystick for navigation.
    ◦ AR users had a higher median head rotation velocity, suggesting they turned their heads more frequently, possibly due to the narrower field of view (FoV) of the AR headset.
• Subjective Measures:
    ◦ No significant differences were found in leadership or talkativeness scores between AR and VR users, contradicting the hypothesis that AR users would emerge as leaders.
    ◦ Presence and co-presence were positively correlated, suggesting that a stronger sense of presence enhances the feeling of collaborating with a real person.
    ◦ Accord (group harmony) was positively correlated with co-presence in the first trial but not in the second, possibly due to participants acclimating to the system.
• Qualitative Insights:
    ◦ Participants noted that spatial referencing (e.g., pointing to virtual posters) was challenging, especially when one user was unaware of what the other was pointing at.
    ◦ Some participants found the AR headset easier to read from, while others preferred the VR headset for navigation.
Conclusion:
The experiment provides valuable insights into how asymmetric MR interfaces (AR and VR) affect collaboration and user behavior. The study used a combination of questionnaires and behavioral metrics to evaluate user experience, revealing differences in how AR and VR users interact in a collaborative environment. The findings contribute to the broader understanding of user experience in XR, particularly in collaborative settings.","Custom - Leadership and Talkativeness 1 - 100,  Interaction Anxiousness Scale,  Presence – General,  Sense of Embodiment,  Unstructured Interviews","Total Distance Travelled,  Movement Kinematics,  Communication Analysis",,,"Questionnaires,  Behavioural"
ID092,Exploring User Engagement in Immersive Virtual Reality Games through Multimodal Body Movements,"Somarathna  Rukshani,  Elvitigala  Don Samitha,  Yan  Yijun,  Quigley  Aaron J,  Mohammadi  Gelareh",,10.1145/3611659.3615687,2023,ACM Symposium on Virtual Reality Software and Technology,VR,Behavioural Dynamics & Exploration,,Human-Computer Interaction (HCI),"The study investigates whether multimodal body movements (captured via IMUs and wearable devices) can be used as indicators of user engagement in immersive VR games. Engagement is conceptualized using motivation and physiological arousal (EDA) as proxies. By analyzing gameplay across 27 VR games, the authors demonstrate that machine learning models trained on head, hand, and foot movement data can predict self-reported motivational states and EDA levels. Clustering of motivational responses also reveals patterns across games. The results show that body movements are promising behavioral indicators of engagement, suggesting that motion patterns can be harnessed for real-time experience optimization in VR.","1) Participants
• Total: 32 participants (16 female, 16 male)
• Age: Mean = 24.5 years (SD = 5.07)
• Criteria:
    ◦ Ages 18–40
    ◦ No prior psychological/neurological disorders
    ◦ Fluent in English
    ◦ No glasses or corrected vision
    ◦ Low susceptibility to motion sickness
    ◦ No vertigo/hearing/vestibular issues
2) Study Design
• Design Type: Within-subjects experimental design
• Sessions: Each participant completed three 100-minute sessions
• Game Exposure: 27 VR games, randomized order across sessions to balance immersion effects
• Purpose: To model engagement using multimodal body movement data correlated with both subjective (motivation) and objective (EDA) engagement proxies
3) Procedure
• Participants first completed demographic, mood, and personality surveys.
• Underwent a VR training session to reduce novelty bias.
• Played each game for 3 minutes in a seated position using:
    ◦ HTC Vive Pro (with emteqPRO headset for facial/head movements)
    ◦ Empatica E4 wristband (for hand motion + physiological data)
    ◦ Foot-mounted IMU (for foot movement tracking)
• Post-game emotion assessments via Geneva Emotion Wheel and CoreGRID surveys.
• End-of-session immersion, presence, and realness rated via Likert-scale survey.
4) Task
• Play a sequence of 27 immersive VR games designed to evoke diverse emotional and motivational states.
• Example game categories: fear, joy, interest, amusement, contentment, compassion, sadness, and anger.
5) Metrics Collected
• Body Movements:
    ◦ Acceleration data (mean and standard deviation) from head (emteqPRO), hand (E4), and foot IMUs across X, Y, Z axes
• Physiological Data (via E4):
    ◦ Electrodermal Activity (EDA)
    ◦ Heart Rate, Blood Volume Pulse, Inter Beat Intervals, Skin Temperature (not analyzed in this study)
• Subjective Measures:
    ◦ Motivation via CoreGRID items (e.g., want to continue, feel unmotivated, urge to resist)
    ◦ Emotional states via Geneva Emotion Wheel
    ◦ Post-session immersion, presence, and realness ratings
The study then used machine learning models (LGBM, Random Forest, XGBoost) to predict engagement levels based on movement features, and employed clustering techniques to identify motivational engagement patterns across games.","CoreGRID,  Custom made,  Custom made - immersion,  Geneva Emotion Wheel,  MSSQ,  Presence – General,  Realness","Body Analysis,  Hand Movement,  Head Analysis","BVP / PPG,  EDA / GSR (Skin Conductance),  Heart Rate,  HRV / IBI",,"Questionnaires,  Behavioural,  Physiological"
ID093,Factors Influencing Engagement in Hybrid Virtual and Augmented Reality,"Li  Yue,  Ch’ng  Eugene,  Cobb  Sue",,10.1145/3589952,2023,ACM Trans. Comput.-Hum. Interact.,"AR, VR",Content & System Design,,Cultural heritage,"This paper investigates how four factors — animation quality, motion control method, rendering quality, and frame rate — influence Quality of Experience (QoE) in an interactive VR animation environment. A user study with 36 participants evaluates users’ perceived immersion, interaction quality, visual effects, and overall QoE, using a 5-point Likert scale questionnaire. Results show that motion control and animation quality had the most significant impact on QoE dimensions. The study contributes to understanding how interactive system components shape the user's immersive experience in VR beyond just visual fidelity.","1) Participants
• Total: 60 participants (30 pairs)
• Demographics: Mostly university students and staff, aged 18 to 51 (Mean = 22.45, SD = 7.11)
• Gender: 13 males, 47 females (reflective of international university demographic)
• Experience:
    ◦ 93.33% had viewed stereoscopic 3D images
    ◦ 61.67% had used VR before
    ◦ 40% had used AR before
• Pairing: Most participants signed up in pairs and knew each other.
2) Study Design
• Type: Within-subjects experimental design
• Conditions: Each participant experienced two sessions:
    ◦ HVAR-Info: Hybrid VR-AR with only informational labels
    ◦ HVAR-UGC: Hybrid VR-AR with both informational and user-generated content (UGC) labels
• Counterbalanced Roles: Participants switched between VR and AR roles across sessions
• Primary Factors Manipulated:
    ◦ Object interactivity (low, medium, high)
    ◦ Presence or absence of UGC
    ◦ Avatar proximity (measured continuously)
3) Procedure
• Session Duration: ~1 hour per pair
• Steps:
    1. Briefing: Consent, background questionnaire
    2. Tutorial: Participants trained on VR/AR navigation and interactions
    3. Two Main Sessions (counterbalanced VR and AR use):
        ▪ Each session involved viewing 7 virtual cultural heritage objects in an HVAR environment.
        ▪ Participants were encouraged to explore freely and discuss the exhibits.
        ▪ mEEG and behavioral data collected in VR user only.
    4. Post-session: Participants completed the ITC-SOPI and UES questionnaires.
    5. Debriefing/Interview: Semi-structured feedback collected.
4) Task
Participants were asked to:
• Explore the virtual exhibition room together (one in VR, one in AR).
• Interact with virtual heritage objects of varying interactivity.
• Engage in optional conversation with their partner.
• Identify the chronological order of the exhibits.
• Reflect on interesting or notable aspects of the exhibits.
5) Metrics Collected
Subjective (Questionnaires):
• ITC-SOPI (Presence, Engagement, Ecological Validity, Negative Effects)
• User Engagement Scale (UES) (Focused attention, Aesthetic appeal, Reward, Perceived usefulness)
Objective (Real-time metrics in VR only):
• Physiological:
    ◦ mEEG (Muse headband): Recorded brain activity at 1-second intervals
    ◦ Calculated indices:
        ▪ Engagement index: Beta / (Alpha + Theta)
        ▪ Arousal index
        ▪ Valence index
• Behavioral:
    ◦ User activity monitoring via Unity:
        ▪ Gaze direction and duration (objects, avatars, labels)
        ▪ Controller interactions
        ▪ Avatar proximity (Euclidean distance between users)
        ▪ Teleportation and movement data","ITC-Sense of Presence Inventory,  User Engagement Scale (UES)","Interpersonal Distance,  Gaze Analysis,  Movement Trajectories",EEG,,"Questionnaires,  Behavioural,  Physiological"
ID094,First Impressions Matter! IVR Haptic Feedback Effect on User Perception Towards Non-Player Characters.,"Dzardanova  Elena,  Kasapakis  Vlasios",,10.1145/3555858.3555911,2022,International Conference on the Foundations of Digital Games,VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This study explores how vibrotactile haptic feedback influences users’ first impressions of non-player characters (NPCs) and the surrounding virtual environment in immersive VR (IVR). Thirty participants were divided into two groups: one received full haptic feedback (FHF) through a ManusVR glove during touch interactions with an NPC, while the other received no haptic feedback (NHF). Results show that participants in the FHF group perceived the NPCs and environment more positively, including feelings of comfort, perceived NPC friendliness, and environmental hospitality. Additionally, physiological data (EDA) revealed increased arousal in the FHF group after initial touch, suggesting stronger emotional engagement.","Here is a summary of the experiments conducted in the paper, including the requested information:
1. Participants
• A total of 30 participants were involved in the study, with 15 individuals in each group (No Haptic Feedback (NHF) and Full Haptic Feedback (FHF)).
• The average age of participants was 24 years old https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=10473,11173 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=8332,9113.
2. Study Design
• The study utilized a between-groups design, 
comparing two conditions: one group experienced no vibrotactile feedback
 (NHF), while the other group received full vibrotactile feedback (FHF) 
during interactions with the NPC https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=5551,6027, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=12287,13152.
3. Procedure
• Participants were equipped with an HTC Vive Pro HMD and ManusVR Prime One gloves for interaction.
• They were seated at a table across from a humanoid NPC in a minimalistic virtual environment with no visible walls or objects.
• Participants were instructed to touch the NPC's left hand, which 
would pull back and maintain eye contact. After 10 seconds, a green box 
appeared, and participants were instructed to touch it https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=6705,7480, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=6028,6704.
• Each participant wore an Empatica E4 wristband to record physiological metrics (Electrodermal Activity and Heart Rate) during the experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=8332,9113, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=9850,10472.
4. Task
• The primary task involved participants touching the NPC and then 
interacting with a green box that appeared in front of the NPC. They 
were also required to answer seven rapid questions regarding their 
perceptions of the NPC and the environment immediately after the 
interaction https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=6705,7480, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=6028,6704.
5. Metrics Collected
• Physiological Metrics: Electrodermal Activity (EDA) and Heart Rate (HR) were recorded to assess emotional responses during the interaction.
• Questionnaire Responses: Participants answered 
seven rapid questions about their perceptions of the NPC and the 
surrounding space, including questions about the NPC's gender, emotional
 state, and the environment's hospitability https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=9114,9849 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea5d6cbb7049c45c23c2e5/?range=11624,12286.
This structured approach allowed the researchers to evaluate the 
effects of haptic feedback on user perceptions and experiences in 
immersive virtual reality settings.",Custom made,,"EDA / GSR (Skin Conductance),  Heart Rate",,"Questionnaires,  Physiological"
ID095,Fostering Empathy in Social Virtual Reality through Physiologically Based Affective Haptic Feedback,"Hecquard  Jeanne,  Saint-Aubert  Justine,  Argelaguet  Ferran,  Pacchierotti  Claudio,  Lécuyer  Anatole,  Macé  Marc",,10.1109/WHC56415.2023.10224380,2023,World Haptics Conference,VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Telecommunications and Collaboration,"The study evaluates user experience in social VR by examining how affective haptic feedback (based on physiological signals) influences empathy, presence, embodiment, and anxiety. It measures subjective empathy levels, anxiety, presence, and embodiment, along with behavioral gaze tracking and reaction time. It compares self-reported empathy, anxiety, and presence scores between two haptic feedback conditions (sympathetic vs. indifferent). Behavioral measures (reaction time, gaze tracking) further quantify how users engage with the virtual presenter.
Sympathetic haptic feedback (dynamic stress cues) was preferred and enhanced perceived connection to virtual agents, though empathy scores did not significantly differ. Strong correlation between anxiety and empathy suggests haptic feedback’s role in emotional engagement.","1) Participants
• Total: 38 participants (27 male, 11 female).
• Demographics:
    ◦ Age range not explicitly stated (likely adults, given context).
    ◦ No prior experience with haptic feedback in VR required.
    ◦ Normal or corrected-to-normal vision.
    ◦ No reported sensory impairments.
• Recruitment: Institutional ethical approval obtained; participants provided informed consent.
2) Study Design
• Design: Within-subjects, counterbalanced.
• Conditions:
    ◦ Sympathetic (Dynamic) Haptic Feedback:
        ▪ Haptic feedback (compression belt + vibrotactile wrist actuator) mirrored the virtual presenter’s stress (e.g., faster heartbeat/shallow breathing during stressors).
    ◦ Indifferent (Constant) Haptic Feedback:
        ▪ Static, relaxed feedback (60 BPM heartbeat, 15 breaths/min) regardless of presenter’s stress.
• Independent Variable: Type of haptic feedback (sympathetic vs. indifferent).
• Dependent Variables:
    ◦ Subjective: Empathy, anxiety, presence, embodiment (via questionnaires).
    ◦ Behavioral: Reaction time to assist presenter, gaze patterns (eye-tracking).
3) Procedure
• Pre-Experiment:
    ◦ Participants completed an adapted Interpersonal Reactivity Index (IRI) to assess baseline empathy traits.
    ◦ Familiarization with VR environment (3–5 mins):
        ▪ Explored empty virtual meeting room.
        ▪ Practiced avatar embodiment (mirror exercise).
• Experimental Phase:
    ◦ Two 4m27s VR meeting scenarios (order counterbalanced):
        1. Sympathetic Condition: Haptic feedback dynamically matched presenter’s stress (e.g., accelerated heartbeat during rude comments).
        2. Indifferent Condition: Constant, relaxed haptic feedback.
    ◦ Stressor Events: Presenter faced three scripted stressors (e.g., dry throat, rude colleagues); participants could intervene (e.g., hand water, offer support).
    ◦ Mid-Experiment Questionnaires: After each condition, rated empathy, anxiety, presence, and embodiment.
• Post-Experiment:
    ◦ Compared preferences between conditions (e.g., ""Which feedback enhanced empathy more?"").
    ◦ Open-ended feedback on haptic experience.
4) Task
• Social VR Interaction:
    ◦ Participants attended a virtual professional meeting where a female presenter discussed COVID-19 guidelines.
    ◦ Presenter’s Stressors:
        1. Laser pointer failure (100s).
        2. Dry throat (180s).
        3. Rude comments from colleagues (250s).
    ◦ User Intervention:
        ▪ Physical actions (e.g., handing water) or verbal support (e.g., positive feedback).
    ◦ Behavioral Metrics:
        ▪ Reaction Time: Time to assist presenter (e.g., delay in handing water).
        ▪ Gaze Tracking: Percentage of time spent looking at presenter vs. objects.
5) Metrics Collected
• Subjective (Questionnaires):
    ◦ Pre-Experiment:
        ▪ IRI (Adapted): Trait empathy (Perspective Taking, Fantasy, Empathic Concern, Personal Distress).
    ◦ Mid-Experiment (Post-Condition):
        ▪ Empathy: ""I felt connected to the presenter"" (7-point Likert).
        ▪ Anxiety: ""What happened made me uncomfortable.""
        ▪ Presence/Embodiment: Standardized scales (e.g., Slater-Usoh-Steed).
    ◦ Post-Experiment:
        ▪ Preference ranking (sympathetic vs. indifferent feedback).
        ▪ Open-ended feedback (e.g., ""The belt made me feel anxious"").
• Behavioral:
    ◦ Reaction Time: Seconds to assist presenter (e.g., 0.12–14.88s range).
    ◦ Gaze Behavior: Eye-tracking data (% time on presenter: 64.22% median).
• Physiological:
    ◦ Simulated via haptics (no direct biosensors).","Custom made,  IRI,  Embodiment Questionnaire","Reaction Time,  Gaze Analysis",,Task Success / Completion,"Questionnaires,  Behavioural,  Performance"
ID096,gender differences in perceiving avatar face and interpersonal distance: exploring realism and social presence in mixed reality,"Woo  S. Kang,  A. Nguyen,  B. Yoon,  K. Kim,  W.",,10.1109/ISMAR62088.2024.00024,2024,International Symposium on Mixed and Augmented Reality (ISMAR),MR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,"Communication & Avatar Design, Social Interaction","This study examines gender differences in perceiving avatar facial expressions and interpersonal distances in Mixed Reality (MR) communication. Thirty-six participants (18 male, 18 female) engaged in avatar-mediated conversations under six conditions varying in interpersonal distance (closer: 1.3 m; farther: 3.3 m) and facial expression complexity(Full blendshapes, Mouth-only, Emotion-based). Participants rated facial animation realism and social presence(Networked Minds Social Presence Inventory).Results revealed that female participants were more sensitive to both avatar proximity and expressiveness: they rated higher facial animation realism, copresence, message understanding, and affective understanding at farther distances, and preferred emotion-based expressions over mouth-only animations. Male participants showed minimal variation across distances and expression conditions. The study highlights how gender-specific perceptual differencesinfluence realism and social presence, and suggests adaptive avatar design strategies that allow adjustable facial expressiveness and interpersonal distance settings to improve inclusivity and comfort in MR communication.","1) Participants
• Number: 36 (18 male, 18 female)
• Age: 19–38 years (M = 26.5 ± 3.4)
• Experience: Varied; ~⅔ had prior VR experience.
• Vision/Hearing: All normal or corrected.

2) Study Design
• Type: 2 × 3 × 2 mixed factorial design.
– Within-subject factors:
  • Interpersonal Distance: Closer (1.3 m) vs. Farther (3.3 m)
  • Facial Expression: Full (52 blendshapes), Mouth-only (26 blendshapes), Emotion-based (37 blendshapes)
– Between-subject factor: Participant Gender (male vs. female).
• Counterbalancing: Balanced Latin square.
• Objective: Explore gender-based differences in perceived facial animation realism and social presence under different nonverbal cue conditions.

3) Task Scenario
• Participants engaged in semi-scripted avatar-mediated communication.
• Avatar (male) discussed six environmental topics (e.g., pollution, climate change); participant acted as listener and gave short verbal feedback.
• Avatar’s facial animation was pre-recorded for consistency.
• Participants rated the experience after each condition and were interviewed post-session (~20 min).

4) Apparatus• Hardware: Meta Quest 3 HMD, passthrough mode.• Software: Unity 2022, Character Creator 4 for avatar, Apple ARKit blendshapes (52).• Distance control: Virtual desk and chairs adjusted for proxemic realism.• Graphics: NVIDIA RTX 4090 laptop GPU for smooth rendering.

5) Metrics Collected
• Questionnaires – Networked Minds Social Presence Inventory (Biocca & Harms):
– Subscales: Co-presence, Attentional Allocation, Message Understanding, Affective Understanding, Emotional & Behavioral Interdependence.
• Questionnaires – Facial Animation Realism (Fraser et al., 2022): Naturalness and realism (7-point Likert).
• Qualitative Metrics: Semi-structured interviews on comfort, expressiveness, realism, and gendered perceptions.
• Reliability: Cronbach’s α > 0.9 for all subscales.

Main Findings
• Gender × Distance Interaction:
 – Females rated higher facial animation realism, copresence, message understanding, and affective understandingat farther distances (3.3 m).
 – Males showed no significant change across distances.
• Gender × Facial Expression Interaction:
 – Females reported higher copresence and message understanding for Emotion-based vs. Mouth-only expressions.
 – Males preferred the Full blendshape condition for behavioral interdependence.
• Qualitative Insights:
 – Females valued emotional clarity and holistic expression (eyebrows + mouth).
 – Males emphasized functional realism for comprehension (eye/mouth accuracy).
 – Both suggested adaptive control of facial expressiveness depending on distance.
• Design Implications: Adaptive avatar systems should personalize interpersonal distance and expressivity for inclusivity and user comfort.","Facial Animation Realism,  Social Presence",,,,Questionnaires
ID097,Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality,"Adkins  Alex,  Canales  Ryan,  Jörg  Sophie",,10.1145/3641825.3687718,2024,ACM Symposium on Virtual Reality Software and Technology,VR,Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,Telecommunications and Collaboration,"This paper investigates how input devices (hand tracking vs. VR controllers) and the presence of audio communication affect collaborative experience and performance in a shared VR task based on NASA’s “Survival on the Moon”. In a 2×2 between-subjects design with 120 participants (15 dyads per condition), pairs solved the survival ranking task first individually and then collaboratively while embodied as full-body astronaut avatars. The authors measured social presence, perceived comprehension, team cohesion, task workload, task performance, group synergy, and task duration. Results show that enabling voice audio significantly increases social presence, perceived comprehension, and team cohesion, but also increases perceived effort workload, group task duration, and worsens strong group synergy. Interaction controls (hand tracking vs. controllers) had limited impact on social presence and performance; however, participants with hand tracking reported lower effort workload overall, and hand tracking without audio led to higher mental workload than controllers without audio, suggesting different cognitive demands when gesturing becomes the main communication channel.","Participants
• 120 participants were recruited for the study (forming 60 dyads) across four experimental conditions.
• Participants were randomly paired into dyads and assigned to one of the 2 × 2 conditions (Hand Tracking vs. Controllers × Audio vs. No Audio).
• The study reports no exclusions due to sickness or technical issues; all dyads completed the experiment.
• Participants interacted in VR using full-body motion-captured avatars representing astronauts.

2. Study Design
• The study used a 2 × 2 between-subjects design, manipulating:
    1. Input Modality:
• Hand Tracking
• VR Controllers
    2. Communication Channel:
• Audio ON (spatialized voice chat)
• Audio OFF
• Each dyad experienced only one condition.
• The collaborative task was adapted from NASA’s ""Survival on the Moon"".
Procedure
1. Pre-VR Setup
    ◦ Participants entered a lab space, were briefed on the study, and completed consent procedures.
    ◦ Each participant completed the individual version of the survival ranking task before entering VR.
2. In-VR Phase
    ◦ Participants donned Meta Quest 2 HMDs.
    ◦ Each participant performed the individual VR version of the survival ranking task alone.
    ◦ Dyads were then placed together in a shared virtual lunar environment to perform the collaborative ranking task.
    ◦ Voice chat was enabled or disabled depending on condition.
    ◦ Avatars were full-body with IK using hand tracking or controllers.
3. Post-VR Phase
    ◦ Participants answered a set of post-task questionnaires assessing social presence, perceived comprehension, team cohesion, and task workload.
    ◦ Task performance scores were automatically computed based on NASA’s scoring scheme.

Tasks
• NASA “Survival on the Moon” Ranking Task:
    ◦ Individual Phase: Each participant ranked 15 survival items by importance.
    ◦ Collaborative Phase: The dyad negotiated and produced a joint ranking.
• The task required discussion (when audio was available) and gestural communication (especially relevant in no-audio + hand-tracking conditions).
• The task format allowed comparison of individual vs. group accuracy, negotiation complexity, and cooperation efficiency.

Metrics Collected
• Questionnaires
    ◦ Social Presence (Nowak & Biocca, SP1–SP6)
    ◦ Perceived Comprehension (Networked Minds – comprehension items, PC1–PC6)
    ◦ Team Cohesion (Michalisin et al., TC1–TC6)
    ◦ Task Workload (SIM-TLX) – mental demand, physical demand, temporal demand, performance, effort, frustration
• Performance Metrics
    ◦ Individual Ranking Error (0–112 NASA scoring)
    ◦ Group Ranking Error
    ◦ Weak Group Synergy (group score vs. mean individual scores)
    ◦ Strong Group Synergy (group score vs. best individual score)
    ◦ Task Duration for both individual and group phases
• System Conditions
    ◦ Input modality: Hand Tracking vs. Controllers
    ◦ Communication channel: Audio vs. No Audio","NASA-TLX,  Networked Minds Questionnaire,  Social Presence,  Team Cohesion",,,,Questionnaires
ID098,Hands-Free Selection in Scroll Lists for AR Devices,"Drewes  Heiko,  Fanger  Yara,  Mayer  Sven",,10.1145/3670653.3670671,2024,Mensch Und Computer,AR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,,"This paper investigates how users perform hands-free selection in scroll lists on AR head-mounted displays by comparing four interaction techniques: Hand input (baseline), Eye-Dwell, Head–Gaze Offset, and Head Gestures. In a within-subject study with 25 participants, each participant completed five selection tasks per method, choosing items located at the top, middle, or bottom of a long scroll list.
The authors measured task completion time, error rate, scroll behavior, and collected SUS, NASA-TLX, and partial SSQ ratings. Results show that Head Gesture interaction was the fastest and most preferred method, combining low mental and physical effort. Eye-Dwell was the slowest due to long dwell time and low scroll speed. Head–Gaze Offsethad higher error rates because of its complexity. Hand input performed well but was physically more demanding due to mid-air interaction (gorilla-arm effect). The paper concludes that users prefer methods with minimal gaze control and moderate head movement, giving generalizable design recommendations for AR hands-free interfaces.","The study recruited 25 participants (23–60 years old, mean 37). After demographics and device familiarization, participants completed an eye-tracking calibration and a training phase for all four techniques. The experimental session used a within-subject Latin-square design, where each participant performed five selection tasks per condition (top/middle/bottom list locations randomized).
Participants saw a scroll list of 50 items (8 visible at a time) and had to scroll and select the correct item using one of the methods. Performance (completion time, errors) was automatically logged. After each interaction method, participants filled out SUS and NASA-TLX, and after the full experiment they completed a short SSQ subset (General Discomfort, Fatigue, Headache) to assess validity and discomfort.
The apparatus included:
• HoloLens 2 with built-in eye tracking
• Custom Unity list UI
• Interaction logic implementing the four methods (Eye-Dwell, Hand, Head-Gesture, Head–Gaze Offset)","NASA-TLX,  SSQ (Simulator Sickness Questionnaire),  System Usability Scale (SUS)",,,"Completion Time,  Error Rate","Questionnaires,  Performance"
ID099,Haptic Space: The Effect of a Rigid Hand Representation on Presence When Interacting with Passive Haptics Controls in VR,"Elbehery  Mostafa,  Weidner  Florian,  Broll  Wolfgang",,10.1145/3428361.3428388,2020,International Conference on Mobile and Ubiquitous Multimedia,VR,Embodiment Avatars & Social Presence,Visualization Techniques.,Human-Computer Interaction (HCI),"The paper investigates how different visual hand representation modes (no hands, rigid hands, rigid hands with snapping) influence presence and user experience when interacting with passive haptic controls in VR. A user study with 45 participants found that the use of rigid hand representations combined with snapping significantly improves presence (measured via SUS) and user experience (measured via UEQ-S) compared to no hand representation. The IPQ subscales did not show significant differences, suggesting the effect was more general than component-specific. The study offers evidence that low-cost, rigid hand models with simple feedback mechanisms can enhance immersion and user experience in haptics-rich VR environments.","1) Participants
• Number: 45 participants (15 per condition).
• Demographics: Mostly students and staff from a technical university.
• Age: Mean age 28.5 years (SD = 7.2), range 19–56 years.
• VR Experience: 18 had no prior VR experience, 16 had <5 hours, 11 had 10+ hours.
2) Study Design
• Type: Between-subjects design (3 conditions).
• Conditions:
    ◦ NoHands: No hand representation.
    ◦ RigidHands: Rigid 3D hand model that faded out near objects.
    ◦ RigidHands+: Rigid 3D hand model with a snapping mechanism for precise interaction.
• Tracking: HTC Vive controllers strapped to wrists for hand position/orientation.
3) Procedure
• Setup: Participants sat at a real desk matching a virtual desk in VR.
• Narrative: ""TU Ilmenau SpaceLab"" theme—defending Earth by destroying asteroids.
• Steps:
    1. HubLevel: Familiarization with the environment (real/virtual desk alignment).
    2. SpaceLevel: Task execution (interacting with passive haptic controls).
    3. Questionnaires: Post-VR presence and user experience surveys.
• Duration: ~5 minutes in VR.
4) Task
• Primary Goal: Destroy 5 asteroids using a virtual spaceship’s controls.
• Interactions:
    ◦ Push/Pull Sliders: Adjust targeting system (distance/height).
    ◦ Rotating Lever: Control ship’s yaw.
    ◦ Fire Button: Launch missiles.
    ◦ Coffee Mug: Move to a designated spot (initial task).
• Passive Haptics: Real objects (e.g., lever, sliders, button, mug) matched virtual counterparts.
5) Metrics Collected
• Presence:
    ◦ Slater-Usoh-Steed (SUS) Questionnaire: 6-item scale (count of 6/7-point responses).
    ◦ igroup Presence Questionnaire (IPQ): Subscales for Realism, Involvement, Spatial Presence.
• User Experience:
    ◦ User Experience Questionnaire (UEQ-S): Short version assessing pragmatic (utility) and hedonic (enjoyment) qualities, plus total score.
• Analysis: Non-parametric tests (Kruskal-Wallis, Dunn post-hoc) due to non-normal data.
Key Findings
• RigidHands+ significantly improved presence (SUS) and user experience (UEQ) compared to NoHands.
• No significant differences in IPQ subscales (Realism, Involvement, Spatial Presence) across conditions.
• RigidHands+ outperformed RigidHands in UEQ, suggesting snapping mechanisms enhance UX.
Limitations
• Small sample size (non-normal data).
• Short exposure time (5 minutes).
• No task performance metrics (e.g., completion time, errors).
This study demonstrated that even simple rigid hand representations with snapping can enhance presence and UX in passive haptic VR environments.","IPQ (Igroup Presence Questionnaire),  Presence – General,  UEQ-S (Short User Experience Questionnaire)",,,,Questionnaires
ID100,Heavy Is the Hand: Effects of Hand-Tracking Input and Gestures on Locomotion Performance and User Experience in Virtual Reality,"Akhoroz  Mehmet,  Yildirim  Caglar",,10.1145/3701571.3701596,2024,International Conference on Mobile and Ubiquitous Multimedia,VR,Interaction Techniques & input Modalities,,Locomotion & Collision Avoidance,"This paper examines how input method (controller vs. hand-tracking) and locomotion metaphor (teleportation vs gesture-walking) jointly influence locomotion performance, cybersickness, mental workload, interaction naturalness, and user preference in VR. In a 2×2 within-subjects experiment with 24 participants, users completed a maze task (efficiency, collision avoidance) and a spatial awareness task (disorientation), then filled UX questionnaires inside VR.
Key findings:
• Controller Teleportation was the best-performing and most preferred technique, with fastest times, fewest collisions, lowest cybersickness, lowest workload, and highest naturalness.
• Hand-tracking input consistently increased mental workload and cybersickness and reduced naturalness compared to controllers.
• Gesture-walking increased physical and cognitive demands, leading to more collisions and higher SSQ scores than teleportation.
• Hand-tracking Gesture-Walking outperformed Hand-tracking Teleportation, showing that gesture metaphors pair better with hand-tracking.
The study provides design guidelines for future VR locomotion techniques based on these results.","The study recruited 24 participants (20–31 years old), all right-handed and with mixed VR experience, who completed four locomotion conditions: Controller Teleportation, Hand-Tracking Teleportation, Controller Gesture-Walking, Hand-Tracking Gesture-Walking. A within-subjects factorial design counterbalanced condition order.
Participants completed two tasks under each condition:
1. Maze task — users navigated a maze to reach six targets; task measured completion time and number of wall collisions.
2. Spatial Awareness task — a triangle completion task measuring angle error to quantify spatial disorientation.
After each condition, participants answered questionnaires inside VR to avoid breaks in presence. Each condition included: a tutorial, both tasks, and then the subjective UX questionnaires.","SSQ (Simulator Sickness Questionnaire),  CNQ (Controller Naturalness Questionnaire),  MWQ (Mental Workload Questionnaire)",,,"Accuracy,  Completion Time,  Error Rate","Questionnaires,  Performance"
ID101,How the Presence and Size of Static Peripheral Blur Affects Cybersickness in Virtual Reality,"Lin  Yun-Xuan,  Venkatakrishnan  Rohith,  Venkatakrishnan  Roshan,  Ebrahimi  Elham,  Lin  Wen-Chieh,  Babu  Sabarish V.",,10.1145/3419984,2020,Transactions on Applied Perception,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"The study evaluates how different sizes of a static central window in a peripheral blur visual manipulation (fixed foveated rendering) influence cybersickness in VR. Thirty participants performed spatial tasks across three sessions (normal view, large window, small window). The results showed that peripheral blurring (both large and small windows) significantly reduced cybersickness compared to no blur. While there was no significant difference between the two window sizes in general, adaptation to cybersickness was stronger over sessions with smaller windows. The blur technique did not negatively impact spatial awareness or presence, indicating that peripheral blur is a promising strategy for reducing VR sickness without compromising experience quality.","
1. Participants:
    ◦ Number: 30 participants (15 females, 15 males).
    ◦ Age: Ranged from 19 to 27 years (average age = 22.7 years).
    ◦ Background: Most participants were college or graduate students majoring in engineering or sciences.
    ◦ Vision: All participants had normal or corrected-to-normal vision.
    ◦ Exclusion Criteria: Participants with less than 6 hours of sleep prior to the experiment or those reporting fatigue or body pain were excluded.
2. Study Design:
    ◦ Design: Within-subjects design with three experimental conditions:
        ▪ Normal Viewing (NV): No peripheral blur effect applied.
        ▪ Large Window (LW): Large full-resolution central window with peripheral blur.
        ▪ Small Window (SW): Small full-resolution central window with peripheral blur.
    ◦ Sessions: Each participant attended three sessions (one for each condition) on separate days, with a one-day break between sessions.
    ◦ Counterbalancing: Six different condition orders were used to counterbalance for order effects.
3. Procedure:
    ◦ Introduction: Participants were greeted, provided informed consent, and filled out a demographics questionnaire.
    ◦ Pre-Experiment: Participants completed the Simulator Sickness Questionnaire (SSQ) before each session.
    ◦ Training: Participants were trained on the VR interaction metaphor in the real world (not in VR) to avoid cybersickness carryover effects.
    ◦ VR Session: Participants were seated on a fixed chair and used the HTC Vive controller to navigate a virtual maze. They performed two tasks: a search task and a spatial orientation task.
    ◦ Discomfort Reporting: Participants verbally reported their discomfort levels on a 10-point scale every two minutes during the simulation.
    ◦ Post-Experiment: After the simulation, participants completed the SSQ again, the Igroup Presence Questionnaire (IPQ), and participated in a semi-structured interview.
4. Task:
    ◦ Search Task: Participants located and collected 100 red balls sequentially spawned along a predefined path in the maze. This task guided participants through the maze and induced cybersickness through virtual movements.
    ◦ Spatial Orientation Task: Participants performed 20 trials where they had to recall and point toward the location of an object they had seen earlier in the maze. This task measured spatial knowledge acquisition and induced rotational movements.
5. Metrics Collected:
    ◦ Cybersickness:
        ▪ SSQ: The difference between pre- and post-experiment SSQ scores was used to measure the change in cybersickness.
        ▪ Discomfort Scores: Participants verbally reported their discomfort levels on a 10-point scale every two minutes during the simulation.
    ◦ Task Performance:
        ▪ Search Time: The time interval between collecting two red balls.
        ▪ Error in Spatial Updating: The angular difference between the participant's pointing vector and the ground truth vector to the target location.
    ◦ Presence: Measured using the Igroup Presence Questionnaire (IPQ).
    ◦ Subjective Perceptions: Semi-structured interviews were conducted after each session to gather qualitative feedback on participants' perceptions of the conditions.
Summary:
The study involved 30 participants who completed three VR sessions, each with a different condition (NV, LW, SW). Participants performed search and spatial orientation tasks in a virtual maze while their cybersickness, task performance, and presence were measured. The study used questionnaires (SSQ, IPQ), behavioral metrics (discomfort scores, search time, spatial updating error), and qualitative interviews to evaluate the impact of static peripheral blurring on cybersickness and user experience in VR.","IPQ (Igroup Presence Questionnaire),  MSSQ,  SSQ (Simulator Sickness Questionnaire),  Unstructured Interviews,  verbal feedback",,,"Search Time,  Error Rate","Questionnaires,  Performance"
ID102,Human Factors at Play: Understanding the Impact of Conditioning on Presence and Reaction Time in Mixed Reality,"Chandio  Yasra,  Interrante  Victoria,  Anwar  Fatima M.",,10.1109/TVCG.2024.3372120,2024,Transactions on Visualization and Computer Graphics,MR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This paper investigates how operant conditioning using reward feedback affects presence and reaction time in a Mixed Reality (MR) environment. A between-subjects study compared two groups — one exposed to positive audio-visual feedback (reward) and one without — while performing a reaction-time task involving an animated agent in MR. Results showed that participants in the reward condition experienced significantly higher presence, as measured by the IPQ, and had faster reaction times. The study highlights how conditioning mechanisms influence both subjective and behavioral user experience in MR systems.","1. Participants:
    ◦ The study involved 62 participants, with 60 included in the final analysis (2 were excluded due to technical difficulties).
    ◦ Participants were evenly divided into three groups: positive conditioning (PC), negative conditioning (NC), and control (20 participants per group).
    ◦ Participants had diverse levels of exposure to VR, AR/MR, and gaming, ranging from novices to experienced users.
    ◦ All participants had normal or corrected-to-normal vision and were compensated with $15 for their participation.
2. Study Design:
    ◦ The study used a between-subjects design, meaning each participant was assigned to one of the three conditioning groups (PC, NC, or control).
    ◦ The experiment was divided into two stages: conditioning stage and trial stage.
    ◦ The conditioning stage aimed to shape participants' expectations and perceptions of the MR environment, while the trial stage assessed the impact of conditioning on presence and reaction time.
3. Procedure:
    ◦ Conditioning Stage:
        ▪ Positive Conditioning (PC): Participants interacted with virtual boxes that behaved realistically (e.g., could not float or be passed through). They used hand gestures to grab, move, and stack the boxes.
        ▪ Negative Conditioning (NC): Participants interacted with virtual boxes that behaved unrealistically (e.g., floated in the air, could be passed through, and lacked physical interactions).
        ▪ Control: Participants did not undergo any conditioning and proceeded directly to the trial stage.
        ▪ After 3 minutes of interaction, participants were asked to rate their sense of connection to the virtual world on a scale of 1 to 10.
    ◦ Trial Stage:
        ▪ All participants, regardless of conditioning group, engaged in a memory puzzle task involving a 3x3 grid of buttons.
        ▪ Participants were shown a sequence of highlighted buttons and had to replicate the sequence by pressing the buttons in the correct order.
        ▪ After each trial, participants were given a 25-second break and asked the same prompt question about their sense of connection to the virtual world.
        ▪ The trial stage consisted of 10 trials, and participants received auditory feedback (correct or incorrect) based on their performance.
4. Task:
    ◦ Conditioning Stage:
        ▪ Participants interacted with virtual boxes using hand gestures. In the PC group, the boxes behaved realistically, while in the NC group, the boxes behaved unrealistically.
    ◦ Trial Stage:
        ▪ Participants performed a memory puzzle task where they had to replicate a sequence of button presses on a 3x3 grid. The task was designed to measure reaction time and assess the impact of conditioning on presence.
5. Metrics Collected:
    ◦ Presence: Measured using three questionnaires:
        ▪ Witmer and Singer Presence Questionnaire (PQ)
        ▪ Igroup Presence Questionnaire (IPQ)
        ▪ Slater-Usoh-Steed Questionnaire (SUS)
        ▪ Participants also answered a prompt question (""How connected do you feel to this virtual world?"") on a 10-point scale during the conditioning stage and between trials.
    ◦ Reaction Time: Measured in milliseconds (ms) during the trial stage. The time taken to press the buttons in the correct sequence was recorded.
    ◦ Task Performance: The accuracy of the button sequences was logged to ensure participants were engaged in the task, though this was not the primary focus of the study.
Summary:
The study involved 60 participants divided into three groups (positive conditioning, negative conditioning, and control) to explore the impact of conditioning on presence and reaction time in MR. Participants first underwent a conditioning stage where they interacted with virtual boxes that either behaved realistically (PC) or unrealistically (NC). In the trial stage, all participants performed a memory puzzle task to measure reaction time and presence. Metrics collected included presence (via questionnaires and prompt questions) and reaction time (measured during the trial stage). The study demonstrated that conditioning significantly impacts presence and that reaction time can be used as an objective measure of presence in MR.","Group Presence Questionnaire (IPQ),  Presence – General",Reaction Time,,Task Success / Completion,"Questionnaires,  Behavioural,  Performance"
ID103,Human Influential Factors Assessment During At-Home Gaming with an Instrumented VR Headset,"Moinnereau  Marc-Antoine,  Oliveira  Alcyr,  Falk  Tiago H.",,10.1109/QoMEX55416.2022.9900912,2022,International Conference on Quality of Multimedia Experience,VR,User states: Cognitive & Affective Experience,,Entertainment and Gaming,"This paper presents a pilot study that assesses human influential factors (HIFs)—including engagement, emotion, presence, flow, and cybersickness—during VR gaming in at-home environments using a custom instrumented VR headset (iHMD). The headset includes EEG, ECG, and EOG sensors to collect multimodal physiological data. Eight participants played Half-Life: Alyx at home, and their subjective experience was recorded using a 10-point Likert questionnaire. Physiological features (e.g., EEG engagement scores, HRV, blink and saccade rates) were correlated with subjective measures, showing promising associations. The study shows that real-time, ecologically valid assessmentsof user experience are possible using wearable biosensing in VR.","1) Participants
• Number of Participants: 8 (3 female)
• Mean Age: 28.9 years (± 2.9)
• Ethics Approval: Received from the INRS Ethics Committee  .
2) Study Design
• Type: Pilot study conducted remotely during COVID-19 lockdowns.
• Setting: Participants played a VR game (Half-Life Alyx) at home using a plug-and-play instrumented VR headset  .
3) Procedure
• Equipment Delivery: A box containing two laptops and the VR headset was delivered to participants' homes.
• Remote Monitoring: The experimenter used ""Teamviewer"" and a videoconference session to monitor signal quality and provide instructions  .
• Gameplay Sessions: Participants engaged in two gaming sessions:
    ◦ Session 1: Exploration phase.
    ◦ Session 2: Puzzle-solving and fighting phases.
• Duration: Approximately 1 hour and 30 minutes of total gameplay  .
4) Task
• Game Played: Half-Life Alyx.
• User Experience Questionnaire: After gameplay, 
participants completed a questionnaire assessing various factors such as
 presence, engagement, immersion, flow, usability, emotion, 
cybersickness, and overall experience using a 10-point Likert scale  .
5) Metrics Collected
• Physiological Signals: 
    ◦ EEG: 11 channels recorded to assess engagement, arousal, and valence.
    ◦ ECG: Heart rate (HR) and heart rate variability (HRV) measures.
    ◦ EOG: Eye blink and saccade rates   .
• User Experience Ratings: Collected via a questionnaire with 84 items, including both Likert scale and open-ended questions  .",UEQ (User Experience Questionnaire),,"ECG,  EOG,  EEG",,"Questionnaires,  Physiological"
ID104,I Think I Don’t Feel Sick: Exploring the Relationship Between Cognitive Demand and Cybersickness in Virtual Reality Using fNIRS,"Pöhlmann  Katharina Margareta Theresa,  Maior  Horia A.,  Föcker  Julia,  O'Hare  Louise,  Parke  Adrian,  Ladowska  Aleksandra,  Dickinson  Patrick",,10.1145/3544548.3581063,2023,Conference on Human Factors in Computing Systems,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"This paper investigates how cognitive demand affects cybersickness in VR by simultaneously measuring subjective sickness levels and prefrontal cortical activation using functional near-infrared spectroscopy (fNIRS). Participants experienced a VR driving simulation under low and high cognitive load conditions while their brain activity was recorded. Results show that higher cognitive load (via a 2-back task) was associated with reduced reported cybersickness, and that increased prefrontal activation correlated with this reduction. The study suggests that cognitive distraction may buffer against cybersickness and that fNIRS is a viable physiological method for measuring user state in VR.","
1. Participants:
    ◦ Number of Participants: 40 participants were initially recruited, but data from 10 participants were excluded due to technical issues, early termination due to cybersickness, or lack of reported cybersickness symptoms. The final sample size was 30 participants.
    ◦ Demographics: Participants ranged in age from 18 to 39 years (M = 22.97, SD = 4.97). Seventeen identified as female, eleven as male, and two as gender non-binary.
    ◦ VR Experience: Six participants had never used VR before, while 24 had varying degrees of prior VR experience (21 had used VR less than 10 times, and two had extensive experience).
2. Study Design:
    ◦ Design: The study used a within-subject design with two independent variables:
        ▪ Mental Task Demand (cognitive task vs. no cognitive task).
        ▪ Visually Represented Motion (roller coaster moving vs. stationary).
    ◦ Conditions: There were four experimental conditions:
        ▪ C1: Motion with Cognitive Task.
        ▪ C2: No Motion with Cognitive Task.
        ▪ C3: Motion with No Cognitive Task.
        ▪ C4: No Motion with No Cognitive Task.
    ◦ Duration: Each condition lasted 3 minutes, with each condition presented twice in two blocks (total of 6 minutes per condition).
3. Procedure:
    ◦ Training: Participants were first trained in VR to get used to the controls (e.g., adjusting cybersickness ratings and responding to targets in the cognitive task).
    ◦ fNIRS Setup: After training, the fNIRS device was fitted to measure brain activity.
    ◦ Experiment Blocks: The experiment consisted of two blocks, with each condition presented once per block in a counterbalanced order.
    ◦ Baseline: Before each condition, participants had a 30-second baseline period with a black screen and no sound.
    ◦ Cybersickness Rating: Participants continuously rated their cybersickness using the Fast Motion Sickness Scale (FMS) during the VR experience.
    ◦ Post-Condition Measures: After each condition, participants completed the Simulator Sickness Questionnaire (SSQ) and the NASA-TLX to assess perceived workload.
    ◦ Breaks: Between conditions, participants had a 3-minute break with a black screen and relaxing music.
4. Task:
    ◦ Cognitive Task: Participants performed a Rapid Serial Visual Presentation (RSVP) task, where they had to respond to target images (monsters) presented rapidly in a sequence. In the cognitive task conditions (C1 and C2), participants had to press a button whenever a target appeared. In the no-cognitive task conditions (C3 and C4), the RSVP stream was presented as a fixation point, but participants did not have to respond.
    ◦ Task Parameters: The RSVP stream lasted 180 seconds, with images presented at 2 Hz. Targets were randomly selected, and participants had to respond within 700ms of the target appearing.
5. Metrics Collected:
    ◦ Cybersickness:
        ▪ Fast Motion Sickness Scale (FMS): Participants continuously rated their cybersickness on a scale from 0 (""no sickness at all"") to 20 (""severe sickness"") during the VR experience.
        ▪ Simulator Sickness Questionnaire (SSQ): Participants completed the SSQ before and after each condition to assess overall cybersickness symptoms (nausea, oculomotor discomfort, disorientation).
    ◦ Cognitive Load:
        ▪ NASA-TLX: Participants completed the NASA-TLX after each condition to assess perceived mental workload.
    ◦ Physiological Metrics:
        ▪ fNIRS (functional Near-Infrared Spectroscopy): Brain activity was measured using fNIRS, focusing on the frontal lobe. The device recorded changes in oxygenated (HbO) and deoxygenated hemoglobin (HbR) levels.
    ◦ Performance Metrics:
        ▪ RSVP Task Performance: Task performance was measured using the detection sensitivity index (d') and reaction times. Participants' accuracy and speed in responding to targets were recorded.
    ◦ Behavioral Metrics:
        ▪ Reaction Time: The time taken to respond to targets in the RSVP task was recorded.
Summary:
The study involved 30 participants who experienced a VR roller coaster simulation while performing a cognitive task (RSVP). The experiment used a within-subject design with four conditions combining motion and cognitive task demand. Participants continuously rated their cybersickness using the FMS, completed the SSQ and NASA-TLX questionnaires, and had their brain activity measured using fNIRS. Task performance was evaluated using the RSVP task, with metrics including detection sensitivity (d') and reaction times. The study aimed to explore the relationship between cognitive demand, cybersickness, and brain activity in VR.","Fast Motion Sickness Scale,  NASA-TLX,  SSQ (Simulator Sickness Questionnaire)",,EEG,Accuracy,"Questionnaires,  Physiological,  Performance"
ID105,Impact of Auditory and Audiovisual Distractors on Task Performance in a VR-based Auditory Attention Task,"Moraes  Adrielle,  Hynes  Eoghan,  Flynn  Ronan,  Hines  Andrew,  Murray  Niall",,10.1109/ISMAR62088.2024.00130,2024,International Symposium on Mixed and Augmented Reality (ISMAR),VR,User states: Cognitive & Affective Experience,,Education and Training,"This paper examines how auditory and audiovisual distractors affect auditory selective attention in a VR classroom environment. Seventy-seven participants completed two VR tasks—a DISTRACTORS task, where they detected background sounds, and a KEYWORDS task, where they identified target words while ignoring distractors with different temporal positions (interstimulus intervals of −1500 ms, 0 ms, and +1500 ms). The study collected a rich set of user metrics, including reaction time, correct/omission/commission errors, skin conductance levels, pupillometry, gaze fixations, and NASA-TLX workload ratings. Results show that audiovisual distractors are more disruptive than auditory ones, positive ISI (distractors after the keyword) enhances performance, gaze time on distractors correlates negatively with performance, and physiological indicators correlate with attentional load. The paper demonstrates that physiological and gaze-based measurements provide meaningful insights into auditory attention in VR, offering design recommendations for immersive learning and attentional assessment systems.","Participants
• 77 participants, 19–62 years old (M = 29)
• 23 VR-novice; normal hearing and color vision
• 2 excluded due to attention disorders
Tasks & Procedure
• Two VR auditory tasks, each based on listening to short stories:
    ◦ DISTRACTORS task: Press button when any auditory/audiovisual stimulus occurs
    ◦ KEYWORDS task: Detect repeated target word “home” while ignoring distractors
• Between-subjects design for ISI timing groups: −1500 ms, 0 ms, +1500 ms
• Calibration → Baseline physiological data → Tutorial → Two tasks → Post-questionnaires
• Conducted in soundproof studio using HTC Vive + Tobii eye tracking and Empatica E4
Metrics Collected
• Performance metrics:
    ◦ Correct detections (CPT-style)
    ◦ Omission & commission errors
    ◦ Reaction time (controller press)
• Physiological metrics:
    ◦ Skin Conductance Level (SCL)
    ◦ Pupillometry
    ◦ Eye-tracking (fixations & dwell time on speaker vs distractors)
• Subjective metrics:
    ◦ NASA-TLX (mental demand, effort, frustration…)
    ◦ Immersion/presence MOS items
Key Findings
• Audiovisual distractors → lower accuracy + faster detection (multisensory enhancement)
• In KEYWORDS task: ISI = +1500 ms → best reaction time
• Longer gaze dwell on speaker → better performance
• More gaze on distractors → worse performance
• SCL increased with task complexity, indicating cognitive load
• NASA-TLX: high mental demand & effort across all conditions","MOS,  NASA-TLX","Reaction Time,  Gaze Analysis","Eye-Tracking,  EDA / GSR (Skin Conductance),  Pupil Analysis","Accuracy,  Error Rate","Questionnaires,  Behavioural,  Physiological,  Performance"
ID106,Impact of Multimodal Instructions for Tool Manipulation Skills on Performance and User Experience in an Immersive Environment,"Simon  Cassandre,  Boukli-Hacene  Manel,  Lebrun  Flavien,  Otmane  Samir,  Chellali  Amine",,10.1109/VR58804.2024.00087,2024,Conference on Virtual Reality and 3D User Interfaces,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This study investigates how different combinations of communication modalities (verbal, visual, haptic) affect the performance and user experience of learners in a VR-based tool manipulation task, simulating a mentoring setup. Results show that the combination of visual and haptic modalities yields the best performance (accuracy, speed, and movement trajectory). However, the triple combination (verbal-visual-haptic) is preferred subjectively, enhancing the user’s sense of presence, co-presence, and perceived learning experience. The findings underline the value of multimodal instruction in immersive training contexts.","1) Participants
• Total: 32 participants
• Gender: 6 female, 26 male
• Age Range: 20–47 years (Mean = 25.5)
• Experience:
    ◦ 26 had previous VR experience (6 were regular users)
    ◦ 50% had used haptic devices before
• Other: All were right-handed, with normal or corrected-to-normal vision
• Recruitment: University students, staff, and external volunteers
2) Study Design
• Type: Within-subjects design
• Conditions: 4 combinations of instruction modalities:
    1. Verbal + Visual
    2. Verbal + Haptic
    3. Visual + Haptic
    4. Verbal + Visual + Haptic
• Trials: Each participant completed 14 trials per condition
• Counterbalancing: Latin square design to mitigate order effects
3) Procedure
1. Introduction and consent
2. Demographic questionnaire
3. Familiarization with the equipment and environment
4. For each of the 4 conditions:
    ◦ Instruction phase: Participants received movement instructions (amplitude/direction) via the assigned modality combination.
    ◦ Manipulation phase: Participants used a haptic stylus in VR to replicate the instructed movement by moving a sphere.
    ◦ Post-condition questionnaires: NASA-TLX and user experience questionnaire
5. Final ranking questionnaire comparing all 4 conditions
• Duration: ~75 minutes per participant
4) Task
• Goal: Replicate tool (sphere) movements of varying direction (X or Y axis) and amplitude (4–8 cm) based on instructor demonstration.
• Instruction delivery: Pre-recorded to simulate a remote instructor
• Execution: Pick-and-place task using a haptic stylus in VR
• Focus: Speed and accuracy of movement, perceived presence and learning experience
5) Metrics Collected
✅ Objective Metrics
1. Manipulation Time – Time taken to complete each trial
2. Distance Estimation Error – Difference between expected and actual final sphere position
3. Trajectory Similarity – Compared to reference using Dynamic Time Warping (DTW)
✅ Subjective Metrics
1. NASA-TLX – Mental workload (raw and subscales)
2. User Experience Questionnaire – Sense of:
    ◦ Presence (Q1–Q5)
    ◦ Social presence (Q6–Q7)
    ◦ Copresence (Q8–Q15)
    ◦ Learning experience (Q16)
3. Comparison Questionnaire – Participants ranked modality combinations by:
    ◦ Accuracy, clarity, engagement, memorability, preference, etc.","Custom made,  NASA-TLX,  Presence – General,  learning experience",Movement Trajectories,,"Distance Estimation Accuracy,  Completion Time","Questionnaires,  Behavioural,  Performance"
ID107,Impact of the Impairment in 360-Degree Videos on Users VR Involvement and Machine Learning-Based QoE Predictions,"Anwar  Muhammad Shahid,  Wang  Jing,  Ahmad  Sadique,  Khan  Wahab,  Ullah  Asad,  Shah  Mudassir,  Fei  Zesong",,10.1109/ACCESS.2020.3037253,2020,IEEE Access,VR,Content & System Design,User states: Cognitive & Affective Experience.,Entertainment and Gaming,"This study investigates how five technical factors—resolution, quantization parameter (QP), initial delay, single interruption, and multiple interruptions—impact four subjective QoE aspects in VR: immersion, acceptability, reality judgment, and attention captivation. The study uses a subjective experiment with 34 participants watching 360° videos in VR via HTC Vive and rating their experience on a 3-point scale. It then trains a decision tree (DT) model to predict QoE levels based on these parameters, achieving 91–93% accuracy across metrics. The results suggest that interruptions (especially multiple) negatively impact experience more than initial delays, and higher resolution and lower QP significantly enhance immersion and realism.","Users experienced 360-degree VR videos under different video resolutions, quantisation parameters, initial delays, and interruptions to assess the impact on users' immersion, attention, acceptability, and perception of reality within the VR environment.",Custom made,,,,Questionnaires
ID108,Impact of VR and Desktop Gaming on Electroencephalogram (EEG) Ratings,"Hufnal  Daniel,  Johnson  Theodore,  Yilderim  Caglar,  Schofield  Damian",,10.1109/ICECCE52056.2021.9514188,2021,Conference on Electrical Communication and Computer Engineering (ICECCE),VR,User states: Cognitive & Affective Experience,Visualization Techniques.,Education and Training,"The study compares user experience in VR gaming vs. desktop gaming using EEG-based physiological metrics and self-reported UX satisfaction scores (GUESS questionnaire). Results show that VR gaming induces significantly higher presence and meditation levels (EEG alpha waves) than desktop gaming, suggesting greater immersion and relaxation. However, self-reported UX scores did not differ significantly between VR and desktop gaming,indicating that users perceived both gaming experiences as similarly satisfying. The findings suggest that EEG metrics can provide additional insights into immersion and presence beyond traditional self-reported questionnaires,making them valuable for more objective VR user experience assessments.

Physiological Metrics (EEG):
• Attention Level (Beta Waves) – Measures focus and engagement.
• Meditation Level (Alpha Waves) – Indicates relaxation and mental calmness.
• Zone Level (Theta Waves) – Represents flow state and deep concentration.
• Presence (EEG-Derived Score) – Quantifies immersion based on EEG activity.

• 
    ◦ Game User Experience Satisfaction Scale (GUESS) – Evaluates usability, playability, enjoyment, and immersion.","1. Participants:
    ◦ Number: 31 participants.
    ◦ Recruitment: Participants were recruited via email and signed up for a timeslot through a web application.
    ◦ Excerpt: ""A total of 31 participants were recruited in this experiment. Participants were recruited by email and were directed to sign up through a web application for a timeslot for the experiment.""
2. Study Design:
    ◦ Design: Within-subjects design with counterbalancing.
    ◦ Conditions: Two conditions were compared—VR gaming (using Oculus Rift) and desktop gaming (using a flat screen).
    ◦ Independent Variable: Display type (Oculus Rift vs. Desktop).
    ◦ Dependent Variables: Game UX satisfaction (measured by GUESS scores) and EEG data (attention, meditation, zone, and presence).
    ◦ Excerpt: ""The experiment was conducted as a within-subjects design with counterbalancing. The participants experienced both the VR and desktop conditions while wearing the EEG headset.""
3. Procedure:
    ◦ Setup: Participants were seated at a computer, filled out an informed consent form, and were fitted with the Myndband EEG headset.
    ◦ Instructions: Participants were informed about the game and controls for each condition (VR and desktop).
    ◦ EEG Setup: For the VR condition, the Myndband EEG unit was clipped to the Oculus Rift. For the desktop condition, the EEG unit was clipped directly to the headband.
    ◦ Gameplay: Participants played the game for 15 minutes in each condition, starting at the prologue stage in story mode.
    ◦ Data Collection: EEG data was recorded using Myndplay software during gameplay, and the GUESS questionnaire was administered after each session.
    ◦ Excerpt: ""Participants were instructed to sit down at the computer and fill out an informed consent form. The experimenter then placed the Myndband headband around the head of the participants and provided them with information about the games they were going to play. The controls for the game were explained on the specific controllers for each designated condition.""
4. Task:
    ◦ Game: Participants played Defense Grid 2: Enhanced VR Edition in the VR condition and DG2: Defense Grid 2 in the desktop condition.
    ◦ Gameplay Duration: 15 minutes per condition.
    ◦ Excerpt: ""The game was then loaded, and the participants were instructed to play for 15 minutes continuously, starting at the prologue stage in story mode.""
5. Metrics Collected:
    ◦ Physiological Metrics:
        ▪ EEG Data: Attention, meditation, zone (flow), presence, and blink detection.
        ▪ Excerpt: ""The Myndband outputs raw brainwave data at 512Hz (Alpha, Beta, Theta, Delta, and Gamma bands), in addition to providing measures of Attention, Meditation (Relaxation), Zone (Flow), Blink detection, as well as providing a measure of EEG signal quality.""
    ◦ Questionnaires:
        ▪ GUESS (Game User Experience Satisfaction Scale): Usability/Playability, Enjoyment, Creative Freedom, Audio Aesthetics, Personal Gratification, and Visual Aesthetics.
        ▪ Excerpt: ""The GUESS questionnaire is a psychometrically validated construct created to avoid some of the limitations present in other gaming-related measures. It contains nine different subscales that are indicative of video game enjoyment and satisfaction.""",GUESS,,EEG,,"Questionnaires,  Physiological"
ID109,Indicators and Predictors of the Suspension of Disbelief: Children’s Individual Presence Tendencies,"Dengel  Andreas,  Plabst  Lucas,  Fernes  David",,10.1109/VRW52623.2021.00129,2021,Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops,VR,User states: Cognitive & Affective Experience,,"Education and Training, Human-Computer Interaction (HCI)","The study investigates Individual Presence Tendency (IPT), a trait-like characteristic influencing how strongly users experience presence in VR across different immersive conditions. Using the Slater-Usoh-Steed (SUS) Presence Questionnaire, the study finds that technological immersion (e.g., using an HMD vs. a laptop) significantly increases presence, but IPT remains a stable predictor across different setups. This suggests that some users are naturally more prone to experiencing presence than others, independent of the hardware used. The findings highlight the importance of considering individual differences when designing adaptive VR experiences, as user traits can shape presence perception just as much as system characteristics.","1) Participants
• Total: 49 participants
• Demographics:
    ◦ Age range: 18–50 years
    ◦ Mixed experience levels with VR
    ◦ Normal or corrected-to-normal vision
2) Study Design
• Within-subjects design
    ◦ Each participant experienced VR under three different immersion conditions:
        1. Low immersion – VR content displayed on a standard laptop screen.
        2. Medium immersion – VR content viewed in mobile VR using a smartphone headset.
        3. High immersion – VR content experienced on a PC-based HTC Vive HMD.
• Independent Variable:
    ◦ Level of technological immersion (Low, Medium, High).
• Dependent Variables:
    ◦ Presence scores (measured via the SUS Presence Questionnaire).
    ◦ Individual Presence Tendency (IPT), derived from presence scores across conditions.
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants completed a demographic survey and VR experience questionnaire.
2. Experimental Phase:
    ◦ Participants viewed a VR scenario under three different immersion levels (laptop, mobile VR, HMD).
    ◦ After each exposure, participants completed the Slater-Usoh-Steed (SUS) Presence Questionnaire.
3. Post-Experiment Phase:
    ◦ Researchers calculated Individual Presence Tendency (IPT) scores based on presence ratings across conditions.
    ◦ Statistical analysis compared presence scores across immersion levels and examined correlations with IPT.
4) Task
• Passive VR Viewing Task:
    ◦ Participants experienced a virtual scene under different immersion conditions.
    ◦ No active task or interaction was required—the study focused purely on presence perception.
5) Metrics Collected
• Questionnaires (Presence Measurement):
    ◦ Slater-Usoh-Steed (SUS) Presence Questionnaire – Measures perceived presence in VR.
    ◦ Individual Presence Tendency (IPT) Score – Derived by averaging presence scores across conditions to assess personal susceptibility to presence.",Presence – General,,,,Questionnaires
ID110,Influence of Interactivity and Social Environments on User Experience and Social Acceptability in Virtual Reality,"Vergari  Maurizio,  Kojic  Tanja,  Vona  Francesco,  Garzotto  Franca,  Moller  Sebastian,  Voigt-Antons  Jan-Niklas",,10.1109/VR50410.2021.00096,2021,Conference on Virtual Reality and 3D User Interfaces,VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"The study found that social environments (SE)—number and proximity of bystanders—affect perceived privacy, safety, and acceptability of VR usage in public. Higher interactivity increased presence, engagement, and hedonic quality but reduced perceived safety. A significant interaction between SE and interactivity was found for arousal. The study concludes that these factors should be considered in VR system design to enhance adoption and user experience in public contexts.","1) Participants
• Total: 28 participants
• Gender: 21 male, 7 female
• Age: Mean = 24.64 years (SD = 2.6 years, range = 21–31 years)
• VR Experience:
    ◦ 9 participants had no prior experience
    ◦ 5 rated themselves as ""not at all familiar,"" 4 as ""low familiar,"" 14 as ""averagely familiar,"" 4 as ""very familiar,"" and 1 as ""extremely familiar""
• Affinity for Technology Interaction (ATI) Score: Mean = 4.55 (SD = 0.69)
2) Study Design
• Experimental Design: Within-subject, repeated-measures design
• Independent Variables:
    ◦ Social Environment (4 levels) simulated via 360° videos

        1. One distant person
        2. Few distant persons
        3. Few close persons
        4. Many close persons
    ◦ Degree of Interactivity (2 levels)
        1. Low interactivity (static game)
        2. High interactivity (dynamic game)
• Total Conditions: 8 (4 Social Environments × 2 Interactivity Levels)
• Randomization: Conditions were randomized for each participant
3) Procedure
1. Introduction & Consent: Participants were welcomed, introduced to the study, and signed a consent form.
2. Pre-Test Questionnaires:
    ◦ Demographics
    ◦ VR experience
    ◦ Affinity for Technology Interaction (ATI) Scale
3. Familiarization with VR Setup:
    ◦ Introduction to Oculus Quest VR headset and controllers
4. Experimental Task Execution:
    ◦ Participants experienced 8 conditions (randomized)
    ◦ After each condition, they completed post-task questionnaires
5. Post-Test Questionnaires:
    ◦ System Usability Scale (SUS) (assessing overall system usability)
    ◦ Open-ended questions on perceptions, usability, and future applications of VR in social settings
4) Task
• Two VR Games: Participants played two different games requiring different interactivity levels.
• Social Environment Manipulation: Each VR session began with a 360° video showing different levels of social presence.
Game 1: Static Game (Low Interactivity)
• Task:
    ◦ Select numbers in ascending order from 1 to 50
    ◦ Used head movements to look around
    ◦ Used one controller to point and select numbers
• Objective: Complete the task in the shortest time possible
Game 2: Dynamic Game (High Interactivity)
• Task: Perform four different physical interaction tasks using VR controllers
    1. Lateral Grabbing – Grab light blue spheres moving laterally
    2. Forward Grabbing – Grab a falling purple balloon
    3. Dodging – Avoid a dark blue obstacle moving towards the participant
    4. Shielding – Use a shield to block a red spiked ball
• Objective: Successfully complete all tasks while remaining aware of the simulated social environment
5) Metrics Collected
• Self-Reported UX & Social Acceptability Measures:
    ◦ igroup Presence Questionnaire (IPQ) – Measures the sense of presence in VR
    ◦ Short User Experience Questionnaire (UEQ-S) – Measures overall UX, hedonic, and pragmatic quality
    ◦ Self-Assessment Manikin (SAM) – Measures Valence (pleasure), Arousal (excitement), and Dominance (control)
    ◦ Social Acceptability Questionnaire (SAQ) – Evaluates public VR acceptability, interaction, isolation, privacy, and safety concerns
• System Usability & Qualitative Feedback:
    ◦ System Usability Scale (SUS) – Assesses overall usability of the VR experience
    ◦ Open-ended questions – Asked about graphical quality, social environment perceptions, and VR usability in public settings
Key Findings from the Experiment
• Social environments affect UX and social acceptability:
    ◦ Larger social environments (more people, closer proximity) decreased UX and social acceptability
    ◦ Privacy and safety concerns were more prominent in crowded environments
• Interactivity level significantly influences UX:
    ◦ Higher interactivity increased presence, engagement, and enjoyment
    ◦ Static interaction felt less immersive and enjoyable
• Safety concerns were a dominant issue in social VR settings.","IPQ (Igroup Presence Questionnaire),  SAM (Self-Assessment Manikin),  Social Acceptability (SAQ),  UEQ-S (Short User Experience Questionnaire),  Usability – Custom / Rating-based",,,,Questionnaires
ID111,"Investigating a Combination of Input Modalities, Canvas Geometries, and Inking Triggers on On-Air Handwriting in Virtual Reality","Venkatakrishnan  Roshan,  Venkatakrishnan  Rohith,  Chung  Chih-Han,  Wang  Yu-Shuen,  Babu  Sabarish",,10.1145/3560817,2022,Transactions on Applied Perception,VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"The study investigates performance and usability metrics in on-air handwriting for VR, specifically writing speed, accuracy, workload, and user preferences under different input conditions. evaluating how different input modalities (brush, raycast, pointing gesture), canvas geometries (plane vs. hemisphere), and inking triggers (button press vs. haptic feedback) affect writing performance and user experience. Results show that the brush input modality provides the best balance of speed and accuracy, while the pointing gesture leads to higher cognitive workload and fatigue. The button-based inking trigger is faster and more efficient, but users appreciated haptic feedback for enhanced control. The planar canvas was preferred for familiarity, though the hemispherical canvas offered ergonomic benefits. These findings inform the design of more effective handwriting and text input systems in VR, with implications for collaborative work, training, and creative applications.","1) Participants
• Total: 24 participants
• Demographics:
    ◦ Age range: 18–40 years
    ◦ Right-handed, normal or corrected-to-normal vision
    ◦ Varied experience with VR and handwriting tasks
2) Study Design
• Within-subjects design
    ◦ Each participant performed handwriting tasks under different conditions.
    ◦ Independent Variables:
        1. Input Modality:
            • Brush (virtual pen-like interaction)
            • Raycast (laser-pointer style)
            • Pointing Gesture (direct hand movement)
        2. Canvas Geometry:
            • Planar (flat surface)
            • Hemispherical (curved surface)
        3. Inking Trigger:
            • Button Press
            • Haptic Feedback
• Dependent Variables:
    ◦ Writing Speed (Words Per Minute - WPM)
    ◦ Handwriting Accuracy (Deviation from Intended Stroke Path)
    ◦ Cognitive Load (NASA-TLX scores)
    ◦ User Preferences (Ease of Use, Fatigue, Control)
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were introduced to the VR environment and handwriting tools.
    ◦ A training session allowed them to familiarize themselves with the different input methods.
2. Experimental Phase:
    ◦ Participants completed handwriting trials using different input modalities, canvas geometries, and inking triggers.
    ◦ Each trial was timed and evaluated for speed, accuracy, and cognitive effort.
    ◦ NASA-TLX and user preference ratings were recorded after each condition.
3. Post-Experiment Phase:
    ◦ Participants completed a final survey ranking their preferred input method and canvas geometry.
    ◦ Qualitative feedback was collected on usability and fatigue.
4) Task
• VR Handwriting Task:
    ◦ Participants were instructed to write short phrases on a virtual surface using different input methods.
    ◦ The task was designed to measure speed, accuracy, and usability of handwriting interactions in VR.
5) Metrics Collected
• Performance Metrics:
    ◦ Writing Speed (Words Per Minute - WPM) – Measures how quickly participants could write.
    ◦ Handwriting Accuracy (Deviation from Ideal Stroke Path) – Evaluates precision in forming characters.
• Questionnaires (Subjective UX Metrics):
    ◦ NASA-TLX – Assesses cognitive workload and mental effort.
    ◦ User Preference Ratings – Measures ease of use, fatigue, and perceived control.","Feedback Prefernces,  NASA-TLX",,,"Accuracy,  Speed","Questionnaires,  Performance"
ID112,Investigating Noticeable Hand Redirection in Virtual Reality using Physiological and Interaction Data,"Feick  Martin,  Regitz  Kora P.,  Tang  Anthony,  Jungbluth  Tobias,  Rekrut  Maurice,  Krüger  Antonio",,10.1109/VR55154.2023.00035,2023,Conference on Virtual Reality and 3D User Interfaces,VR,Embodiment Avatars & Social Presence,Interaction Techniques & input Modalities.,Human-Computer Interaction (HCI),"This study investigates how redirected hand movement affects users’ sense of embodiment in virtual reality. The authors use a within-subjects design to introduce increasing degrees of redirection to participants’ virtual hand movement during a pointing task. Using questionnaires and detection thresholds, the study evaluates when redirection becomes noticeable and how it impacts three dimensions of embodiment: ownership, agency, and self-location. Results show that ownership and agency degrade when redirection surpasses user detection thresholds, while small redirections go unnoticed. The study provides practical redirection limits to maintain embodiment in VR interaction.","1. Participants
• 32 participants formed into 16 pairs.
• 8 pairs of close friends and 8 pairs of strangers (quota sampling).
• Balanced gender representation: 16 male, 16 female.
• Age range 21–42, mean age 25.
• Prior expertise collected for:
    ◦ Gaming (1–5)
    ◦ VR technology (1–5)
    ◦ Real-life table tennis (1–5)
    ◦ First-person shooter (FPS) games (1–5)
• Many participants experienced gamers; nearly half were VR novices.

2. Study Design
• Within-game repeated test scenarios for two games:
    1. Eleven Table Tennis (ETT) – realistic physics, sports simulation
    2. Blaston – stylized, competitive FPS duel
• Network conditions manipulated across scenarios:
    ◦ 4G
    ◦ 5G
    ◦ Ethernet
    ◦ Ethernet + 100 ms added RTT
    ◦ Ethernet + 200 ms added RTT
• Test scenarios arranged as pairs (Table 1 on page 9) to enable both absolute and comparative ratings.
• Order of games and scenarios fully randomized.

3. Procedure
1. Arrival & Pre-study Questionnaire
    ◦ Consent signing
    ◦ Demographics, prior relationship, expertise (gaming, VR, FPS, table tennis)
    ◦ Competitiveness Index + Big Five Mini-IPIP
2. Setup & Tutorial
    ◦ Participants assigned to separate labs (Lab A & Lab B).
    ◦ Short tutorial explaining mechanics of the first game.
3. Gameplay Sessions
    ◦ Each scenario lasted 2–4 minutes.
    ◦ Participants experienced all network conditions for the first game, then repeated for the second.
    ◦ Voice chat enabled for natural communication.
4. Post-Scenario Questionnaires
    ◦ After each scenario: QoE (ACR), interaction quality (modified GIPS).
    ◦ After every pair of scenarios: CCR comparison ratings.
5. Post-Game Questionnaire
    ◦ GEQ Core Module
    ◦ Social presence (selected items)
    ◦ Competitive feelings, bonding, co-presence, desire to win
    ◦ Perceived skill vs. opponent
    ◦ Preference for single/multiplayer mode

4. Tasks
• Competitive multiplayer gameplay using Meta Quest VR HMDs + controllers.
• For each network scenario in each game:
    ◦ ETT: Play one full set to 11 points.
    ◦ Blaston: Play three combat rounds involving shooting, dodging, and switching weapons.
• Tasks required:
    ◦ Real-time coordination and movement (ETT).
    ◦ Fast reaction, physical dodging, weapon switching (Blaston).
    ◦ Communication using voice chat.","Custom made,  Embodiment Questionnaire","Adaptation Over Repeated Trials,  Hand Movement,  Total Distance Travelled","ECG,  EDA / GSR (Skin Conductance),  Respiration,  EEG",Completion Time,"Questionnaires,  Behavioural,  Physiological,  Performance"
ID113,Investigating Sign Language Interpreter Rendering and Guiding Methods in Virtual Reality 360-Degree Content,Anderton  Craig,,10.1145/3517428.3563373,2022,Conference on Computers and Accessibility,VR,Visualization Techniques,,"Accessibility, Human-Computer Interaction (HCI)","investigation of sign language interpreter rendering and guiding methods within VR 360-degree content, addressing a gap in existing accessibility guidelines that primarily focus on subtitles. By empirically studying the impact of different rendering modes and guiding methods on user experience, the research provides valuable insights for creating more inclusive VR environments for sign language users.

 UX: A custom UX questionnaire with closed questions measured on a 7-point Likert scale
• Qualitative data: Think-aloud protocol, audio recording, paper notes, and semi-structured interviews","1) Participants:
Eight participants were recruited, half of whom were female, with ages ranging from 24 to 64 years old (M = 39). Four participants had some level of BSL fluency, while two participants had fluency in Makaton. The final two participants had no BSL knowledge but had significant prior VR experience.
2) Study design:
A within-subject two-factor independent variable design was used. The independent variables were rendering method (fixed-position, always-visible) and guiding method (arrows, radar). The dependent variables were presence, sickness, usability, and UX.
3) Procedure:
Participants followed a think-aloud protocol throughout the test, with audio recording software and paper notes being used. Each video was watched with the participant wearing an Oculus Rift HMD while sat in a swivel office chair. Participants first watched a one-minute acclimation video in a simplified virtual environment. They then watched eight testing videos, each between three and four minutes long. After each video, participants completed questionnaires for presence, sickness, usability, and UX. Finally, a semi-structured interview was conducted to gather additional preferences and opinions.
4) Task:
Participants were asked to watch 360-degree videos featuring sign language interpreters. In the rendering mode videos, the interpreter told a narrative story. In the guiding method videos, two virtual characters spoke to each other, and participants were asked to follow the active speaker using either arrows or radar.
5) Metrics collected:
• Presence: Igroup Presence Questionnaire (IPQ)
• Sickness: Virtual Reality Sickness Questionnaire (VRSQ)
• Usability: System Usability Scale (SUS) questionnaire
• UX: A custom UX questionnaire with closed questions measured on a 7-point Likert scale
• Qualitative data: Think-aloud protocol, audio recording, paper notes, and semi-structured interviews","IPQ (Igroup Presence Questionnaire),  Likert Scake (7-Point),  System Usability Scale (SUS),  VRSQ (Virtual Reality Sickness Questionnaire),  Semi-structured Interviews,  Think-aloud protocol",,,,Questionnaires
ID114,Investigating Spatial Representation of Learning Content in Virtual Reality Learning Environments,"Belani  Manshul,  Singh  Harsh Vardhan,  Parnami  Aman,  Singh  Pushpendra",,10.1109/VR55154.2023.00019,2023,Conference on Virtual Reality and 3D User Interfaces,VR,Visualization Techniques,User states: Cognitive & Affective Experience.,Education and Training,"Forty-two participants completed an immersive tutorial on operating a laser-cutting machine while the textual instructions were presented in one of four spatial placements: world-anchored, controller-anchored, HMD-anchored, or object-anchored. Knowledge-gain, knowledge-transfer, three cognitive-load instruments (single-item, Leppink, NASA-TLX) and the UEQ were collected. Learning outcomes and cognitive load did not differ significantly among placements, but object-anchored content scored higher on UEQ attractiveness, stimulation and novelty, while controller-anchored and object-anchored panels were most preferred in a follow-up within-subjects study. The authors recommend proximity, dynamicity and cueing of learning content to minimise extraneous workload. ","The paper describes two experiments conducted to investigate the effects of different spatial representations of learning content in Virtual Reality Learning Environments (VRLEs). Below is a summary of the experiments:
Experiment 1: Spatial Representation Effect Study
1. Participants:
    ◦ 42 participants (27 male, 15 female) with an average age of 24.01.
    ◦ Participants had varying levels of experience with VR (mean self-reported experience: 1.88 on a 5-point scale) and familiarity with the topic of laser cutting (mean familiarity: 2.2 on a 5-point scale).
2. Study Design:
    ◦ Between-subjects design with four experimental conditions, each representing a different spatial representation of learning content:
        ▪ World-anchored: Content displayed on a fixed TV screen in the environment.
        ▪ User-anchored (Controllers): Content displayed on a panel anchored to the user's VR controllers.
        ▪ User-anchored (HMD): Content displayed on a panel anchored to the user's head-mounted display (HMD).
        ▪ Object-anchored: Content displayed on a panel anchored to the object associated with the current instruction.
    ◦ Participants were randomly assigned to one of the four conditions.
3. Procedure:
    ◦ Participants signed up for the study and completed a pre-session survey to collect demographics and prior experience with VR and the topic.
    ◦ They were then immersed in a VR simulation designed to teach the operation of a laser cutting machine.
    ◦ After completing the VR tutorial, participants took a post-task survey, which included a knowledge gain test, a knowledge transfer test (performed on a real-world laser cutting machine), and questionnaires to assess cognitive load and user experience.
4. Task:
    ◦ Participants were tasked with learning how to operate a laser cutting machine through an immersive VR tutorial.
    ◦ The tutorial included both factual/conceptual content (e.g., principles of laser cutting) and procedural content (e.g., step-by-step instructions for operating the machine).
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ User Experience Questionnaire (UEQ): Measured attractiveness, perspicuity, efficiency, dependability, stimulation, and novelty.
        ▪ NASA TLX: Assessed cognitive workload (mental, physical, temporal demands, performance, effort, and frustration).
        ▪ Single-item cognitive load measures: Evaluated intrinsic, extraneous, and germane cognitive load.
    ◦ Performance Metrics:
        ▪ Knowledge Gain Test: 8 multiple-choice questions assessing factual, conceptual, and procedural knowledge.
        ▪ Knowledge Transfer Test: Participants performed the learned procedure on a real-world laser cutting machine, with scores based on the number of steps correctly recalled.
    ◦ Behavioral Metrics:
        ▪ Simulator logs and screen recordings were used to track user interactions and behaviors during the VR experience.
Experiment 2: User Preferences Study
1. Participants:
    ◦ 22 participants (14 male, 8 female) from the first experiment, with an average age of 24.54.
2. Study Design:
    ◦ Within-subject design where participants could choose between the four spatial representations of learning content (world-anchored, user-anchored controllers, user-anchored HMD, and object-anchored) during the VR simulation.
    ◦ Participants could switch between placements at any time during the experiment.
3. Procedure:
    ◦ Participants were immersed in the same VR learning environment as in Experiment 1, but with the added ability to change the placement of learning content.
    ◦ After the VR session, participants underwent a semi-structured interview to discuss their preferences, likes, and dislikes regarding the different content placements.
4. Task:
    ◦ Similar to Experiment 1, participants learned how to operate a laser cutting machine through a VR tutorial.
    ◦ The key difference was that participants could dynamically choose and change the placement of learning content during the tutorial.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Semi-structured interviews: Qualitative data on user preferences, likes, dislikes, and rationale for choosing specific content placements.
    ◦ Behavioral Metrics:
        ▪ Simulation logs and screen recordings: Tracked when and how often participants changed the placement of learning content.
    ◦ Performance Metrics:
        ▪ Knowledge Gain and Transfer Tests: Similar to Experiment 1, but not the primary focus of this study.
Key Findings from Experiments:
• Experiment 1: No significant differences in knowledge gain or transfer between the four placements, but the object-anchored placement scored significantly higher on user experience scales (attractiveness, stimulation, novelty).
• Experiment 2: Participants preferred user-anchored (controller) and object-anchored placements. The HMD-anchored placement was found to be obstructive during procedural tasks but convenient for factual/conceptual content.","NASA-TLX,  Single Item Cognitive Load Measures,  UEQ (User Experience Questionnaire),  Semi-structured Interviews",,,,Questionnaires
ID115,Investigating the Effects of Leading and Following Behaviors of Virtual Humans in Collaborative Fine Motor Tasks in Virtual Reality,"Liu  Kuan-Yu,  Wong  Sai-Keung,  Volonte  Matias,  Ebrahimi  Elham,  Babu  Sabarish V.",,10.1109/VR51125.2022.00052,2022,Conference on Virtual Reality and 3D User Interfaces,VR,Embodiment Avatars & Social Presence,,Telecommunications and Collaboration,"This work designs a physics-plausible VR system where a user transports furniture or boxes side-by-side with a virtual human (VH) that either leads—issuing step-by-step commands—or follows the user’s spoken instructions. Twelve participants completed 40 tasks split between a living-room and warehouse scene, experiencing both leader-VH and follower-VH conditions in a counter-balanced, within-subjects study. The researchers collected weighted NASA-TLX workload, GEQ, SPGQ, Networked Minds, UEQ, Simulator Sickness scores and objective metrics such as task time, collisions and grasp attempts. Leader VHs imposed significantly higher workload and required more grasp attempts, yet did not change task time, collisions, game experience, social presence or user-experience ratings. Qualitative feedback showed 11 of 12 participants preferred the follower agent for its ease and freedom. The study is one of the first controlled comparisons of leader-versus-follower agent strategies for fine-motor co-manipulation in VR, suggesting follower-type agents better support efficiency when minimising user burden is paramount.","1. Participants
• A total of 12 participants were recruited from a university campus, consisting of 4 males and 8 females. The age range was 20-30 years, with a mean age of 22.92 years and a standard deviation of 3.42 years https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=32891,33626 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=31540,32289.
2. Study Design
• The study employed a within-subjects design with two conditions: Leader Virtual Human (LVH) and Follower Virtual Human (FVH). Participants were randomly split into two groups, each experiencing both conditions in a counterbalanced manner https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=3177,3746, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=14993,15788.
3. Procedure
• Participants began by signing a consent form and completing a 
demographics questionnaire and a Simulator Sickness Questionnaire. They 
then underwent a training phase to familiarize themselves with the 
controls and communication with the virtual humans (VHs). After 
training, participants completed two sessions, each involving 20 tasks.
 After each session, they filled out several questionnaires, including 
the Game Experience Questionnaire and NASA Task Load Index https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=33627,34456, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=34457,35359.
4. Task
• The tasks involved collaborative object transportation in two environments: a living room and a warehouse. Participants performed tasks such as arranging furniture and shelving items, with a total of 40 tasks across both conditions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=36205,36882, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=32290,32890.
5. Metrics Collected
• Quantitative Metrics: 
    ◦ Task completion time
    ◦ Number of collisions with obstacles
    ◦ Number of attempts to grab and lift the target object
    ◦ Number of commands issued by the participants and VHs https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=37773,38456, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=49882,50778.
• Qualitative Metrics: 
    ◦ Subjective measures collected through questionnaires, including the 
Game Experience Questionnaire, NASA Task Load Index, and User Experience
 Questionnaire https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=39850,40537 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67cad8799e328eb67ceb3b4b/?range=34457,35359.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, tasks, and metrics collected.","NASA-TLX,  SSQ (Simulator Sickness Questionnaire),  Social Presence,  UEQ (User Experience Questionnaire),  Game Experience Questionnaire (GEQ)",,,"Completion Time,  Task Success / Completion","Questionnaires,  Performance"
ID116,Investigating the Impact of Virtual Element Misalignment in Collaborative Augmented Reality Experiences,"Vona  Francesco,  Stern  Michael,  Ashrafi  Navid,  Kojić  Tanja,  Hinzmann  Sina,  Grieshammer  David,  Voigt-Antons  Jan-Niklas",,10.1109/QoMEX61742.2024.10598298,2024,International Conference on Quality of Multimedia Experience,AR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Telecommunications and Collaboration,"This paper examines how virtual element misalignment, positional synchrony, and avatars affect user experience in collaborative augmented reality. Through two controlled studies with 48 participants working in pairs, the authors manipulate divergence in virtual object appearance, positional synchrony between collaborators, and the presence or absence of avatars. User experience is measured through adapted UEQ items, selected Flow State Scale items, and the IPQ, supported by qualitative analysis of communication behavior. The results show that while divergent object perceptions do not hinder collaboration, they substantially increase communication precision and discussion. The strongest determinant of user experience is positional synchrony, which significantly improves perceived learnability and flow, whereas avatar presence plays a minor and context-dependent role. The study provides generalizable insights into how alignment and synchronization influence collaborative AR experience and offers design recommendations for future multi-user AR systems.","Participants
• 48 participants, forming 24 dyads, ages 21–51
• Mixed AR/VR experience levels
Experiment 1 — Divergent vs Convergent Object Perception
• Task: collaboratively sort cubes and spheres by color & shape
• Conditions:
    ◦ Convergence (both users see identical objects)
    ◦ Divergence (one object differs in appearance)
• Measures: UEQ (Perspicuity/Dependability), selected Flow items, qualitative comments
Experiment 2 — Positional Synchrony & Avatars
• Task: one user verbally guides partner to place a sphere in a hidden cube
• Four conditions:
    ◦ Sync-w/o-A: synchronized positions, no avatar
    ◦ Sync-w-A: synchronized positions, avatar active
    ◦ Async-w/o-A: asynchronous positions, no avatar
    ◦ Async-w-A: asynchronous positions, avatar active
• Measures: UEQ, Flow (selected items), IPQ Presence
• Qualitative coding of communication patterns
Main Findings
• Divergence increases discussion and precision but does not damage collaboration
• Positional synchrony strongly improves learnability, Flow, and overall UX
• Avatar presence has a limited and context-dependent effect
• Asynchronous conditions increase confusion, doubt, and communication breakdowns","Flow State Scale,  IPQ (Igroup Presence Questionnaire),  UEQ (User Experience Questionnaire)",,,,Questionnaires
ID117,Investigation of Personal Space Perception in Augmented Reality,"Vergari  Maurizio,  Spang  Robert,  Kojic  Tanja,  Hesse  Britta,  Moller  Sebastian,  Voigt-Antons  Jan-Niklas",,10.1109/QoMEX55416.2022.9900887,2022,International Conference on Quality of Multimedia Experience,AR,Embodiment Avatars & Social Presence,,Entertainment and Gaming,"Using a smartphone-based AR app, 31 participants repeatedly spawned either a male or female virtual character that approached and retreated until the users tapped to mark where the figure first crossed into and then left their personal space. Two placement modalities were compared: a fixed 4.5 m spawn lane and a user-chosen dynamic distance. After every triple of trials participants completed SAM emotion scales, the short UEQ-S, and an adapted Networked Minds social-presence inventory; a System Usability Scale followed the full session. Predefined placement produced wider and more distant personal-space boundaries and slightly lower hedonic quality than dynamic placement. Female characters were allowed closer and evoked lower emotional contagion, while sexual-attraction groups differed in hedonic ratings and dominance feelings. The work emphasises designing AR interactions that keep users in control of avatar proximity.","1. Participants
• Total Participants: 31
• Age Range: 19 to 44 years (Mean = 27.5, SD = 5.4)
• Gender Distribution: 14 identified as female, 17 as male. 
• AR Experience: 74.2% indicated prior use of AR, with an average familiarity score of 3.097 on a scale of 1 (unfamiliar) to 5 (very familiar) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=10979,11826 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=16755,17699.
2. Study Design
• Design Type: 2x2 factorial within-subject design.
• Independent Variables:
    ◦ Virtual Character Gender: Male and Female.
    ◦ Placement Modality: Predefined distance and Dynamic distance https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=13397,14177, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=25545,26509.
3. Procedure
• Participants were invited to a lab room individually.
• They received an introduction to the study and signed a consent form.
• A pre-questionnaire collected demographic information, sexual orientation, gender identity, and previous AR experience.
• Participants were introduced to the proxemic zones model and the definition of Personal Space.
• They interacted with a test application to familiarize themselves with the AR experience.
• Each participant experienced all four conditions (triplets) and completed web-based questionnaires after each condition https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=16088,16754, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=16755,17699.
4. Task
• Participants were required to interact with virtual characters that 
approached them in different conditions (gender and placement modality).
• They had to touch the screen to make the virtual character appear 
and walk towards them, with the option to stop or resume the character's
 movement https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=14178,15094 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=12590,13394.
5. Metrics Collected
• Questionnaires: 
    ◦ Self-Assessment Manikin (SAM) for emotional response.
    ◦ User Experience Questionnaire (UEQ-S) for hedonic quality.
    ◦ Networked Minds Social Presence Inventory (NMSPI) for social presence https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=10190,10978, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=16755,17699.
• Performance Metrics: 
    ◦ Measurements of Personal Space boundaries (inner and outer) based on character approach.
    ◦ Usability ratings using the System Usability Scale (SUS) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=23190,23953 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d003b41d226719c6d17963/?range=16755,17699.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, tasks, and metrics collected.","SAM (Self-Assessment Manikin),  Social Presence,  System Usability Scale (SUS),  UEQ-S (Short User Experience Questionnaire),  Semi-structured Interviews",Interpersonal Distance,,,"Questionnaires,  Behavioural"
ID118,Is Video Gaming a Cure for Cybersickness? Gamers Experience Less Cybersickness Than Non-Gamers in a VR Self-Motion Task,"Pöhlmann  Katharina M. T.,  Li  Gang,  Wilson  Graham,  McGill  Mark,  Pollick  Frank,  Brewster  Stephen",,10.1109/TVCG.2024.3456176,2024,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,,Entertainment and Gaming,"This paper examines whether habitual first-person video gamers—who regularly experience self-motion in virtual environments—show greater resilience to cybersickness when exposed to visually induced self-motion in a VR tunnel-travel task. Using a between-subjects design with 32 VR-naive participants classified as gamers or non-gamers, the authors measure cybersickness and cognitive performance during 20 minutes of induced vection. The results show that gamers consistently experienced significantly less cybersickness (lower FMS and SSQ nausea scores), none dropped out due to sickness, and they demonstrated faster reaction times and better discriminative performance in a visual-attention task. The study argues that gaming may provide a form of transferable habituation to self-motion cues or enhance visuospatial abilities, both of which contribute to sickness resilience. The paper offers broader implications for XR design and experimentation, particularly regarding participant screening, training strategies, and individual-difference factors influencing VR comfort.","Participants
• 32 participants (18–34 years), VR-naive or very limited prior VR use
• Two groups: Gamers (n=15) vs Non-Gamers (n=17)
• Gamers defined via ≥5 h/week first-person games
Procedure
• Screening: motion sickness susceptibility (MSSQ), gaming hours, VR exposure
• Pre-session: SSQ
• VR tunnel-travel task (vection-inducing, 20 minutes)
    ◦ Five 4-min sessions
    ◦ Linearly increasing visual speed
• Integrated cognitive tasks:
    ◦ Visual discrimination (respond to target shape/color)
    ◦ Visuomotor tracking (track moving objects in tunnel)
• After each session: FMS sickness rating (0–20)
• Post-session: SSQ
• Dropout if FMS ≥ 11
Data Collected
• Subjective:
    ◦ SSQ (pre/post)
    ◦ FMS (per 4-min block)
• Performance:
    ◦ Reaction times
    ◦ d-prime (accuracy)
    ◦ Tracking accuracy
• Background:
    ◦ MSSQ
    ◦ Detailed game-genre history","Fast Motion Sickness Scale,  SSQ (Simulator Sickness Questionnaire)",,,Task Success / Completion,"Questionnaires,  Performance"
ID119,Jogging-in-Place: Exploring Body-Steering Methods for Jogging in Virtual Environments,"Hedlund  Martin,  Lundström  Anders,  Bogdan  Cristian,  Matviienko  Andrii",,10.1145/3626705.3627778,2023,International Conference on Mobile and Ubiquitous Multimedia,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"Twelve adults jogged through an eight-turn virtual track using three steering methods that coupled their forward speed to step detection but differed in how turning was controlled: yaw of the head-mounted display, yaw of a phone held in the hand, or yaw of a phone strapped to the torso. Hand- and head-steering allowed significantly faster completion times than torso-steering, while head-steering also induced the highest step count. Collisions with track walls and Simulator Sickness Questionnaire scores did not differ across methods, yet System Usability Scale ratings favoured head and hand control over torso. Participants judged hand steering most user-friendly, head steering most realistic and torso steering the hardest to learn, suggesting that lightweight hand or head interfaces best support energetic in-place jogging without extra sickness or error costs.","1. Participants
• Number of Participants: 12 (9 male, 3 female)
• Age Range: 20 to 29 years (M = 23.8, SD = 2.6)
• Experience with VR: 
    ◦ 8 participants had never or only once experienced VR.
    ◦ 2 participants had tried VR approximately 2-5 times.
    ◦ 2 participants had tried VR more than five times.
    ◦ Only 1 participant had experience with a walk-in-place application https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=13198,14004 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=1,830.
2. Study Design
• Type: Within-subject design
• Independent Variable: Body-steering method with three levels:
    1. Head-based locomotion
    2. Hand-based locomotion
    3. Torso-based locomotion https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=3893,4572, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=25246,26019.
3. Procedure
• Participants provided informed consent and demographic data.
• They were given an overview of the locomotion methods and familiarized themselves with each method through a test task.
• The main experiment involved navigating a predefined route as quickly as possible while avoiding collisions with virtual walls.
• After completing the tasks, participants were interviewed about 
their preferences and experiences with the different body-steering 
methods https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=18948,19836, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=15765,16508.
4. Task
• Participants had to follow a predefined curved route consisting of 
eight left and right turns, totaling 52 meters in length. The goal was 
to reach the endpoint as fast as possible while avoiding collisions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=15765,16508, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=18948,19836.
5. Metrics Collected
• Task Completion Time: Time taken to complete the route.
• Number of Steps: Count of physical steps taken during the task.
• Number of Collisions: Count of collisions with virtual walls along the route.
• Virtual Reality Sickness: Assessed using the Simulation Sickness Questionnaire (SSQ).
• Usability: Evaluated using the System Usability Scale (SUS) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=22766,23204, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67d14f001d226719c6eaa815/?range=23205,23768.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, task, and metrics collected.","SSQ (Simulator Sickness Questionnaire),  System Usability Scale (SUS)",,,"Collisions / Safety Errors,  Steps / Cadence,  Completion Time","Questionnaires,  Performance"
ID120,Measuring Human Trust in a Virtual Assistant Using Physiological Sensing in Virtual Reality,"Gupta  Kunal,  Hajika  Ryo,  Pai  Yun Suen,  Duenser  Andreas,  Lochner  Martin,  Billinghurst  Mark",,10.1109/VR46266.2020.00099,2020,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"The study examines trust in virtual assistants in VR and finds that cognitive load and system accuracy significantly influence user trust. Higher cognitive load, measured via EEG alpha power and NASA-TLX scores, correlates with lower trust levels, suggesting that users struggle to rely on the assistant when experiencing mental strain. Galvanic Skin Response (GSR) and Heart Rate Variability (HRV) indicate that emotional arousal also plays a role in trust formation. Additionally, users are more likely to follow the assistant’s guidance when its accuracy is high, as reflected in behavioral measures like head movement congruency. These findings suggest that adaptive AI systems in VR should minimize cognitive load and optimize accuracy to enhance user trust and engagement.
Questionnaires (Subjective UX Metrics):
• System Trust Scale (STS) – Measures perceived trust in the virtual assistant.
• NASA-TLX – Assesses cognitive workload.
• Subjective Mental Effort Questionnaire (SMEQ) – Captures self-reported cognitive effort.Physiological Metrics:
• EEG Alpha Band Power – Measures cognitive load and mental effort.
• Galvanic Skin Response (GSR) – Indicates emotional arousal and trust dynamics.
• Heart Rate Variability (HRV, LF/HF Ratio) – Evaluates stress and trust-related physiological responses.Behavioral Metrics:
• Head Movement Congruency – Tracks whether users follow the assistant’s guidance, reflecting trust in the system.","1) Participants
• Total: 30 participants
• Demographics:
    ◦ Age range: 18–40 years
    ◦ Mixed experience levels with VR and AI-based systems
    ◦ No history of neurological or cognitive disorders
2) Study Design
• Within-subjects design
    ◦ Each participant interacted with a virtual assistant in VR under different conditions.
    ◦ Independent Variables:
        1. Cognitive Load Levels (low vs. high)
        2. Assistant Accuracy (low vs. high)
• Dependent Variables:
    ◦ Trust ratings, physiological responses, and behavioral compliance (head movement).
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants completed baseline questionnaires on VR experience and cognitive workload.
    ◦ Physiological sensors (EEG, GSR, HRV) were attached for continuous monitoring.
2. Experimental Phase:
    ◦ Participants were guided through two VR sessions, each involving interaction with a virtual assistant that provided navigation instructions.
    ◦ Each session varied cognitive load (simple vs. complex tasks) and assistant accuracy (correct vs. incorrect guidance).
    ◦ Physiological signals were recorded in real-time, along with trust ratings and behavioral responses.
3. Post-Experiment Phase:
    ◦ Participants rated their trust in the assistant and reported cognitive effort using subjective scales (STS, NASA-TLX, SMEQ).
    ◦ Data analysis explored correlations between trust, cognitive load, physiological signals, and behavior.
4) Task
• VR Navigation Task:
    ◦ Participants were placed in a virtual environment where a virtual assistant provided directional guidance.
    ◦ They had to decide whether to follow the assistant’s instructions based on trust in its reliability.
5) Metrics Collected
• Questionnaires (Subjective UX Metrics):
    ◦ System Trust Scale (STS) – Measures perceived trust in the virtual assistant.
    ◦ NASA-TLX – Assesses cognitive workload.
    ◦ Subjective Mental Effort Questionnaire (SMEQ) – Captures self-reported cognitive effort.
• Physiological Metrics:
    ◦ EEG Alpha Band Power – Measures cognitive load and mental effort.
    ◦ Galvanic Skin Response (GSR) – Indicates emotional arousal and trust dynamics.
    ◦ Heart Rate Variability (HRV, LF/HF Ratio) – Evaluates stress and trust-related physiological responses.
• Behavioral Metrics:
    ◦ Head Movement Congruency – Tracks whether users follow the assistant’s guidance, reflecting trust in the system.","NASA-TLX,  STS,  Subjective Mental Effort Questionnaire","Movement Congruency with Avatar,  Head Analysis","EDA / GSR (Skin Conductance),  HRV / IBI,  Heart Rate,  EEG",,"Questionnaires,  Behavioural,  Physiological"
ID121,Mid-Air Gestures for Manipulation of Multiple Targets in the Physical Space: Comparing the Usability of Two Interaction Models,"Vogiatzidakis  Panagiotis,  Koutsabasis  Panayiotis",,10.1145/3489410.3489425,2021,CHI - National subgroup,AR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This paper compares two mid-air gesture interaction models—Address-then-Command (A-t-C) and Address-and-Command (A&C)—in a spatial augmented reality prototype simulating smart home device control. Both models achieved high task success and user satisfaction, but A&C outperformed A-t-C in terms of task completion time and false negative errors, making it the more efficient and robust method. Users rated both systems positively on usability (SUS) and user experience (UEQ), suggesting that gesture-based control of multiple physical targets in XR is both feasible and effective, with the A&C model offering particular usability advantages.

Memorability Errors – Mistakes due to forgetting the gesture-command association
False Positives (Midas Touch Errors) – When a gesture unintentionally triggers a commandFalse Negatives – When a correct gesture fails to be recognized","1) Participants
• Case Study 1 – Address-then-Command (A-t-C):
    ◦ Total: 19 participants
    ◦ Gender: 5 women
    ◦ Age: Range = 26–49 years
    ◦ Mean age (Mage): 40 years
    ◦ Standard deviation (SD): 6.8
• Case Study 2 – Address-and-Command (A&C):
    ◦ Total: 17 participants
    ◦ Gender: 3 women
    ◦ Age: Range = 32–50 years
    ◦ Mean age (Mage): 42 years
    ◦ Standard deviation (SD): 5.3
• In both case studies:
    ◦ 9 participants had previous experience with gesture interfaces.
2) Study Design
• Two independent case studies, each testing one interaction model.
• Metrics and procedures were consistent across studies, allowing for indirect comparison.
• Each study used a between-subjects design—participants experienced only one of the two models.
3) Procedure
1. Training Phase:
    ◦ Participants were introduced to the system and learned the gestures.
2. Interaction Tasks:
    ◦ Participants used mid-air gestures to perform control tasks on multiple smart devices (e.g., lights, speakers).
    ◦ Tasks were completed in a spatial augmented reality smart home simulation.
3. Post-Task Evaluation:
    ◦ Participants filled out SUS and UEQ questionnaires.
    ◦ Performance and error metrics were recorded automatically.
4) Task
• Smart Home Device Control Task
    ◦ Tasks involved turning devices on/off, adjusting settings, or switching between targets.
    ◦ Mid-air gestures were used to issue commands either in sequence (A-t-C) or in combination (A&C).
5) Metrics Collected
• Performance Metrics:
    ◦ Task Completion Time
    ◦ Task Success Rate
• Error Metrics:
    ◦ False Positives (unintended activations)
    ◦ False Negatives (unrecognized gestures)
    ◦ Memorability Errors
• Questionnaires:
    ◦ System Usability Scale (SUS)
    ◦ User Experience Questionnaire (UEQ)","System Usability Scale (SUS),  UEQ (User Experience Questionnaire)",,,"Memorability Errors,  Classification Outcomes,  Completion Time,  Task Success / Completion","Questionnaires,  Performance"
ID122,Modeling Gamer Quality-of-Experience Using a Real Cloud VR Gaming Testbed,"Lee  Kuan-Yu,  Fang  Jia-Wei,  Sun  Yuan-Chun,  Hsu  Cheng-Hsin",,10.1145/3592834.3592877,2023,Workshop on Immersive Mixed and Virtual Environment Systems,,Content & System Design,User states: Cognitive & Affective Experience.,"Entertainment and Gaming, Human-Computer Interaction (HCI), Immersive Media and Streaming Optimization in XR","The study investigates Quality of Experience (QoE) in cloud VR gaming, considering the effects of encoding settings, network conditions, and game genres on user perception. It uses Mean Opinion Score (MOS) to assess gamer QoE and develops regression-based QoE models to predict perceived quality based on measurable Quality of Service (QoS) metrics. Findings indicate that bitrate has the most significant impact on QoE, followed by frame rate and resolution, while cybersickness is more dependent on individual susceptibility.

The study models Quality of Experience (QoE) in cloud VR gaming, showing that bitrate is the most influential factor for perceived visual quality, followed by frame rate and resolution. Lower bitrates significantly reduce immersion and user preference to continue playing, while cybersickness is more dependent on individual susceptibility rather than network conditions. The proposed regression-based QoE models accurately predict user perception based on network parameters, providing insights for optimizing streaming settings in cloud-based VR gaming to balance quality and latency.

Questionnaires (Subjective QoE Measures):
• Mean Opinion Score (MOS) – Measures overall perceived quality of experience.
• Visual Quality Rating – Assesses perceived clarity of the streamed video.
• Immersion Level Rating – Captures user engagement and sense of presence.
• Cybersickness Rating – Evaluates motion sickness severity.
• Continue Playing (Binary Response) – Indicates whether the participant would continue gaming under the tested conditions.","1) Participants
• Total: 30 participants
• Demographics:
    ◦ Age range: 18–40 years
    ◦ Mixed experience levels with VR gaming
    ◦ No reported visual impairments or prior severe motion sickness
2) Study Design
• Within-subjects design
    ◦ Each participant experienced multiple cloud VR gaming sessions with different streaming conditions.
    ◦ Independent Variables:
        1. Bitrate levels (low, medium, high)
        2. Frame rate settings (30 FPS, 60 FPS, 90 FPS)
        3. Resolution settings (low, medium, high)
• Dependent Variables:
    ◦ QoE ratings, immersion levels, cybersickness ratings, and willingness to continue playing.
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were given an introduction to cloud VR gaming and the evaluation process.
    ◦ A baseline questionnaire collected data on prior gaming and VR experience, as well as cybersickness susceptibility.
2. Experimental Phase:
    ◦ Participants played a VR game streamed via a cloud gaming platform, with each session using different streaming conditions.
    ◦ After each session, participants rated their experience using QoE questionnaires.
    ◦ Session order was randomized to prevent learning or fatigue effects.
3. Post-Experiment Phase:
    ◦ Participants provided qualitative feedback on their gaming experience.
    ◦ Data from QoE ratings and network conditions were analyzed to develop predictive models.
4) Task
• Cloud VR Gaming Task:
    ◦ Participants played a pre-selected VR game streamed from a remote server.
    ◦ The game involved navigation, interaction with objects, and task completion to simulate a typical gaming experience.
    ◦ Participants were instructed to engage with the game naturally while experiencing various streaming conditions.
Questionnaires (Subjective QoE Measures):
• Mean Opinion Score (MOS) – Measures overall perceived quality of experience.
• Visual Quality Rating – Assesses perceived clarity of the streamed video.
• Immersion Level Rating – Captures user engagement and sense of presence.
• Cybersickness Rating – Evaluates motion sickness severity.
• Continue Playing (Binary Response) – Indicates whether the participant would continue gaming under the tested conditions.","Cybersickness Rating,  Immersion Level Rating,  MOS,  Visual Quality Rating",,,,Questionnaires
ID123,Modeling the User Experience of Watching 360° Videos with Head-Mounted Displays,"Fan  Ching-Ling,  Hung  Tse-Hou,  Hsu  Cheng-Hsin",,10.1145/3463825,2022,Transactions on Multimedia Computing Communications and Applications,360° Videos with Head-Mounted Displays,Content & System Design,Behavioural Dynamics & Exploration.,"Human-Computer Interaction (HCI), Immersive Media and Streaming Optimization in XR, multimedia content quality assessment in AR should this belongs to Telecommunication?","The study focuses on Quality of Experience (QoE) in 360° video viewing with Head-Mounted Displays (HMDs) and explores subjective (MOS, IS) and objective (visual quality, motion, cybersickness) metrics. It examines how content, human, and context factors influence QoE and models the relationship between user experience ratings and measured factors using regression-based QoE models. Findings indicate content factors (e.g., video quality, bitrate) dominate QoE perception, while cybersickness is more influenced by human factors (e.g., historical motion sickness).

The study models Quality of Experience (QoE) in 360° video streaming on HMDs, identifying key factors influencing user perception. Content factors (e.g., video quality, bitrate) had the strongest impact on perceived image quality and immersion, while human factors (e.g., prior motion sickness history) played a larger role in cybersickness susceptibility. Head and gaze rotation speed correlated with user ratings, indicating movement behavior affects QoE perception. The proposed QoE model integrates subjective (MOS, IS) and objective metrics, providing insights for optimizing immersive video experiences by balancing quality, motion, and user comfort.

Questionnaires (Subjective QoE Measures):
• Mean Opinion Score (MOS) – Measures overall perceived quality of experience.
• Individual Score (IS) – Captures personalized user experience ratings.
• QoE Feature Ratings:
    ◦ Image Quality (IQ) – Perceived visual clarity.
    ◦ Fragmentation (FG) – Level of visual distortion.
    ◦ Immersion (IM) – Sense of presence and engagement.
    ◦ Cybersickness (CS) – User discomfort due to motion-related issues.
    ◦ Attractiveness (AT) – Overall enjoyment and appeal.Behavioral Metrics:
• Head and Gaze Rotation Speed – Evaluates movement behavior and its impact on QoE perception.","1) Participants
• Total: 32 participants
• Demographics:
    ◦ Age range: 18–40 years
    ◦ Normal or corrected-to-normal vision
    ◦ Varied levels of experience with VR and 360° video
    ◦ Some participants had a history of motion sickness
2) Study Design
• Within-subjects design
    ◦ Each participant viewed multiple 360° video sequences under different conditions.
    ◦ Independent Variables:
        1. Content Factors: Video resolution, bitrate, scene complexity
        2. Human Factors: Individual motion sickness susceptibility
        3. Context Factors: Interaction level, movement speed
• Dependent Variables:
    ◦ QoE scores, immersion ratings, cybersickness levels, and head/gaze movement behavior.
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were given an introduction to 360° video in VR and the evaluation process.
    ◦ A baseline questionnaire was completed, assessing prior VR experience and motion sickness susceptibility.
2. Experimental Phase:
    ◦ Participants watched multiple 360° videos while wearing an HMD.
    ◦ Videos varied in resolution, bitrate, and scene type (e.g., static vs. dynamic environments).
    ◦ Head and gaze movements were recorded to measure navigation behavior.
    ◦ After each video, participants rated their experience using subjective QoE questionnaires.
3. Post-Experiment Phase:
    ◦ Participants provided qualitative feedback on their viewing experience.
    ◦ Data from subjective ratings and behavioral metrics were analyzed for QoE modeling.
4) Task
• Passive 360° Video Viewing Task:
    ◦ Participants were instructed to freely explore the VR video content using natural head and gaze movements.
    ◦ They were asked to rate their experience after each video, focusing on quality, immersion, and comfort.
5) Metrics Collected
• Questionnaires (Subjective QoE Measures):
    ◦ Mean Opinion Score (MOS) – Overall perceived quality of experience.
    ◦ Individual Score (IS) – Personalized QoE rating.
    ◦ QoE Feature Ratings:
        ▪ Image Quality (IQ)
        ▪ Fragmentation (FG)
        ▪ Immersion (IM)
        ▪ Cybersickness (CS)
        ▪ Attractiveness (AT)
• Behavioral Metrics:
    ◦ Head and Gaze Rotation Speed – Measures movement behavior and its impact on QoE perception.","MOS,  QoE Feature Ratings",Movement Kinematics,,,"Questionnaires,  Behavioural"
ID124,More Immersed but Less Present: Unpacking Factors of Presence Across Devices,"Bujić  Mila,  Salminen  Mikko,  Hamari  Juho",,10.1145/3573381.3596152,2023,Conference on Interactive Media Experiences,VR,Content & System Design,User states: Cognitive & Affective Experience.,(immersive) journalism,"Eighty-seven participants each consumed The Guardian’s Sea Prayer either as a mobile-HMD 360-video, a desktop 360-video, or a scrollable text-and-stills article, then rated a 15-item Presence scale whose factor analysis yielded Involvement, Distraction, Locality and Naturalness, plus the Immersive Tendencies Questionnaire. Both video formats raised Involvement yet also heightened hardware-related Distraction when compared with the article, while Naturalness and Locality remained unaffected. Overall presence did not differ between HMD and desktop viewing, indicating that extra technological immersion did not strengthen the subjective “being-there” feeling for passive 360° media. High immersive-tendency participants showed amplified contrasts in Involvement and Distraction, underscoring that personal traits modulate presence effects. The authors conclude that mobile VR may boost engagement but simultaneously introduce intrusive distractions, and that platform choice should weigh content type and audience predispositions.","1) Participants
• Total: 87 participants.
• Conditions:
    ◦ HMD-360 (VR): 31 participants.
    ◦ Monitor-360 (2D): 29 participants.
    ◦ Monitor-article (Article): 27 participants.
• Sampling: Non-probability convenience sampling.
• Language: Most participants were non-native English speakers (only one native speaker).
2) Study Design
• Between-Subjects Design: Each participant was assigned to one of three conditions representing different levels of technological immersion:
    ◦ High Immersion (HMD-360): 360-degree video viewed via a Google Daydream VR headset.
    ◦ Medium Immersion (Monitor-360): 360-degree video viewed on a 24-inch 2D monitor.
    ◦ Low Immersion (Monitor-article): Text article with video stills on the same monitor.
• Independent Variable: Technological immersion level (VR, 2D, Article).
• Dependent Variables: Presence (and its sub-dimensions: Involvement, Distraction, Locality, Naturalness), measured via a custom 15-item scale.
3) Procedure
1. Pre-Questionnaire:
    ◦ Administered online days before the experiment.
    ◦ Collected demographics and Immersive Tendencies (using a 14-item scale adapted from Witmer & Singer).
2. Experiment:
    ◦ Conducted in an office room with a research assistant present but unobtrusive.
    ◦ Content: The Sea Prayer (a 7-minute 360-degree immersive journalism video by Guardian VR).
        ▪ VR/2D: Original video.
        ▪ Article: Transcript + 3 video stills.
    ◦ Interaction:
        ▪ VR: Head movements to navigate.
        ▪ 2D: Click-and-drag with a mouse.
        ▪ Article: Scrolling with a mouse.
    ◦ All participants used headphones and sat in a swivel chair.
3. Post-Questionnaire:
    ◦ Measured Presence (19 items, later refined to 15) immediately after the task.
    ◦ Compensation: Two movie tickets per participant.
4) Task
• Primary Task: Experience the assigned media content (VR, 2D, or Article).
    ◦ VR/2D: Watch the 360-degree video passively (no interaction beyond navigation).
    ◦ Article: Read the transcript and view stills.
• Goal: Compare how different media formats affect the sense of presence.
5) Metrics Collected
• Quantitative:
    ◦ Presence:
        ▪ Involvement (engagement with content, e.g., ""How involved were you in the experience?"").
        ▪ Distraction (hardware interference, e.g., ""How much did the control devices interfere?"").
        ▪ Locality (sense of ""being there,"" e.g., ""Did you feel part of the story?"").
        ▪ Naturalness (consistency with real-world experiences, e.g., ""How natural did interactions seem?"").
    ◦ Immersive Tendencies: Pre-existing propensity for psychological immersion (e.g., ""Do you become deeply involved in media?"").
• Qualitative:
    ◦ Observations from post-experiment discussions (e.g., hardware familiarity, novelty effects).
Key Findings
• No Overall Difference in Presence: No significant difference in total presence scores across conditions (VR: 4.39, 2D: 4.21, Article: 4.01; p = .394).
• Sub-Dimensions:
    ◦ Higher Involvement in VR/2D vs. Article (p = .002).
    ◦ Higher Distraction in VR/2D vs. Article (p = .001), attributed to hardware (e.g., HMD discomfort).
    ◦ No Difference in Locality/Naturalness.
• Immersive Tendencies:
    ◦ Participants with high immersive tendencies reported stronger presence (p = .010), especially in Involvement (p < .001).
    ◦ Trait amplified differences in Distraction between immersive and Article conditions (p = .005).
Limitations
• Stimuli: Single 360-degree video (stationary, non-interactive) may limit generalizability.
• Sampling Bias: Convenience sampling and non-native English speakers.
• Hardware: Google Daydream VR (older mobile HMD) may not reflect current VR experiences.
Conclusion
The study challenges the assumption that higher technological immersion (VR) inherently enhances presence. While VR/2D increased Involvement, it also introduced Distraction, balancing out overall presence scores. Designers should consider user traits (e.g., immersive tendencies) and content interactivity to optimize presence. For immersive journalism, 360-degree videos may not yet fulfill the promise of ""being there"" without deeper embodiment or interaction.
Design Implications:
• Prioritize reducing hardware distractions (e.g., lighter HMDs).
• Leverage user traits (e.g., target high-immersion-prone audiences for VR content).
• Explore interactive or photorealistic content to enhance Naturalness and Locality.",Presence – General,,,,Questionnaires
ID125,Neurophysiological Effects of Presence in Calm Virtual Environments,"Dey  Arindam,  Phoon  Jane,  Saha  Shuvodeep,  Dobbins  Chelsea,  Billinghurst  Mark",,10.1109/VRW50115.2020.00223,2020,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"The study investigates presence in VR using both subjective (questionnaires) and objective (neurophysiological) measures, making it relevant for assessing user experience in immersive environments. It uses physiological (heart rate, electrodermal activity) and neurological (EEG power spectral density) metrics to evaluate how presence levels impact user responses. Findings indicate that higher presence environments result in increased heart rate and distinct EEG activity patterns, supporting neurophysiological markers as implicit indicators of presence.

""Higher presence environments resulted in increased heart rate, suggesting greater engagement.""""EEG data revealed that high-presence conditions led to stronger frontal activity, indicating higher cognitive involvement.""""No significant differences in electrodermal activity were observed between high- and low-presence conditions.”


• Witmer & Singer Presence Questionnaire (WS) – Measures control, sensory factors, and realism.
• Slater-Usoh-Steed Presence Questionnaire (SUS) – Assesses overall presence in VR.
Heart Rate (HR) – Higher in high-presence (HP) environments, indicating increased engagement.Electrodermal Activity (EDA) – Measured but showed no significant difference between conditions.
EEG Power Spectral Density (PSD) – HP conditions led to increased frontal brain activity (higher cognitive engagement), while LP conditions required more occipital processing.","1) Participants
• Total: 24 participants
• Demographics:
    ◦ Age range: 18–40 years
    ◦ Normal or corrected-to-normal vision
    ◦ No history of neurological or cardiac conditions
    ◦ No prior VR experience required
2) Study Design
• Within-subjects design
    ◦ Each participant experienced two different presence conditions in VR:
        1. High Presence (HP): A more immersive and interactive VR environment.
        2. Low Presence (LP): A simpler, less engaging VR environment.
    ◦ Order of exposure was randomized to prevent order effects.
• Independent Variable:
    ◦ Presence level (High vs. Low)
• Dependent Variables:
    ◦ Neurophysiological (EEG), physiological (HR, EDA), and subjective presence measures.
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were fitted with an EEG cap and physiological sensors (heart rate, electrodermal activity).
    ◦ A VR calibration session ensured proper tracking and equipment setup.
2. Experimental Phase:
    ◦ Participants experienced both HP and LP VR conditions, with a short break between sessions to avoid carryover effects.
    ◦ During each session, EEG, HR, and EDA data were continuously recorded while participants engaged with the virtual environment.
3. Post-Experiment Phase:
    ◦ Participants completed two presence questionnaires (WS & SUS) for each condition.
    ◦ EEG and physiological data were analyzed to determine correlations with presence scores.
4) Task
• Passive Observation Task:
    ◦ Participants were exposed to different virtual environments under high- and low-presence conditions.
    ◦ No active task was required; they were instructed to immerse themselves naturally in the VR experience.
5) Metrics Collected
• Subjective Questionnaires:
    ◦ Witmer & Singer Presence Questionnaire (WS) – Assesses control, sensory factors, and realism.
    ◦ Slater-Usoh-Steed Presence Questionnaire (SUS) – Measures overall presence in VR.
• Physiological Metrics:
    ◦ Heart Rate (HR) – Used to assess arousal and engagement levels in different presence conditions.
    ◦ Electrodermal Activity (EDA) – Measured autonomic nervous system response to presence.
• Neurological Metrics:
    ◦ EEG Power Spectral Density (PSD) – Captures brain activity changes in frontal and occipital regions between HP and LP conditions.","Presence – General,  WS",,"EDA / GSR (Skin Conductance),  Heart Rate,  EEG",,"Questionnaires,  Physiological"
ID126,On Shooting Stars: Comparing CAVE and HMD Immersive Virtual Reality Exergaming for Adults with Mixed Ability,"Elor  Aviv,  Powell  Michael,  Mahmoodi  Evanjelin,  Hawthorne  Nico,  Teodorescu  Mircea,  Kurniawan  Sri",,10.1145/3396249,2020,ACM Transactions on Computing for Healthcare,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"Forty participants—thirteen adults with developmental disabilities and twenty-seven without—played a three-minute session of the “Project Star Catcher” upper-limb exercise game twice, once in a four-wall CAVE and once in an HTC Vive Pro. The HMD elicited larger full-body movements, higher star-catch scores and greater non-dominant-arm activity than the CAVE, and raised physiological arousal in non-disabled users while boosting EEG beta- and gamma-band power in both groups. Subjective questionnaires showed stronger immersion, engagement and effort for the HMD; 100 % of non-disabled and 62 % of disabled participants preferred it, citing ease, realism and fun, whereas the remainder valued the CAVE’s comfort. The findings suggest modern consumer HMDs can surpass research-grade CAVEs for motivating vigorous, enjoyable and physiologically engaging VR exercise across mixed-ability populations.","1) Participants
• The study involved adults with mixed abilities, though the exact number and demographic details (e.g., age, gender, disability type) would need verification from the full text.
• Participants engaged in an exergaming task using two different VR systems.
2) Study Design
• Comparative study: The experiment compared CAVE-based VR and HMD-based VR.
• Within-subject design: Participants experienced both conditions, allowing direct comparison of user experience across the two systems.
3) Procedure
• Participants first received an introduction and familiarization session with both VR systems.
• They completed the exergaming task in both the CAVE and HMD environments, likely in a counterbalanced order.
• Post-experience, they provided feedback on their experience.
4) Task
• The study used a custom exergaming experience designed for adults with mixed ability.
• The task likely involved physical movements (e.g., reaching, stepping, or interacting with virtual objects).
• The gameplay mechanics and objectives would need verification from the full text.
5) Metrics Collected
• Questionnaires: Subjective user experience ratings (e.g., presence, comfort, usability).
• Behavioral Metrics: Movement patterns and user interactions within the VR environments.
• Performance Metrics: Task completion rates or accuracy in the exergame.
• Physiological Metrics: Unclear, but potential inclusion of heart rate or motion tracking for exertion levels.
Would you like a more detailed breakdown of any specific section?",IEQ (Immersion Experience Questionnaire),Movement Trajectories,"EDA / GSR (Skin Conductance),  Heart Rate,  EEG",Task Success / Completion,"Questionnaires,  Behavioural,  Physiological,  Performance"
ID127,"Optimizing visual complexity for physiologically-adaptive VR systems: Evaluating a multimodal dataset using EDA, ECG and EEG features","Chiossi  Francesco,  Ou  Changkun,  Mayer  Sven",,10.1145/3656650.3656657,2024,Proceedings of the International Conference on Advanced Visual Interfaces,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,"Adaptive Systems, Cognitive Workload Monitoring","This paper explores how visual complexity in virtual environments affects physiological responses—specifically, electrodermal activity (EDA), electrocardiography (ECG), and electroencephalography (EEG)—to inform physiologically adaptive VR systems. Using a multimodal dataset (N=20) collected during an adaptive N-Back task, the authors analyzed physiological patterns under two adaptation logics: a Test Adaptive System (reducing visual complexity when arousal increased) and a Reverse Adaptive System (increasing complexity when arousal increased). Results showed that EDA measures (tonic and phasic components) reliably tracked system adaptations, confirming their role as indicators of cognitive workload and engagement. While EEG alpha power and the alpha/theta ratio increased with higher visual complexity (indicating mental fatigue and workload), ECG metrics (HR, HRV) were not significantly affected. The study demonstrates that multimodal physiological metrics—especially EDA and EEG—can enhance adaptive VR systems’ capacity to adjust visual complexity dynamically, improving user modeling and preventing cognitive overload.",,,,"ECG,  EDA / GSR (Skin Conductance),  EEG",,Physiological
ID128,Patterns in Motion: On Head- and Non-Head Movers in VR During Viewport Control,"Lee  Hock Siang,  Weidner  Florian,  Gellersen  Hans",,10.1109/VRW62533.2024.00125,2024,Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops,VR,Behavioural Dynamics & Exploration,Interaction Techniques & input Modalities.,Human-Computer Interaction (HCI),"The study examines how users interact with viewport control techniques in VR, categorizing them into ""head movers"" and ""non-head movers"" based on their movement behavior. It uses eye and head movement data to measure interaction tendencies and viewport control behavior, identifying patterns in gaze-based navigation.
The study identifies two distinct user behavior patterns in VR viewport control: ""head movers,"" who rely more on head rotations for navigation, and ""non-head movers,"" who primarily use eye movements. Using cumulative head and eye yaw measurements, the study shows that different viewport control techniques (Controller Snap, Dwell Snap, and Gaze Pursuit) influence user adaptation and comfort. Head movers performed better with controller-based techniques, while non-head movers preferred gaze-based navigation. These findings highlight the importance of designing adaptable viewport control methods that accommodate diverse user movement behaviors in VR.

Cumulative Head Yaw – Measures how much participants rotate their heads for viewport control.Cumulative Eye Yaw – Assesses how much participants rely on eye movement instead of head movement.Viewport Control Technique Performance – Evaluates user adaptation across Controller Snap, Dwell Snap, and Gaze Pursuit techniques.","1) Participants
• Total: 30 participants
• Demographics:
    ◦ Age range: 18–40 years
    ◦ Normal or corrected-to-normal vision
    ◦ No prior VR experience required
2) Study Design
• Within-subjects design
    ◦ Each participant completed tasks using three different viewport control techniques:
        1. Controller Snap – Viewport adjusted using a handheld controller.
        2. Dwell Snap – Viewport changed after focusing on a point for a set duration.
        3. Gaze Pursuit – Viewport smoothly followed the user's eye movements.
    ◦ Participants were categorized as head movers or non-head movers based on their movement patterns.
• Independent Variable:
    ◦ Viewport Control Technique (Controller Snap, Dwell Snap, Gaze Pursuit)
• Dependent Variables:
    ◦ Cumulative Head Yaw, Cumulative Eye Yaw, and Technique Performance.
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were given an introduction to VR and the viewport control techniques.
    ◦ A training session allowed them to familiarize themselves with the different control methods.
2. Experimental Phase:
    ◦ Participants performed navigation tasks using each viewport control technique.
    ◦ Head and eye movement data were recorded in real time to classify users as head movers or non-head movers.
3. Post-Experiment Phase:
    ◦ Data was analyzed to determine how each technique affected user behavior and performance.
4) Task
• Viewport Navigation Task:
    ◦ Participants navigated a virtual environment using each viewport control technique.
    ◦ They were instructed to orient themselves toward specific visual targets as efficiently as possible.
5) Metrics Collected
• Behavioral Metrics (Movement Patterns in VR):
    ◦ Cumulative Head Yaw – Measures the extent of head movement for viewport control.
    ◦ Cumulative Eye Yaw – Measures the extent of eye movement for viewport control.
    ◦ Viewport Control Technique Performance – Evaluates the effectiveness of each technique for navigation.",,"Gaze Analysis,  Head Analysis",,"Accuracy,  Completion Time","Behavioural,  Performance"
ID129,Performance and Navigation Behavior of Using Teleportation in VR First-Person Shooter Games,"Prithul  Aniruddha,  Lynam  Hudson,  Folmer  Eelke",,10.1145/3661133,2024,Conference on Games,VR,Interaction Techniques & input Modalities,Behavioural Dynamics & Exploration.,Entertainment and Gaming,"This study compares teleportation vs. continuous locomotion during a VR first-person shooter (FPS) task. Using a within-subject design, 21 participants played two 5-minute sessions: one with teleportation and one with continuous locomotion. AI agents replaced human opponents to ensure controlled difficulty.
The authors measured 12 behavioural and performance metrics, including movement patterns, strafing, head–locomotion angle, stationary behaviour during combat, hits taken, and teleport distance.
Main results:
• Teleportation users traveled significantly farther virtually and physically (46% more virtual movement, 18% more physical movement).
• Teleportation users stayed still almost 2× longer during combat → dramatically increased vulnerability.
• Teleportation users were hit nearly twice as much.
• Continuous locomotion supported strafing, while teleportation did not (head-locomotion angle: 23° vs. 70°).
• Users preferred the predictability of continuous locomotion, especially when evaluating AI agents.
Contribution: High-resolution behavioural mapping of locomotion differences that generalize beyond FPS games. It improves understanding of navigation behaviour in VR, making this a strong behavioural dynamics paper relevant to your scoping review.","1) Participants
• N = 21
• Age: M = 25.14, SD = 3.8
• Gender: 18 male, 2 female, 1 non-binary
• Wide range of VR & gaming experience
• Paid $10 compensation
• IRB approved
2) Study Design
• Within-subjects, Latin-square counterbalanced
• Two conditions:
    1. Teleportation
    2. Continuous locomotion
• 5 minutes of gameplay per condition
• Tutorial allowed practice with both techniques before testing
• Same VE and same AI agent behaviour in both conditions
3) Task
• Player must shoot as many AI agents as possible while avoiding being hit
• Agents strafed, navigated, and fired at players
• Visual cues for damage (red vignette)
• Player cannot die (for fairness/consistency)
4) Apparatus
• Meta Quest 2, 120 Hz experimental mode
• Unity + XR Toolkit
• Teleport with projectile arc
• Continuous locomotion with thumbstick
• FOV restrictor (to reduce sickness)
• Tracking space: 2.5 × 2.5 m
• Full capture of head, controller, joystick direction, virtual position every 0.1 s
5) Metrics Collected
Performance Metrics
• Shooting accuracy
• Hits taken
• Number of shots fired
Behavioural Metrics
(Extensive, fine-grained behavioural analysis)
• Total virtual movement
• Total physical movement
• Movement during combat
• Stationary time (combat and total)
• Head–locomotion angle
• Joystick direction distribution
• Standard distance of physical & virtual movement
• Mean teleport distance
• Head rotation total
Questionnaires – Custom (Usability & Perception)
• Efficiency
• Learnability
• Accuracy
• Likability
• Perception of AI using each locomotion method (difficulty, predictability, likability)
",Usability – Custom / Rating-based,"Interaction Time,  Movement Trajectories,  Gaze Analysis,  Navigation Behaviour",,"Accuracy,  Task Success / Completion","Questionnaires,  Behavioural,  Performance"
ID130,Photorealistic True-Dimensional Visualization of Remote Panoramic Views for VR Headsets,"Livatino  Salvatore,  Regalbuto  Alessio,  Morana  Giuseppe,  Signorello  Giovanni,  Gallo  Giovanni,  Torrisi  Alessandro,  Padula  Gianluca,  Pelc  Katarzyna,  Malizia  Alessio,  Farinella  Giovanni Maria",,10.1109/ACCESS.2023.3285709,2023,IEEE Access,VR,Visualization Techniques,User states: Cognitive & Affective Experience.,"Human-Computer Interaction (HCI), Tourism and Cultural Heritage","The study examines the effect of photographic realism in VR headsets when observing real-world places through omnidirectional, high-resolution panoramic images. Findings indicate that higher display pixel-density enhances perceived image lighting and contributes to a stronger sense of presence. The viewed environment significantly affects spatial presence, depth perception, and emotional responses, with well-lit and colorful environments (e.g., an island setting) eliciting higher realism and enjoyment, while darker and enclosed environments (e.g., a cave) provoke higher anxiety and spatial awareness. Additionally, place familiarity increases perceived realism and presence, showing that prior knowledge of a location enhances the immersive experience. Overall, the study highlights the importance of display resolution, environmental conditions, and user familiarity in optimizing realistic VR experiences.","1. Participants
• User Study 1 (Display & Environment Effects):
    ◦ 20 participants, ages 22–53 (avg. 28).
    ◦ Within-subjects design (all participants experienced both displays and environments).
• User Study 2 (Familiarity Effects):
    ◦ 40 participants, ages 25–58 (avg. 32).
    ◦ Between-subjects design (split into ""site-familiar"" vs. ""unfamiliar"" groups).
2. Study Design
• Independent Variables:
    ◦ Display: LG (high pixel-density) vs. iPhone (lower pixel-density, better lighting specs).
    ◦ Environment:
        ▪ Island (rich lighting/colors, far objects).
        ▪ Cave (low lighting/colors, close objects).
    ◦ Familiarity: Prior knowledge of the environment (site-familiar vs. unfamiliar users).
• Dependent Variables:
    ◦ Perceived realism, presence, emotions, depth impression, distance estimation accuracy.
3. Procedure
1. Introduction & Training:
    ◦ Participants completed consent forms, vision tests (Snellen chart), and practice trials.
2. Observation & Ratings:
    ◦ Wore VR headset (Shinecon goggles) and explored static 3D panoramic scenes.
    ◦ Rated image lighting (pixel-density, color, contrast, etc.) and human factors (realism, presence, emotions).
3. Distance Estimation Task:
    ◦ Viewed pre-selected scene elements (e.g., rocks, trees) and estimated distances (egocentric/relative).
4. Questionnaires:
    ◦ Post-observation Likert-scale ratings (1–7) for presence (IPQ), realism, and emotions.
4. Task
• Primary Task:
    ◦ Observe and explore photorealistic 360° environments via head rotation (no locomotion).
    ◦ Verbally report distance estimates (e.g., ""How far is the rock?"") in meters.
• Secondary Tasks:
    ◦ Rate perceived image quality (sharpness, vividness).
    ◦ Self-report emotions (happiness, anxiety, etc.) and presence (""sense of being there"").
5. Metrics Collected
1. Questionnaires (Self-Reported Metrics)
• Presence:
    ◦ IPQ (Igroup Presence Questionnaire) – Measured overall presence, spatial presence, and involvement (e.g., ""How much did you feel present in the virtual environment?"").
    ◦ Subscales:
        ▪ Being there (P1)
        ▪ Surrounding reality (SP1)
        ▪ Feeling present in virtual space (SP3)
        ▪ Attention to VR world (INV4)
• Visual Realism:
    ◦ 4 Realness subscales (e.g., ""How similar was the experience to the real world?"").
    ◦ Photorealism rating (""How photorealistic was the environment?"").
    ◦ Similarity to photo/video (""Did it feel like watching a photo or video?"").
• Emotions:
    ◦ Rated happiness, enjoyment, relaxation, scariness, sadness, anxiety, anger, surprise.
    ◦ ""Back to reality"" sensation post-experience.
2. Behavioral Metrics (Observed Actions)
• Distance Estimation:
    ◦ Egocentric distance (6 pre-selected objects, e.g., ""Estimate how far the rock is from you."").
    ◦ Relative distance (5 object pairs, e.g., ""How far is Object A from Object B?"").
    ◦ Accuracy measured vs. ground truth (error %).
3. Performance Metrics (Quantitative Outcomes)
• Depth Perception Accuracy:
    ◦ Calculated from distance estimation errors.
    ◦ Compared between environments (Island vs. Cave).
• Speed of 3D Impression:
    ◦ ""How quickly did you perceive depth in the scene?"" (Likert scale).
4. Display & Image Quality Metrics (Subjective Ratings)
• Perceived image lighting:
    ◦ Pixel-density, lightness, color, contrast, vividness, sharpness, definition.
    ◦ Rated on relevance to realistic viewing (Likert scale).","IPQ (Igroup Presence Questionnaire),  Likert-scale ratings (realism emotions)",,,"Accuracy,  Distance Estimation Accuracy","Questionnaires,  Performance"
ID131,Psychophysiological Approach for Measuring Social Presence in A Team-Based Activity: A Comparison Between Real and Virtual Environments,"Wang  Xunan,  Li  Xi,  Chen  Bixun,  Ghannam  Rami",,10.1109/ICECS202256217.2022.9970857,2022,International Conference on Electronics Circuits and Systems (ICECS),VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Telecommunications and Collaboration,"Nine university students first rested, then cooperatively separated virtual wooden blocks in a Unity-based headset game, and finally performed the identical task around a table while ECG was recorded. HRV features SDNN, RMSSD and pNN50 fell progressively from rest to VR and dropped further in the real-world phase, signalling greater physiological stress during physical collaboration than in immersive VR. Questionnaire ratings showed high satisfaction (4.44/5) and engagement (4.67/5) in VR, and at least 77 % of participants said they would choose VR again for teamwork. The authors conclude that headset-based collaboration can foster stronger participation feelings while reducing stress compared with face-to-face teamwork.","1. Participants
• Number of Participants: 10 students from the 
University of Glasgow were selected through convenient sampling. However, data from one participant were discarded due to a headset issue, resulting in data analysis from 9 participants.
2. Study Design
• The study employed a comparative design to assess feelings of social presence in both virtual reality (VR) and real-world environments. Participants engaged in a cooperative game in both settings to measure physiological responses and subjective experiences.
3. Procedure
• The experiment consisted of three phases:
    1. Resting State: Participants were kept in a resting state for 2 minutes without movement or talking.
    2. VR Game: Participants played the VR game ""Separate Wooden Blocks"" for approximately 3 minutes.
    3. Physical World: Participants then played the same 
game in a real-world setting for about 2 minutes, with a 3-minute rest 
period between the VR and real-world tasks https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bd9ddd4a32aba7efffaf14/?range=8667,9552, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67bd9ddd4a32aba7efffaf14/?range=7883,8666.
4. Task
• The task involved a collaborative game where participants needed to separate disordered wooden blocks. They had to communicate and work together to achieve the objective, both in the VR environment and in reality.
5. Metrics Collected
• Physiological Metrics: Cardiac electrical signals were measured using ECG sensors, focusing on heart rate variability (HRV) features such as:
    ◦ SDNN: Standard deviation of RR intervals.
    ◦ RMSSD: Root mean square of successive differences of RR intervals.
    ◦ pNN50: Percentage of interval differences of successive RR intervals greater than 50 ms.
• Questionnaire Metrics: Participants completed an
 online questionnaire assessing their feelings of satisfaction, 
engagement, and excitement during the tasks, rated on a scale of 1 to 5. This summary encapsulates the key aspects of the experiments conducted in the study, providing a clear overview of the participants, design, procedure, task, and metrics collected.",Custom made,,ECG,,"Questionnaires,  Physiological"
ID132,"Quality of Experience Comparison of Stereoscopic 3D Videos in Different Projection Devices: Flat Screen, Panoramic Screen and Virtual Reality Headset","Choy  Siu-Ming,  Cheng  Eva,  Wilkinson  Richardt H.,  Burnett  Ian,  Austin  Michael W.",,10.1109/ACCESS.2021.3049798,2021,IEEE Access,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"Fifteen participants watched five one-minute stereoscopic sequences on each of three devices: a 25-inch flat 3-D screen, a six-projector 360° panoramic room and an HTC Vive headset. Simulator-Sickness scores and EEG theta-plus-alpha over beta ratios showed the VR headset produced the highest visual-fatigue and VIMS, whereas the panoramic screen yielded the lowest sickness and the highest Absolute-Category enjoyment ratings. Eye-blink frequency was greatest on the flat screen and least with the headset, aligning with the physiological-fatigue pattern, while attention and meditation levels remained neutral across conditions. ANOVA confirmed device type and video content significantly affected sickness, brain-wave fatigue and enjoyment, establishing projection screen as a dominant determinant of QoE in stereoscopic-video viewing.","
1. Participants:
    ◦ Number: 15 participants (11 males and 4 females).
    ◦ Age Range: 18 to 46 years old (mean age: 29.1 years).
    ◦ Pre-Screening: Participants completed a short Stereoscopic 3D (S3D) vision test based on the ITU-R BT.2021 standard to ensure they were suitable for the experiment.
2. Study Design:
    ◦ Objective: To compare the Quality of Experience (QoE) and Visually Induced Motion Sickness (VIMS) across three different projection devices:
        1. Flat 3D Screen (Panasonic BT-3DL2550).
        2. Panoramic Screen (UTS Data Arena, a hemispherical room with 360° projection).
        3. VR Headset (HTC Vive).
    ◦ Design: Within-subjects design, where each participant experienced all three projection devices.
    ◦ Video Content: Participants watched five 1-minute S3D video sequences extracted from Big Buck Bunny and RMIT3DV databases. The videos varied in 3D effects and content (e.g., animation, outdoor scenes with water, flames, etc.).
3. Procedure:
    ◦ Pre-Experiment: Participants were briefed on the experiment and how to use the devices (especially the VR headset).
    ◦ Experiment Sessions: Each participant completed three sessions, one for each projection device. Between sessions, there was a 10-minute break to minimize motion sickness and fatigue.
    ◦ Video Presentation: In each session, participants watched five 1-minute S3D video sequences in a random order to prevent bias.
    ◦ Post-Video Tasks: After each video, participants completed the Simulator Sickness Questionnaire (SSQ) to rate their experience of motion sickness and provided an enjoyment rating using the Absolute Category Rating (ACR) method.
    ◦ EEG and Eye Tracking: During the video viewing, participants wore a NeuroSky Mindwave EEG headset to record EEG signals (brain activity) and eye blink rates. The EEG data was used to calculate brain wave power ratios (e.g., (θ+α)/β(θ+α)/β) to assess visual fatigue.
4. Task:
    ◦ Primary Task: Participants were asked to watch the S3D video sequences on each of the three projection devices.
    ◦ Secondary Task: After each video, participants completed the SSQ to report any symptoms of motion sickness and rated their enjoyment of the video using the ACR method.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Simulator Sickness Questionnaire (SSQ): Used to measure Visually Induced Motion Sickness (VIMS). The SSQ includes 16 symptoms rated on a 4-point scale (none, slight, moderate, severe) and calculates scores for Nausea (N), Oculomotor (O), Disorientation (D), and Total Severity (TS).
        ▪ Enjoyment Rating: Participants rated their enjoyment of each video on a 5-point scale (1: not enjoyable at all, 5: very enjoyable) using the Absolute Category Rating (ACR) method.
    ◦ Physiological Metrics:
        ▪ EEG (Electroencephalography): Recorded brain activity to calculate brain wave power ratios (e.g., (θ+α)/β(θ+α)/β) as an indicator of visual fatigue.
        ▪ Eye Blink Rate: Measured using the EEG headset to assess eye fatigue.
    ◦ Behavioral Metrics:
        ▪ Attention and Meditation Levels: Measured using the NeuroSky Mindwave EEG headset to assess participants' mental states during video viewing.
    ◦ Performance Metrics:
        ▪ Enjoyment Rating: Used as a performance metric to evaluate user satisfaction with the video content and projection devices.
Summary:
The study involved participants watching S3D videos on three different projection devices (flat screen, panoramic screen, and VR headset) while their physiological (EEG, eye blink rate), behavioral (attention, meditation levels), and subjective (SSQ, enjoyment rating) responses were recorded. The goal was to compare the impact of different projection devices on Quality of Experience (QoE) and Visually Induced Motion Sickness (VIMS). The findings indicated that VR headsets caused higher visual fatigue and VIMS, while panoramic screens provided the highest enjoyment rating.","Absolute Category Rating,  SSQ (Simulator Sickness Questionnaire)",,"Eye Blink Analysis,  EEG",,"Questionnaires,  Physiological"
ID133,Quantifying User Behaviour in Multisensory Immersive Experiences,"Gougeh  Reza Amini,  De Jesus  Belmir J.,  Lopes  Marilia K. S.,  Moinnereau  Marc-Antoine,  Schubert  Walter,  Falk  Tiago H.",,10.1109/MetroXRAINE54828.2022.9967498,2022,IEEE International Conference on Metrology for Extended Reality Artificial Intelligence and Neural Engineering,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"Eleven adults played a two-minute orange-shooting game on an instrumented Oculus Quest first with standard audio-visual feedback and then with the same visuals accompanied by ambient forest scents, citrus puffs on each hit and arm-sleeve vibrations. Multisensory play significantly raised presence, engagement and overall experience ratings and increased game scores by roughly 25 %. Physiologically, heart-rate, blink-rate, EEG engagement index and theta-band workload all climbed during multisensory trials, while frontal-alpha asymmetry shifted from negative to positive values, signalling a move from withdrawal to approach motivation. Head-tracking data showed many more horizontal and vertical movements when scent and haptics were active, confirming greater exploratory behaviour. The authors argue that integrating real-time biosignal sensing into headsets could let future XR titles adapt content dynamically to sustain high quality-of-experience.","1) Participants
• Number of Participants: 11 (2 female)
• Mean Age: 25.4 years (± 2.3 years)
• Ethics Compliance: The study followed COVID-19 safety measures approved by the institution’s Ethics Committee .
2) Study Design
• Type of Study: A pilot experiment using a custom-developed VR game.
• Conditions: Two main conditions were tested:
    ◦ Audio-Visual (AV)
    ◦ Audio-Visual-Smell-Haptics (AVSH)
• Counterbalancing: Condition ordering was counterbalanced to avoid biases  .
3) Procedure
• Participants played a VR game where they moved around an orange grove and shot at oranges within a two-minute time frame.
• Feedback was provided through various modalities:
    ◦ Visual: Shooting oranges produced a mist and sound.
    ◦ Olfactory: Ambient scents and bursts of citrus smell were diffused.
    ◦ Haptic: Vibrations were provided via haptic sleeves when oranges were shot.
• After each condition, participants rated their experience on a 
5-point Likert-type scale for immersion, presence, realism, engagement, 
and overall experience  .
4) Task
• Main Task: Shoot oranges in a VR environment.
• Game Mechanics: Oranges disappeared for 3.5 seconds after being shot and then reappeared  .
5) Metrics Collected
• Subjective Ratings: Immersion, presence, realism, engagement, and overall experience.
• Physiological Measures:
    ◦ Heart Rate
    ◦ Blink Rate
    ◦ Frontal Alpha Asymmetry (FAA)
    ◦ Engagement Index (EI)
    ◦ Mental Workload (MW)
• Performance Metrics: Number of successful interactions (oranges shot) recorded as the user's score   .",Likert Scale (5-point) for QoE,,"Eye Blink Analysis,  EEG,  Heart Rate",Task Success / Completion,"Questionnaires,  Physiological,  Performance"
ID134,Searching Across Realities: Investigating ERPs and Eye-Tracking Correlates of Visual Search in Mixed Reality,"Chiossi  Francesco,  Trautmannsheimer  Ines,  Ou  Changkun,  Gruenefeld  Uwe,  Mayer  Sven",,10.1109/TVCG.2024.3456172,2024,Transactions on Visualization and Computer Graphics,MR,User states: Cognitive & Affective Experience,,,"This paper investigates how users perform visual search tasks in Augmented Reality (AR) and Augmented Virtuality (AV), examining differences when targets are physical or virtual.
It uses a rich multimodal measurement approach combining:
• Behavioral performance (accuracy, reaction time)
• Eye tracking (fixations, saccades, pupillary activity)
• EEG / ERP (distractor positivity PD component)
• Subjective workload (NASA-TLX)
The study uses a 2×2 within-subjects design (AR/AV × Physical/Virtual Targets) with 20 participants completing 400 trials per condition.
Main findings:
• AV reduces cognitive load (lower IPA, lower NASA-TLX).
• AV improves search efficiency (faster reaction time, fewer fixations, higher saccade frequency).
• Virtual targets are faster to detect and produce fewer misses.
• ERP PD amplitude is lower in AV, indicating more effective distractor suppression.
• Physiological indexes reflect underlying cognitive processes useful for adaptive MR systems.
These results provide generalizable insights into how perception, attention, and cognitive load operate across MR conditions, and demonstrate validated, multimodal metrics highly relevant for XR UX research.","Participants
• N = 20 (11 female, 9 male; M = 24.85)
• Prior familiarity with AR/AV/VR assessed
• Excluded: neurological or psychological disorders, color blindness
Design
• 2×2 within-subjects:
    ◦ Actuality: AR vs. AV
    ◦ Target type: Physical vs. Virtual
Procedure
• Consent + EEG prep
• Eye-tracking calibration
• Training phase (20 trials)
• Main task: 4 blocks × 100 trials = 400 trials total
• Each block followed by raw NASA-TLX
• Participants seated 3m from shelf
Task
Visual search task:
• Identify one target among 24 distractors
• Target type randomized
• Reaction time limit: 5000 ms
• Selection via ray-cast controller
Stimuli
• 4 shapes (sphere, cylinder, cube, pyramid)
• In 4 colors
• Both physical and virtual versions
• 25 items displayed per trial (1 target, 24 distractors)
Apparatus
• HTC Vive Pro Eye (with integrated eye tracking)
• EEG (32-channel water-based R-Net cap)
• Unity + MR prototyping toolkits
• LSL for data synchronization
Metrics collected
Performance Metrics
• Accuracy
• Missed trials
• Reaction time
Eye-Tracking Metrics
• Fixation duration
• Fixation count
• Last fixation duration
• Saccade frequency
• Index of Pupillary Activity (IPA) (cognitive load)
EEG/ERP Metrics
• PD (distractor positivity) amplitude (300–900 ms window)
• Reflects distractor suppression and attentional efficiency
Questionnaires – Standard
• NASA-TLX (workload)
",NASA-TLX,Reaction Time,"Eye-Tracking,  Pupil Analysis,  EEG","Error Rate,  Accuracy","Questionnaires,  Behavioural,  Physiological,  Performance"
ID135,Seeing is Believing: The Effect of Video Quality on Quality of Experience in Virtual Reality,"Zheleva  Aleksandra,  Durnez  Wouter,  Bombeke  Klaas,  Van Wallendael  Glenn,  De Marez  Lieven",,10.1109/QoMEX48832.2020.9123075,2020,International Conference on Quality of Multimedia Experience,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"Thirty-two people watched the same five-minute VR movie four times, each time at a lower picture resolution. Viewers’ ratings of picture quality, sensory immersion and motion sickness stayed roughly the same until the very lowest resolution, where all three got noticeably worse. Measures of presence, story immersion and fatigue did not change with resolution. Brain-wave recordings showed that alpha power at the back of the head fell steadily as the image got blurrier, suggesting the brain worked harder to process the poorer picture. The study shows that VR streaming can drop resolution without hurting the experience up to a point, but going too low harms both how the film feels and how the brain reacts.","1) Participants
• Number: 32 participants (24 female, 8 male).
• Age: Mean = 24.24 years (SD = 3.93).
• Experience: 47.05% had prior VR experience.
• Exclusion Criteria:
    ◦ Left-handed individuals.
    ◦ History of head trauma/thick curly hair (due to EEG requirements).
2) Study Design
• Type: Within-subjects experiment (all participants experienced all conditions).
• Independent Variable (IV):
    ◦ Video Quality (4 levels):
        1. Q1 (High): 2469 × 2743 pixels.
        2. Q2 (Medium 1): 1808 × 2009 pixels.
        3. Q3 (Medium 2): 1169 × 1298 pixels.
        4. Q4 (Low): 512 × 549 pixels.
• Counterbalancing: Order randomized via balanced Latin squares.
• VR Setup: HTC Vive Pro Eye HMD (6DoF, standing).
3) Procedure
1. Pre-Test:
    ◦ Online screening questionnaire (demographics, VR experience).
2. Lab Session (1.5 hours):
    ◦ EEG cap fitting (64 electrodes, impedance <20Ω).
    ◦ HMD placement over EEG cap.
3. VR Exposure:
    ◦ Watched INVASION! (5-minute animated 6DoF movie) in all 4 quality versions.
    ◦ Free exploration (walking/looking around).
4. Post-Video:
    ◦ Removed HMD → Completed 5-minute questionnaire.
    ◦ Repeated for each quality condition.
4) Task
• Primary Activity: Passive viewing of INVASION! (narrative-driven VR movie).
    ◦ Plot: Earth viewed from space → Alien encounter → Bunny ""saves"" viewer.
• Interaction: No explicit tasks; natural exploration (standing, looking around).
5) Metrics Collected
Subjective (Questionnaires)
• Video Quality: Absolute Category Rating (ACR) scale (1–5).
• Immersion:
    ◦ Sensory Immersion: Isolation from physical environment.
    ◦ Narrative Immersion: Engagement with story.
• Presence: Sensation of ""being there"" (Usoh et al. scale).
• Simulator Sickness: Symptoms post-exposure.
• Fatigue: Karolinska Sleepiness Scale (KSS).
Physiological (EEG)
• Alpha Band Power (8–13 Hz):
    ◦ Parietal/occipital regions (linked to cognitive engagement/fatigue).
    ◦ Processed via ICA (artifact removal), Welch’s method for spectral analysis.
Key Results
• Subjective:
    ◦ Q4 (Low quality) rated worse than Q1–Q3 for ACR (p < .001).
    ◦ Lower sensory immersion (p < .01) and higher simulator sickness (p < .05) for Q4.
    ◦ No differences in narrative immersion, presence, or fatigue.
• EEG:
    ◦ Lower alpha power for Q4 vs. Q1–Q3 (p < .001), suggesting increased cognitive load (contrary to 2D video findings).
Key Takeaways
• Video Quality Matters: Degradation harms sensory immersion and increases sickness.
• EEG as Objective Metric: Alpha power reflects cognitive strain from low-quality VR.
• Design Implications: Prioritize high-quality rendering for immersive VR experiences.
Limitations: No behavioral metrics (e.g., gaze tracking) or task performance data.","Absolute Category Rating,  Karolinska Sleepiness Scale (KSS),  Presence – General,  IEQ (Immersion Experience Questionnaire)",,EEG,,"Questionnaires,  Physiological"
ID136,Spatial Audio in 360° Videos: Does It Influence Visual Attention?,"Hirway  Amit,  Qiao  Yuansong,  Murray  Niall",,10.1145/3524273.3528179,2024,Multimedia Systems Conference (MMSys),VR,Behavioural Dynamics & Exploration,Content & System Design.,Human-Computer Interaction (HCI),"Twenty adults viewed ten one-minute indoor and outdoor 360° clips twice—once with normal stereo sound and once with spatial Ambisonics—inside an HTC Vive with eye-tracking. Spatial sound made people turn their heads through a wider area outdoors and enlarged peak pupil size in both scene types, suggesting higher arousal or effort. Eye-gaze fixations stayed roughly in the same regions under either soundtrack, but head-pose fixations flipped: more in stereo for indoor clips, more in spatial audio for outdoor ones. Post-viewing ratings gave spatial sound higher marks for clarity and realism, although presence, immersion and fatigue did not differ. Overall, Ambisonics encourages broader visual exploration and stronger physiological engagement without affecting narrative presence.","1. Participants
• Number of Participants: 20
• Demographics: Average age of 27 years, consisting of 11 men and 9 women. Eight participants had prior experience with VR https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=0,0 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=43992,44558.
2. Study Design
• The study employed an experimental design with a within-subjects 
approach, where each participant experienced different audio conditions 
(stereo vs. spatial audio) while watching 360° videos https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=24253,25153, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=38924,39587.
3. Procedure
• The assessment was divided into five phases:
    1. Information Phase: Participants were informed about the study and signed consent forms (10 minutes).
    2. Screening Phase: Visual and auditory acuity were assessed using tests like the Snellen test and Ishihara test (10 minutes).
    3. Training Phase: Participants familiarized 
themselves with the VR environment by watching a 60-second training 
video with non-spatial audio (5 minutes).
    4. Testing Phase: Participants watched two 5-minute 
360° video segments (one indoor and one outdoor) with either stereo or 
spatial audio, with the order randomized to mitigate bias (15 minutes).
    5. Questionnaire: Participants completed a subjective questionnaire assessing their experience (5-10 minutes) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=21909,22741, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=28466,29202.
4. Task
• Participants were tasked with watching 360° videos while seated in a
 rotating chair, allowing them to explore the full 360° field of view. 
They were instructed to engage in free viewing without specific tasks https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=10472,11474, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=24253,25153.
5. Metrics Collected
• Behavioral Metrics: Head pose and eye gaze data were recorded to analyze visual attention.
• Physiological Metrics: Pupil diameter was measured as an indicator of cognitive effort and engagement.
• Performance Metrics: The number of fixations and their distribution across different audio conditions were analyzed.
• Questionnaires: Participants rated their perceptions of presence, immersion, and sound spatiality using a five-point Likert scale https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=24253,25153, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b46b40372f2f774374c93d/?range=37924,38923.
This structured approach allowed the researchers to evaluate the 
influence of spatial audio on visual attention in immersive 360° video 
experiences.",Absolute Category Rating,"Head Analysis,  Gaze Analysis",Pupil Analysis,,"Questionnaires,  Behavioural,  Physiological"
ID137,"Spatial Presence, Performance, and Behavior between Real, Remote, and Virtual Immersive Environments","Khenak  Nawel,  Vezien  Jeanne,  Bourdot  Patrick",,10.1109/TVCG.2020.3023574,2020,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,Healthcare and Medical Training,"This paper investigated spatial presence, performance, and behavioural responses in three types of immersive environments: real, remote, and virtual. Participants performed a navigation task where they had to navigate a wheelchair while avoiding obstacles in each environment. The study evaluated spatial presence using subjective questionnaires and performance metrics such as task completion time, memory recall, and collision frequency. Behavioural metrics were also captured from the participants’ wheelchair trajectories, providing insight into how the environments affected their movement behaviour, such as avoidance distance and path curvature.","1) Participants
• Number of Participants: 27 (18 males, 9 females)
• Age Range: 21 to 40 years (mean = 27.7, SD = 5.7)
• Criteria: Normal or corrected-to-normal vision and non-impaired hearing. All participants volunteered without financial compensation.
• Handedness: 23 right-handed, 3 left-handed, 1 ambidextrous.
• Background: Majority were students from the university; others were staff from various scientific fields.
• VR Experience: Varied from no experience to expert levels (4 no experience, 4 beginners, 15 intermediate, 4 experts) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=33192,34171, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=34172,35055.
2) Study Design
• Type: Within-subjects design.
• Independent Variables:
    ◦ ENV: Type of environment (REAL, REMOTE, VIRTUAL).
    ◦ PATH: Type of path (three different paths with the same geometrical characteristics) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=30471,31464, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=31465,32231.
3) Procedure
• Participants signed an informed consent form and filled out a background information document.
• They were then assigned to one of the three conditions (REAL, 
REMOTE, VIRTUAL) and underwent a training phase to familiarize 
themselves with the environment.
• The experiment consisted of three phases:
    1. Training: Acclimatization to the environment without obstacles.
    2. Task: Navigate a course as quickly as possible while avoiding obstacles.
    3. Post-assessment: Completion of the Spatial Presence
 in Immersive Environments (SPIE) questionnaire and a memory test 
regarding the location of signs https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=35056,36000, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=71343,71923.
4) Task
• Participants navigated using a wheelchair (real in REAL and REMOTE 
conditions, virtual in VIRTUAL condition) while avoiding four obstacles 
(two pyramids of cans and two chairs).
• The path was indicated by seven numbered signs that participants had to follow in sequence https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=24036,24973 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=35056,36000.
5) Metrics Collected
• Subjective Measures: Responses to the SPIE questionnaire, which assessed:
    ◦ Perceived spatial presence
    ◦ Affordance of the environment
    ◦ Enjoyment
    ◦ Realness attributed to the environment
    ◦ Attention allocated to the task
    ◦ Perceived cybersickness
• Objective Measures:
    ◦ MEMORY: Number of correct answers in the memory test.
    ◦ TIME: Task completion time.
    ◦ COLLISION: Number of obstacles collided.
    ◦ CLEAR_DIST: Minimum distance between the path and obstacles.
    ◦ CURVE_ABS: Curvilinear abscissa around obstacles https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=37596,37925, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811ee7bf33d5a39716cf8a9/?range=37926,38498.
This summary encapsulates the key aspects of the experiments 
conducted, providing a clear overview of the participants, study design,
 procedure, task, and metrics collected.",SPIE (Spatial Presence),"Interpersonal Distance,  Movement Trajectories",,"Collisions / Safety Errors,  Completion Time,  Memory Performance","Questionnaires,  Behavioural,  Performance"
ID138,"Stimulus Sampling With 360-Videos: Examining Head Movements, Arousal, Presence, Simulator Sickness, and Preference on a Large Sample of Participants and Videos","Jun  Hanseul,  Miller  Mark Roman,  Herrera  Fernanda,  Reeves  Byron,  Bailenson  Jeremy N.",,10.1109/TAFFC.2020.3004617,2022,Transactions on Affective Computing,VR,Behavioural Dynamics & Exploration,User states: Cognitive & Affective Experience.,Entertainment and Gaming,"This paper presented a large-scale study with 511 participants and 80 different 360-degree videos, addressing the lack of stimulus sampling in previous VR research. The paper introduced the exploration range metric to quantify head movement and scene exploration behaviours across participants and content. It analysed relationships between head movement, arousal, presence, simulator sickness, and user preference, showing nuanced interactions depending on content and user experience. The study provided key insights into individual differences, learning and fatigue effects over time, and implications for VR content design.","1) Participants
• Total: 511 participants (247 female, 262 male, 2 other).
• Recruitment:
    ◦ 378 from a museum in the San Francisco Bay Area.
    ◦ 134 undergraduate students (course credit or paid).
• Demographics:
    ◦ Age groups: 19 & under (98), 19–25 (169), 26–45 (148), 45+ (96).
    ◦ Prior VR experience: 190 had none, 321 had some.
2) Study Design
• Type: Large-scale, within-subjects experiment with stimulus sampling.
• Independent Variables:
    ◦ Video Category (People, Animal, Nature Scene, Underwater, Other).
    ◦ Participant Factors (age, biological sex, prior VR experience).
• Dependent Variables:
    ◦ Head movements (exploration range).
    ◦ Self-reported measures (arousal, presence, simulator sickness, preference).
• Stimuli: 80 different 20-second 360-videos (selected for varied emotional content).
• Randomization: Each participant viewed 5 videos (randomly assigned subset).
3) Procedure
1. Pre-Screening: Participants completed eligibility checks (e.g., no epilepsy/seizure disorders).
2. VR Setup:
    ◦ Used HTC Vive (90Hz refresh rate, 110° FOV).
    ◦ Hand controllers for questionnaire responses.
3. Demographics: Participants answered age, sex, race, and VR experience questions in VR.
4. Video Exposure:
    ◦ Watched five 20-second 360-videos (randomly selected from 80).
    ◦ After each video, answered a post-experience questionnaire in VR.
5. Duration: ~15 minutes total.
4) Task
• Primary Task: Passive viewing of 360-videos (no interactive tasks).
• Secondary Task:
    ◦ Natural head movement exploration (no instructed focus points).
    ◦ Self-reporting responses after each video.
5) Metrics CollectedCategorySpecific MetricsMeasurement MethodBehavioralHead movement (yaw, pitch)6-DOF tracking (90Hz)Exploration range (0–1, horizontal rotation)Calculated from yaw dataQuestionnairesArousal (1–9 SAM scale)Self-Assessment ManikinPresence (3-item Likert scale, α=0.89)""I felt present in the virtual environment""Simulator Sickness (dizziness, nausea)5-point Likert scalePreference (willingness to recommend/continue watching)5-point Likert scaleDemographicAge, sex, prior VR experiencePre-experiment survey
Key Notes on Methodology
• Large Stimulus Sampling: 80 videos to avoid single-stimulus bias.
• Mixed-Effects Models: Used to analyze order effects (learning/fatigue).
• Correlational Analysis: Examined relationships (e.g., presence-arousal, exploration-preference).
• Exploratory Focus: No hypothesis testing, but descriptive patterns (e.g., sex differences in sickness).
This study provides a comprehensive dataset on 360-video UX, combining behavioral tracking (head movements) with self-reported measures (presence, arousal, sickness) across a diverse participant pool. The findings are relevant for XR design (e.g., focal points, sickness mitigation) and research methods (stimulus sampling in VR studies).","Presence – General,  SAM (Self-Assessment Manikin),  SSQ (Simulator Sickness Questionnaire)","Head Analysis,  Movement Trajectories",,,"Questionnaires,  Behavioural"
ID139,StuckInSpace: Exploring the Difference Between Two Different Mediums of Play in a Multi-Modal Virtual Reality Game,"Malinov  Yoan-Daniel,  Millard  David E.,  Blount  Tom",,10.1109/VR50410.2021.00074,2021,Conference on Virtual Reality and 3D User Interfaces,VR,User states: Cognitive & Affective Experience,,Human-Computer Interaction (HCI),"The study introduced “Stuck in Space”, an asymmetric co-located VR game in which the headset player is an astronaut and the second player helps either through a desktop keyboard-and-mouse view or a tracked smartphone “window”. Quantitative data from 24 paired sessions showed no significant difference in co-presence or immersion between phone and PC helpers, while headset wearers were, as expected, more immersed than non-headset players. Interviews revealed that the phone modality increased physical awareness and conversational engagement but also forced the headset user to juggle a mental model of the real room, which slightly undercut their immersion; the PC modality avoided that overhead but felt less socially present. Overall, letting a friend join via a tracked phone was not detrimental to immersion or co-presence and is a viable, consumer-friendly way to share single-HMD games.","1) Participants
• Total: 24 participants (5 females, 19 males).
• Age: Average age of 21.46 years (SD = 1.52).
• Experience: Most had minimal prior VR experience, though some were familiar with games like Keep Talking and Nobody Explodes.
• Groups: Participants were divided into pairs, with only 2 pairs consisting of strangers; the rest knew each other beforehand.
2) Study Design
• Mixed Design: Two groups played two versions of the game:
    ◦ HMD-Phone: One player used a VR headset (HMD), and the other used a tracked smartphone (Phone).
    ◦ HMD-PC: One player used an HMD, and the other used a PC (keyboard/mouse).
• Role Swap: Each pair played twice, swapping roles (HMD and non-HMD) to experience both perspectives.
• Independent Variable: Platform (HMD-Phone vs. HMD-PC).
• Dependent Variables: Co-presence and immersion.
3) Procedure
1. Setup: Conducted in a university lab with an HTC Vive (3x4 m play area), headphones, and a desktop PC.
2. Gameplay:
    ◦ First playthrough: One participant as HMD, the other as Phone/PC.
    ◦ Questionnaires: After each session, participants completed the SUS (usability), IPQ (immersion), and NMMoSP (co-presence).
    ◦ Role swap: Repeated the game with roles reversed.
3. Interview: Semi-structured group interview post-session, transcribed and thematically analyzed.
4) Task
• Game: Stuck in Space, an asymmetric cooperative VR game.
    ◦ HMD Player: Astronaut fixing a space station.
    ◦ Non-HMD Player: Drone assisting the astronaut via Phone (AR) or PC.
• Actions: Collaborative tasks like pressing buttons in sequence, solving keypad codes, using a flashlight, and fixing the drone’s camera (see Table 1 in the paper).
• Goal: Complete nine stages of puzzles requiring communication and coordination. No failure states were included to focus on interaction metrics.
5) Metrics Collected
• Quantitative:
    ◦ Co-presence: Modified NMMoSP questionnaire (sub-scales: Co-Presence, Attentional Allocation, Perceived Message Understanding).
    ◦ Immersion: Modified IPQ questionnaire (sub-scales: General Presence, Spatial Presence, Involvement).
    ◦ Usability: System Usability Scale (SUS).
    ◦ Time: Session duration logged to ensure task completion.
• Qualitative:
    ◦ Thematic analysis of interview transcripts (5 themes, e.g., Cognitive Engagement, Embodiment).
    ◦ Observations (e.g., physical interactions, communication patterns).
Key Findings
• No significant difference in co-presence between HMD-Phone and HMD-PC.
• HMD players reported significantly higher immersion than non-HMD players (p < 0.005), but no difference between HMD-Phone and HMD-PC.
• Qualitative insights: Physical proximity in HMD-Phone increased embodiment but also mental strain (balancing virtual/real worlds), while PC players focused more on the virtual environment.
This study highlights that both Phone and PC are viable for co-located multiplayer VR, with trade-offs in design (e.g., physical proximity vs. interface simplicity).","IPQ (Igroup Presence Questionnaire),  Social Presence,  System Usability Scale (SUS),  Semi-structured Interviews",,,,Questionnaires
ID140,Studying the Influence of Translational and Rotational Motion on the Perception of Rotation Gains in Virtual Environments,"Brument  Hugo,  Marchal  Maud,  Olivier  Anne-Hélène,  Argelaguet Sanz  Ferran",,10.1145/3485279.3485282,2021,Symposium on Spatial User Interaction,VR,Behavioural Dynamics & Exploration,Content & System Design.,Human-Computer Interaction (HCI),"This paper explores how combined translational and rotational movements affect users' ability to detect rotation gains in a 6-DoF Virtual Reality (VR) environment. It demonstrates that rotation gains are harder to perceive at lower rotational speeds, and this detection becomes more challenging when translational motion is introduced, particularly at slower speeds. The research also provides insights into user behaviors by analyzing gaze and body movements in response to varying rotation gains and speeds.","1) Participants
• Number of Participants: 14 (8 males, 6 females)
• Age Range: 21 to 53 years (mean age: 26.43 ± 7.4)
• Experience with VR: 
    ◦ 5 participants reported regular use of VR and HMDs
    ◦ 7 had used them a few times
    ◦ 2 had never used them
• Gaming Experience: Half of the participants had regular experiences with 3D video games https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=17416,18176, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=20353,21015.
2) Study Design
• Type of Study: Within-subjects design
• Conditions: 
    ◦ 2 Translation Speeds: No Translation (nT) at 0 m/s and With Translation (T) at 1.4 m/s
    ◦ 3 Rotational Speeds: 20°, 30°, and 40° per second
    ◦ 6 Rotational Gains: Ranging from 0.5 to 1.5
• Hypotheses: 
    ◦ Adding virtual translational motion would help users better discriminate rotation gains.
    ◦ Slower rotation speeds would result in higher perceived speed errors (PSE) and detection thresholds (DTs) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=16173,16879, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=30457,30989.
3) Procedure
• Session Structure: The experiment was divided into 
two sessions, each lasting approximately 45 minutes, separated by at 
least 24 hours to mitigate cybersickness.
• Initial Steps: Participants signed a consent form and completed a Simulator Sickness Questionnaire (SSQ) and a demographic questionnaire https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=20353,21015, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=64977,65641.
• Tasks:
    ◦ Proprioception Task: Participants performed 90-degree turns in a virtual environment (VE) with and without visual cues.
    ◦ Perception Task: Participants completed four 
randomized blocks of 18 trials, assessing their ability to determine 
whether their virtual rotation speed was faster or slower than their 
real one https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=6063,6884 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=23346,24092.
4) Task
• Proprioception Task: Participants aligned their body with a virtual sphere and performed turns, indicating when they completed the turn.
• Perception Task: Participants faced a virtual 
sphere that moved left or right at specified rotational speeds, and they
 had to align their body to face the sphere throughout the motion. After
 each trial, they answered a forced-choice question regarding their 
perceived rotation speed https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=23346,24092, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=21635,22478.
5) Metrics Collected
• Performance Metrics: 
    ◦ Probability of answering ""Faster"" from the one-alternative forced choice (1AFC) question.
    ◦ Relative error in estimating 90-degree turns during the proprioception task.
• Gaze and Body Segment Analysis: 
    ◦ Head and pelvis orientation were recorded to analyze turning behavior.
    ◦ Eye-tracking data was collected to assess gaze direction and dispersion https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=27369,28285, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=33558,34289.
• Cybersickness Metrics: Pre and post-session SSQ scores were collected to assess the impact of the experimental conditions on participants' comfort https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=24093,24688, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681dc805eb0e761ec5c404e0/?range=28286,29200.",SSQ (Simulator Sickness Questionnaire),"Body Analysis,  Head Analysis",Eye-Tracking,Accuracy,"Questionnaires,  Behavioural,  Physiological,  Performance"
ID141,Subjective Evaluation of Group User QoE in Collaborative Virtual Environment (CVE),"Moharana  Bhagyabati,  Keighrey  Conor,  Scott  David,  Murray  Niall",,10.1145/3534086.3534333,2022,Workshop on Immersive Mixed and Virtual Environment Systems,"Collaborative Virtual Environments, VR",Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"The study evaluates Quality of Experience (QoE) in Collaborative Virtual Environments (CVE), considering immersion, interaction, collaboration, system effects, and post-usage acceptability as key factors.
The study found that Quality of Experience (QoE) in Collaborative Virtual Environments (CVE) varies by user role, with Finders reporting higher collaboration and interaction scores, while Describers experienced greater immersion and system effects. Post-usage acceptability was high across both groups, indicating a positive reception of collaborative VR experiences. Cybersickness and discomfort were minimal, suggesting that interactive tasks help mitigate motion sickness. These findings highlight the importance of role-specific design considerations in multi-user VR applications to optimize user experience.
Questionnaires (Subjective QoE Measures):
• Immersion – Presence and realism in VR.
• Interaction – Ease of manipulation and engagement with virtual objects.
• Collaboration – Synchronization and communication with other users.
• System Effects – Cybersickness, discomfort, and hardware limitations.
• Post-Usage Acceptability – Willingness to use VR in future tasks.
","1) Participants
• Total: 24 participants
• Demographics:
    ◦ Age range: 18–35 years
    ◦ All had normal or corrected-to-normal vision
    ◦ No prior experience with the specific CVE was required
2) Study Design
• Between-subjects design
    ◦ Participants were randomly assigned to one of two roles in the Collaborative Virtual Environment (CVE):
        1. Describer – Provided verbal instructions to help locate virtual objects.
        2. Finder – Used the Describer’s instructions to identify and collect objects in VR.
    ◦ QoE metrics were assessed across both roles to determine how different responsibilities impact user experience.
• Independent Variable:
    ◦ User role (Describer vs. Finder)
• Dependent Variables:
    ◦ Quality of Experience (QoE) scores across five dimensions: immersion, interaction, collaboration, system effects, and post-usage acceptability.
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were given an overview of the VR system and their assigned role.
    ◦ A short training session allowed them to familiarize themselves with the controls and CVE environment.
2. Experimental Phase:
    ◦ Participants worked together in the CVE, with Describers guiding Finders to locate virtual objects.
    ◦ Each participant completed the task in their assigned role while their interactions were monitored.
3. Post-Experiment Phase:
    ◦ Participants completed subjective QoE questionnaires.
    ◦ Structured interviews were conducted to collect qualitative feedback on immersion, collaboration, and system usability.
4) Task
• Collaborative Object-Finding Task:
    ◦ The Describer viewed objects in VR and provided verbal instructions to the Finder.
    ◦ The Finder navigated the virtual space to locate and collect the described objects.
    ◦ The task required effective communication and coordination between both users.
5) Metrics Collected
• Questionnaires (Subjective QoE Measures):
    ◦ Immersion – Presence and realism in VR.
    ◦ Interaction – Ease of manipulation and engagement with virtual objects.
    ◦ Collaboration – Synchronization and communication with other users.
    ◦ System Effects – Cybersickness, discomfort, and hardware limitations.
    ◦ Post-Usage Acceptability – Willingness to use VR in future tasks.
• Qualitative Data:
    ◦ Collected user feedback on collaboration, task difficulty, and usability.",Subjective QoE Measures,,,,Questionnaires
ID142,Subjective Evaluation of the Impact of Spatial Audio on Triadic Communication in Virtual Reality,"Immohr  Felix,  Rendle  Gareth,  Kehling  Christian,  Lammert  Anton,  Göring  Steve,  Froehlich  Bernd,  Walessa  Marc",,10.1109/QoMEX61742.2024.10598292,2024,International Conference on Quality of Multimedia Experience,VR,User states: Cognitive & Affective Experience,Content & System Design.,"Social Interaction / Communication, Telecommunications and Collaboration","This paper investigates how spatial audio influences perceived plausibility and social presence during triadic communication in virtual reality. Sixty-six participants (22 triads) performed a collaborative “survival task” under three conditions: real-world, VR with spatial audio, and VR with non-spatial (diotic) audio. Using the Networked Minds Social Presence Inventory (NM-SPI) and custom plausibility questionnaires, results showed that while real-world interaction scored highest overall, spatial audio yielded slightly higher but not statistically significant improvements over diotic audio. The study concludes that more complex communication settings or additional metrics (e.g., behavioral or physiological) may be needed to detect subtle effects of spatial audio on user experience.","1) Participants
• Number of Participants: 66 (42 male, 24 female).
• Groups: 22 triads (three participants per group).
• Demographics: Average age 26.79 years (SD = 3.72); participants recruited from the university body.
• Familiarity: Nine triads had no prior familiarity, four had full familiarity, and the remaining had partial familiarity among members.
• Screening: Visual acuity checked with a Snellen chart; interpupillary distance individually adjusted.
2) Study Design
• Type: Within-subjects design with three experimental conditions.
• Conditions:
1. REAL: Real-world interaction.
2. SPATIAL: VR with binaural spatial audio.
3. DIOTIC: VR with non-spatial (diotic) audio.
• Objective: To evaluate how spatial audio affects perceived social presence and audiovisual plausibility in triadic VR communication.
3) Procedure
• Participants provided informed consent and filled in a demographic questionnaire (hearing ability, prior VR experience, familiarity).
• Each triad completed a short training phase to familiarize with the virtual environment and equipment.
• The experiment consisted of three six-minute trials, one per condition, presented in counterbalanced order (Greco-Latin square).
• After each condition, participants completed digital questionnaires (NM-SPI and custom plausibility items) using the UNIPARK platform.
• Short breaks were given between trials; total duration was approximately 90 minutes per session.
• Participants received €18 compensation.
4) Task
• Task Type: Collaborative Survival Task adapted from ITU-T Rec. P.1301 Appx. VI.
• Goal: Groups discussed and selected six out of twelve survival items most useful in a given scenario (desert, winter, or sea).
• Interaction: In VR, items appeared as manipulable 3D boxes (11×11×10 cm) placed on the floor; in the REAL condition, equivalent cardboard items were used.
• Ending Condition: Task ended after six selections or when six minutes elapsed.
5) Metrics Collected
• Quantitative Metrics:
◦ Questionnaires – NM-SPI: Co-presence, Perceived Message Understanding, Mutual Assistance.
◦ Custom Plausibility Scale: Ratings on coherence, naturalness, and audiovisual consistency.
◦ Overall Experience Rating (5-point scale).
◦ Preference Rankings for each condition.
• Qualitative Metrics:
◦ Post-experiment open feedback on perceived realism, comfort, and communication.
• Recorded Data (for future analysis):
◦ Speech and scene state recordings, HMD and controller tracking data.","Networked Minds Social Presence Inventory (NMSPI),  QoE - ACR",,,,Questionnaires
ID143,Subjective Evaluation of Visual Quality and Simulator Sickness of Short 360 Videos: ITU-T Rec. P.919,"Gutiérrez  Jesús,  Pérez  Pablo,  Orduna  Marta,  Singla  Ashutosh,  Cortés  Carlos,  Mazumdar  Pramit,  Viola  Irene,  Brunnström  Kjell,  Battisti  Federica,  Cieplińska  Natalia,  Juszka  Dawid,  Janowski  Lucjan,  Leszczuk  Mikołaj,  Adeyemi-Ejeye  Anthony,  Hu  Yaosi,  Chen  Zhenzhong,  Wallendael  Glenn Van,  Lambert  Peter,  Díaz  César,  Hedlund  John,  Hamsis  Omar,  Fremerey  Stephan,  Hofmeyer  Frank,  Raake  Alexander,  César  Pablo,  Carli  Marco,  García  Narciso",,10.1109/TMM.2021.3093717,2022,Transactions on Multimedia,VR,Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"This paper assesses and validates subjective evaluation methodologies for 360◦ VR video. Audiovisual quality, simulator sickness symptoms, and exploration behaviour were evaluated with short (from 10 seconds to 30 seconds) 360◦ sequences. The following factors’ influences were also analyzed: assessment methodology, sequence duration, HMD device, uniform and non-uniform coding degradations, and simulator sickness assessment methods. The obtained results have demonstrated the validity of ACR and DCR for subjective tests with 360◦ videos. Also, more efficient methods than the long Simulator Sickness Questionnaire have been proposed to evaluate related symptoms with 360◦ videos.","1) Participants
• A total of 306 participants took part in the cross-lab tests, with an age range of 18 to 79 years (average age of 28.8). The gender distribution was 38.9% women and 61.1% men. Vision screening was performed to ensure participants had standard or corrected-to-normal vision https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=37002,37667, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=35265,36156.
2) Study Design
• The study employed a cross-lab design involving ten laboratories. Participants evaluated two test conditions in a within-subject design, where each participant experienced both conditions https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=35265,36156, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=26904,27642.
3) Procedure
• The experimental procedure included:
    ◦ An introductory session where participants received instructions, 
underwent visual screening, and filled out consent forms and background 
questionnaires.
    ◦ Evaluation of test stimuli for the first condition, followed by a 15-minute break, and then evaluation of the second condition.
    ◦ Participants completed questionnaires to assess simulator sickness at various points during the session https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=36159,37001, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=32327,33112.
4) Task
• Participants were tasked with watching and exploring 360° videos
 of varying lengths (10, 20, and 30 seconds) and rating them based on 
audiovisual quality and simulator sickness. Two methodologies were used 
for quality assessment:
    ◦ Absolute Category Rating (ACR): Participants rated videos independently on a five-point scale.
    ◦ Degradation Category Rating (DCR): Participants first watched a reference video and then rated the degradation https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=1557,2193, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=31045,31750.
5) Metrics Collected
• The following metrics were collected during the experiments:
    ◦ Audiovisual Quality Ratings: Mean Opinion Scores (MOS) were collected using ACR and DCR methodologies.
    ◦ Simulator Sickness: Assessed using the Simulator Sickness Questionnaire (SSQ), Vertigo Scale, and Short-SSQ at multiple points during the session https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=32327,33112, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6810d01802b5cb31d42ea059/?range=24336,24949.
This summary encapsulates the key aspects of the experiments 
conducted, providing a clear overview of the participants, study design,
 procedure, tasks, and metrics collected.","Absolute Category Rating,  Degradation Category Rating,  MOS,  NASA-TLX,  SSQ (Simulator Sickness Questionnaire),  Vertigo Scale",Head Analysis,,,"Questionnaires,  Behavioural"
ID144,Subjective QoE Evaluation of User-Centered Adaptive Streaming of Dynamic Point Clouds,"Subramanyam  Shishir,  Viola  Irene,  Jansen  Jack,  Alexiou  Evangelos,  Hanjalic  Alan,  Cesar  Pablo",,10.1109/QoMEX55416.2022.9900879,2022,International Conference on Quality of Multimedia Experience,VR,Content & System Design,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"In this paper, the authors perform a user study in VR to quantify the gains adaptive tile selection strategies can bring with respect to non-adaptive solutions.  Adaptive streaming that raises the quality of tiles inside the user’s field of view outperforms non-adaptive baselines for every point-cloud sequence tested. The weighted-hybrid allocation strategy (W3) produces the best Mean-Opinion-Scores and achieves comparable visual quality to a uniform stream at only a third of the bitrate, whereas a greedy strategy that causes sharp tile-to-tile jumps is least liked. V-PCC compression still tops the quality chart but at much higher computational cost. Participants moved around more while viewing adaptive content; movement patterns did not fade with time or depend on VR experience. Simulator-sickness scores stayed low throughout, and single-item Vertigo or VRSQ questions tracked the full SSQ when symptoms were mild.","1) Participants
• A total of 30 participants were recruited (16 males, 14 females).
• Participants had varying levels of VR experience: 
    ◦ 15 reported 1-3 prior VR experiences.
    ◦ 6 participants had never used a VR headset before.
    ◦ 9 participants declared to be very experienced with VR https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=1742,2510, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=15521,16324.
2) Study Design
• The study employed a subjective evaluation methodology using the Absolute Category Rating test method with Hidden References (ACR-HR), following ITU-T Recommendation P.910 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=15521,16324, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17830,18647.
• The experiment was divided into three stages: screening, training, and testing https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=16325,17062, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17063,17829.
3) Procedure
• Screening: Participants' color vision was checked using the Ishihara chart.
• Training: Participants were shown three versions of
 content not used in the test, representing examples of different 
quality ratings (1-Bad, 5-Excellent, and 3-Fair).
• Testing: 
    ◦ Participants viewed a loop of 300 frames of each dynamic point cloud sequence at 30 fps for a minimum of 10 seconds.
    ◦ The order of stimuli was randomized for each participant and 
session, with dummy samples added at the beginning to ease participants 
into the task https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17063,17829, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=15521,16324.
4) Task
• Participants were tasked with rating the visual quality of point 
cloud sequences on a scale from 1 to 5 (1-Bad, 2-Poor, 3-Fair, 4-Good, 
5-Excellent) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=16325,17062, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=14746,15520.
• The sequences were rendered with randomized initial rotations to encourage user movements and prevent bias https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=16325,17062, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=23834,24659.
5) Metrics Collected
• Subjective Quality Scores: Ratings were collected for each stimulus, and outlier detection was performed on these scores https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17063,17829, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17830,18647.
• Mean Opinion Score (MOS): Computed for each stimulus independently per viewing condition https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17830,18647, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17063,17829.
• Differential MOS (DMOS): Obtained by applying hidden reference removal https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=17830,18647, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=18648,19581.
• Simulator Sickness Questionnaire (SSQ): Participants filled this out before and after each session to assess cybersickness https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=27422,28049, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=34802,35470.
• User Navigation Data: The position and rotation of the user’s viewport were recorded to analyze movement patterns https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=24660,25536, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6821c08e695f58ee68285c33/?range=13148,13926.","MOS,  SSQ (Simulator Sickness Questionnaire)",Movement Trajectories,,,"Questionnaires,  Behavioural"
ID145,Surround Sound Spreads Visual Attention and Increases Cognitive Effort in Immersive Media Reproductions,"Mendonça  Catarina,  Korshunova  Victoria",,10.1145/3411109.3411118,2020,International Audio Mostly Conference,VR,Content & System Design,Behavioural Dynamics & Exploration.,"Entertainment and Gaming, Human-Computer Interaction (HCI)","The study investigates how different spatial sound configurations (mono, stereo, 5.1, and 7.4.1) influence visual attention and cognitive effort in an immersive audiovisual environment. Findings show that wider spatial sound configurations (5.1 and 7.4.1) increase gaze dispersion and cognitive effort, while mono sound results in more focused attention. The study found that wider spatial sound configurations (5.1 and 7.4.1) increased visual attention dispersion and cognitive effort, as measured by eye-tracking and pupil dilation. Participants in these conditions exhibited more frequent gaze shifts toward non-task-relevant areas, suggesting that immersive audio can enhance presence but also introduce distractions. In contrast, mono and stereo sound focused visual attention more effectively, reducing cognitive load. These findings highlight the need for careful spatial sound design in XR to balance immersion and user performance.

Fixation Distribution – Measures gaze dispersion and attention shifts.
Gaze Heatmaps – Visualizes areas of interest and distraction based on spatial sound conditions.
Pupil Dilation – Used as an indicator of cognitive load, showing that immersive sound increased task difficulty.","1) Participants
• Total: 30 participants
• Demographics:
    ◦ Age range: 18–35 years
    ◦ All had normal or corrected-to-normal vision
    ◦ No reported hearing impairments
2) Study Design
• Within-subjects design
    ◦ Each participant experienced all four spatial sound conditions:
        1. Mono (1.0)
        2. Stereo (2.0)
        3. Surround (5.1)
        4. 3D Immersive (7.4.1)
    ◦ Counterbalancing: The order of sound conditions was randomized to prevent learning effects.
• Independent Variable:
    ◦ Spatial sound configuration (Mono, Stereo, 5.1, 7.4.1)
• Dependent Variables:
    ◦ Gaze behavior (fixation distribution, heatmaps)
    ◦ Cognitive load (pupil dilation as an indicator of effort)
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were briefed on the study and given eye-tracking calibration.
    ◦ A baseline pupil dilation measurement was taken in a neutral environment.
2. Experimental Phase:
    ◦ Participants watched immersive audiovisual content under each spatial sound condition.
    ◦ Their eye movements and pupil dilation were recorded in real time.
    ◦ After each condition, participants rested to avoid carryover effects.
3. Post-Experiment Phase:
    ◦ Participants were debriefed on the purpose of the study.
4) Task
• Passive Viewing Task:
    ◦ Participants watched a series of immersive 360° video clips with different spatial sound conditions.
    ◦ They were not given specific instructions but were expected to naturally explore the environment with their gaze.
5) Metrics Collected
• Behavioral Metrics (Eye-Tracking):
    ◦ Fixation Distribution – Measures spread of visual attention across the scene.
    ◦ Gaze Heatmaps – Identifies high-attention areas and distractions.
• Physiological Metrics:
    ◦ Pupil Dilation – Used as an indicator of cognitive effort, with wider spatial sound configurations increasing cognitive load.",,Gaze heatmap,"Eye-Tracking,  Pupil Analysis",,"Behavioural,  Physiological"
ID146,"Take a Seat - the Influence of Sitting down in a Virtual Environment on Workload, User Experience and Presence","Brade  Jennifer,  Rüffert  Danny,  Kögel  Alexander,  Bernhagen  Max,  Klimant  Franziska,  Bullinger  Angelika C.",,10.1145/3473856.3473991,2021,Mensch Und Computer,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"The study found that standing-only conditions in VR led to higher presence and ecological validity compared to a stance-switching approach (sitting and standing). Participants reported trust issues with the real chair, which disrupted immersion and reduced presence, highlighting the importance of seamless integration between virtual and physical elements. Cognitive workload (NASA-TLX) was similar across conditions, suggesting that stance-switching did not add significant strain. While some participants preferred standing for greater movement freedom, others appreciated sitting for stability and realism, indicating that user preference plays a key role in designing VR experiences that incorporate real-world physical objects.

ITC-Sense of Presence Inventory (ITC-SOPI) – Assesses spatial presence, engagement, ecological validity, and negative effects.User Experience Questionnaire (UEQ) – Measures attractiveness, perspicuity, efficiency, dependability, stimulation, and novelty of the experience.NASA Task Load Index (NASA-TLX) – Evaluates cognitive workload while performing the VR assembly task.","1) Participants
• Total: 30 participants
• Demographics:
    ◦ Age range: 19–39 years
    ◦ No prior VR experience required
    ◦ All participants were healthy with no reported mobility issues
2) Study Design
• Within-subjects design
    ◦ Each participant experienced both conditions:
        1. Standing-only – Participants remained standing throughout the VR task.
        2. Stance-switching (Sitting & Standing) – Participants were allowed to sit during specific parts of the task.
    ◦ Counterbalancing: The order of conditions was randomized to control for learning effects.
• Independent Variable:
    ◦ Sitting condition (Standing-only vs. Stance-switching (Sitting & Standing))
• Dependent Variables:
    ◦ Presence (ITC-SOPI), Usability (UEQ), Workload (NASA-TLX), and User Perceptions (Interviews).
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants received an introduction to VR and task instructions.
    ◦ They completed a training phase to familiarize themselves with the VR controls and environment.
2. Experimental Phase:
    ◦ Participants performed the virtual assembly task under both sitting and standing conditions.
    ◦ Gave real-time feedback on their experiences after each condition.
3. Post-Experiment Phase:
    ◦ Participants completed questionnaires (ITC-SOPI, UEQ, NASA-TLX) to assess their experiences.
    ◦ Conducted structured interviews on presence, usability, and workload.
4) Task
• Virtual Assembly Task:
    ◦ Participants assembled a virtual object in a controlled VR workspace.
    ◦ Required grabbing, positioning, and interacting with virtual components.
    ◦ The task was designed to simulate real-world seated and standing assembly work.
5) Metrics Collected
• Questionnaires:
    ◦ ITC-Sense of Presence Inventory (ITC-SOPI) – Measures presence, engagement, ecological validity, and negative effects.
    ◦ User Experience Questionnaire (UEQ) – Evaluates perceived usability and experience quality.
    ◦ NASA Task Load Index (NASA-TLX) – Assesses cognitive workload during the assembly task.
• Qualitative Data:
    ◦ Structured Interviews – Captures user perceptions of sitting vs. standing in VR.","ITC-Sense of Presence Inventory,  NASA-TLX,  UEQ (User Experience Questionnaire),  Semi-structured Interviews",,,,Questionnaires
ID147,Task-Based Methodology to Characterise Immersive User Experience with Multivariate Data,"Robert  Florent,  Wu  Hui-Yin,  Sassatelli  Lucile,  Winckler  Marco",,10.1109/VR58804.2024.00092,2024,Conference on Virtual Reality and 3D User Interfaces,VR,Behavioural Dynamics & Exploration,,Human-Computer Interaction (HCI),"This paper introduces a task-based methodology for analyzing experiential UX in VR that (1) aligns low-level tasks with multivariate behavior metrics (gaze, motion, skin conductance), (2) defines performance components (attention, decision, efficiency) with baseline values for evaluation, and (3) characterizes task performance using multivariate user behavior data. This methodology was applied to an existing dataset from a VR road-crossing study, and the results show fine-grained relationships between behavior profiles and task performance. The paper identified individual profiles of users for whom a behavioural metric is strongly correlated to the performance of the task.","Existing Dataset in 
F. Robert, H.-Y. Wu, L. Sassatelli, S. Ramanoel, A. Gros, and
M. Winckler. An integrated framework for understanding multimodal embodied experiences in interactive virtual reality. In Proceedings of the 2023 ACM International Conference on Interactive Media Experiences, pp. 14–26, 2023.

1) Participants
• Number: 40 participants
• Activity: Each participant engaged in 24 virtual reality (VR) scenarios
• Total scenarios: 960 road-crossing tasks recorded
2) Study Design
• Design Type: Experimental, within-subjects
• Dataset Used: CREATTIVE3D multimodal dataset (collected in prior work by the authors)
• Scenarios: 6 scenarios, each with varied interactivity and stress-inducing elements
• Conditions: 4 combinations of:
    ◦ Normal vs. simulated low vision
    ◦ Real physical walking vs. simulated joystick-based walking
3) Procedure
• Participants engaged in short (≈2-minute) VR scenarios involving everyday activities, notably:
    ◦ Exiting a virtual home
    ◦ Crossing a virtual road with simulated traffic
    ◦ Performing tasks like grabbing objects and using them appropriately
• Tasks were decomposed into high-level and low-level steps (e.g., grabbing a key, pressing a button)
4) Task
Participants performed a sequence of realistic VR tasks such as:
• Open the door (find key → grab key → go to door → open door)
• Cross the road safely (observe traffic light & cars → cross at green)
• Take objects (e.g., garbage bag or box)
• Deposit objects (e.g., in trashcan or on table)
Each scenario included some combination of these tasks. One of the most complex tasks—crossing the road—was used as a focus to illustrate the methodology.
5) Metrics Collected
Multivariate behavioral and physiological data:
• Contextual/System Logs:
    ◦ Object positions, traffic light states, car positions, user interactions, current task
• User Logs:
    ◦ Gaze focus (lookedAtItemName), objects in view, task states, user actions
• Motion Capture (60Hz):
    ◦ 17 body sensor positions and rotations (Xsens MVN Awinda suit)
• Physiological Sensors (15Hz):
    ◦ Skin Conductance (EDA), Heart Rate (Shimmer GSR3+)
• Eye and Head Tracking (120Hz):
    ◦ Gaze vector, pupil size, eye openness (HTC Vive Pro Eye)
Derived Metrics for Performance Evaluation:
• Efficiency: Time to complete task based on GOMS-like modeling
• Attention: Whether the participant looked at key scene elements (e.g., traffic light, car)
• Decision: Whether the participant acted correctly (e.g., crossing only when traffic light is green)
Behavioral Analysis Metrics:
• Electrodermal activity (EDA) — emotional arousal
• Gaze Fixation Duration (GFD) — visual attention
• Center of Pressure Inclination (COPI) — body balance and movement",,"Body Analysis,  Movement Trajectories,  Gaze Analysis","EDA / GSR (Skin Conductance),  Heart Rate",Completion Time,"Behavioural,  Physiological,  Performance"
ID148,"Task-Dependent Visual Behavior in Immersive Environments: A Comparative Study of Free Exploration, Memory and Visual Search","Malpica  Sandra,  Martin  Daniel,  Serrano  Ana,  Gutierrez  Diego,  Masia  Belen",,10.1109/TVCG.2023.3320259,2023,Transactions on Visualization and Computer Graphics,VR,Behavioural Dynamics & Exploration,,Human-Computer Interaction (HCI),"The study provides broad insights into gaze and attention mechanisms in VR, useful for designing task-oriented XR applications (e.g., training, education, entertainment).
It introduces a structured dataset of eye-tracking and head movement metrics for cognitive tasks, offering a validated experimental approach for analyzing visual behavior in VR.
The study demonstrates that visual behavior in VR is highly task-dependent, with significant differences in fixation duration, saccade amplitude, and head movement patterns across free exploration, memory, and visual search tasks. Free exploration led to longer, more dispersed fixations, while memory tasks involved a mix of short and long fixations with frequent revisits to key areas. Visual search induced short fixations and high saccade frequency, reflecting active scanning behavior. Head movement entropy was highest in memory tasks, suggesting more dynamic search strategies, while eye eccentricity remained stable across conditions, confirming that head movement can serve as a reliable proxy for gaze direction in VR. These findings provide valuable insights for designing task-specific VR environments that optimize visual attention and cognitive engagement.
Eye gaze:
Number of Fixations (per second).
Duration of Fixations.
Dwell Time.
Number of Saccades (per second).
Duration of Saccades.
Amplitude of Saccades.
Eye Eccentricity: The average angular distance between gaze points and the viewport centre, which indicates how much participants explored peripheral areas.

Head Orientation Entropy: Shannon entropy of head orientation, capturing the variety in head movement patterns and suggesting how participants scanned their surroundings.","1) Participants
• Total: 30 participants
• Demographics:
    ◦ Age range: 18–35 years
    ◦ All had normal or corrected-to-normal vision
    ◦ No prior VR experience was required
2) Study Design
• Within-subjects design
    ◦ Each participant completed all three tasks (free exploration, memory, and visual search).
    ◦ Tasks were presented in randomized order to control for learning effects.
• Independent Variable:
    ◦ Task type (Free Exploration, Memory, Visual Search)
• Dependent Variables (User Metrics):
    ◦ Gaze and head movement behaviors
    ◦ Fixation and saccade characteristics
    ◦ Head movement entropy
3) Procedure
1. Pre-Experiment Phase:
    ◦ Participants were briefed about the study and VR setup.
    ◦ They completed a calibration phase for eye-tracking to ensure accuracy.
2. Experimental Phase:
    ◦ Participants wore a VR headset with integrated eye-tracking and performed three tasks in a virtual environment.
    ◦ Each task required different levels of attention, memory, and search behavior.
    ◦ Gaze and head movements were recorded in real time.
3. Post-Experiment Phase:
    ◦ Participants provided feedback on task difficulty and experience.
    ◦ Data was collected for fixation, saccade, and head movement analysis.
4) Task
• Task 1: Free Exploration
    ◦ Participants freely observed the virtual scene without a specific goal.
    ◦ Used to establish baseline gaze and head movement patterns.
• Task 2: Memory Task
    ◦ Participants memorized specific objects and their locations.
    ◦ Later, they had to recall and locate those objects.
    ◦ Examined fixation revisits and head movement patterns related to memory retrieval.
• Task 3: Visual Search Task
    ◦ Participants were given a target object and had to find it in the virtual environment as quickly as possible.
    ◦ Measured active scanning behavior (fixation duration, saccade frequency, head movement entropy).
5) Metrics Collected
• Behavioral Metrics (Eye and Head Movements):
    ◦ Fixation Rate (Fixations per second) – Measures how frequently participants fixated on objects.
    ◦ Fixation Duration – Measures cognitive processing time per fixation.
    ◦ Dwell Time per Trial – Total time spent fixating in a session.
    ◦ Saccade Rate (Saccades per second) – Measures search and scanning activity.
    ◦ Saccade Duration and Amplitude – Reflects visual exploration strategies.
    ◦ Eye Eccentricity – Measures gaze distribution across the field of view.
    ◦ Head Orientation Entropy – Shannon entropy of head movements, indicating task complexity and variability in search behavior.
",,"Head Analysis,  Entropy Analysis,  Gaze Analysis,  Fixation Analysis",,,Behavioural
ID149,The Application of Eye Tracking on User Experience in Virtual Reality,"Su  Chuanzhi,  Huang  Mengjie,  Zhang  Jingjing,  Yang  Rui",,10.1109/CVR58941.2023.10395365,2023,International Conference on Cognitive Aspects of Virtual Reality (CVR),VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This study explores eye-tracking as a primary interaction method in virtual reality, beyond its typical use for object selection. It evaluates eye-only, hand-only, and hand-eye hybrid interactions in VR. Results suggest that eye-tracking interaction is perceived as the most favorable and controllable, highlighting its potential as a standalone input method.","1) Participants
• Number of Participants: 12 volunteers (8 males and 4 females)
• Age: 21 ± 1.50 years
• Background: All participants were current 
university students, in good visual and physical condition. Three were 
left-handed, and five had no previous VR experience. None wore glasses 
or contact lenses that could affect pupil measurement https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=15791,16716, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=15070,15790.
2) Study Design
• The study employed a within-subjects design, where each participant experienced three different interaction methods:
    1. Eye interaction
    2. Hand interaction
    3. Combined eye and hand interaction
• The order of the interaction methods was randomized to control for order effects https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=27883,28784, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=26930,27882.
3) Procedure
• Participants wore a head-mounted display (HTC VIVE Pro Eye) and stood in a fixed experimental area.
• Each participant completed tasks using the three different 
interaction methods, with eye calibration performed before the 
experiments to adjust for individual differences in eye size and spacing
 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=26930,27882, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=19165,20006.
4) Task
• The experimental task involved participants acting as a knight in a 
virtual medieval church setting, where they needed to unlock and knock 
down six ghosts using different interaction methods:
    ◦ Eye Interaction: Participants used their eyes to shoot a laser to unlock and knock down ghosts.
    ◦ Hand Interaction: Participants used a 3D printed sword to unlock and knock down ghosts.
    ◦ Combined Interaction: Participants used their eyes to unlock ghosts and then used the sword to knock them down https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=18315,19164, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=17529,18314.
5) Metrics Collected
• Objective Metrics: 
    ◦ Task completion time for each interaction method.
    ◦ Eye tracking data to assess gaze behavior and interaction efficiency.
• Subjective Metrics: 
    ◦ User experience ratings collected via a 5-point Likert scale questionnaire, focusing on:
        ▪ Sense of agency
        ▪ Presence
        ▪ System usability https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=21632,22440, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=24293,25177.
• Post-Experimental Interviews: Participants provided qualitative feedback on their experiences with the different interaction methods https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=20007,20931, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/682363700c6f2fdffec1d78a/?range=26930,27882.","Post-Experiment Interview,  Presence – General,  Sense of Agency,  System Usability Scale (SUS)",Gaze Analysis,,Completion Time,"Questionnaires,  Behavioural,  Performance"
ID150,The Correlation Analysis Between Cybersickness and Postural Behavior in Immersive VR Experience,"Zhong  Ying,  Zhao  Ke-Ao,  Zhang  Leping,  Zhao  Fangming,  Wei  Wentao,  Han  Feilin",,10.1109/ICME57554.2024.10687394,2024,International Conference on Multimedia and Expo,VR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,Cybersickness,"This paper investigates the relationship between cybersickness intensity and postural behaviour in immersive VR by analysing forearm surface electromyography (sEMG) signals and hand-movement trajectories. In a controlled VR viewing task designed to induce visually driven self-motion discomfort, 33 participants provided repeated cybersickness ratings (FMS) and SSQ data across three exposure sessions while wearing a Myo armband and being recorded via RGB camera. By correlating sEMG features (MAV, RMS, MDF, MNF) and joint-movement patterns with cybersickness ratings, the authors show that muscle activation in specific forearm channels and involuntary hand movements increase as cybersickness becomes more severe. The results validate the postural instability theory and demonstrate the potential of sEMG-based continuous cybersickness tracking, offering a promising physiological metric for VR content creators to monitor user discomfort without interrupting immersion.","Participants
• 33 participants (after exclusions), ages ~21
• MSSQ used for screening; all had prior VR exposure
• One participant stopped early due to severe sickness
Procedure
• Baseline sEMG recording (30s relaxed)
• VR forest-touring video designed to induce cybersickness
• Three sessions × 220 seconds each
• FMS ratings every 15 seconds (voice cue inside VR)
• SSQ collected pre-session and after each session
• RGB camera captured hand & arm motion
• Myo armband recorded sEMG at 200 Hz
Data Collected
• Subjective metrics:
    ◦ FMS (continuous, every 15s)
    ◦ SSQ (pre and post each session)
• Physiological metrics:
    ◦ sEMG channels (MAV, RMS, MDF, MNF)
    ◦ Power spectral density
• Behavioural metrics:
    ◦ Joint trajectories for 11 hand joints (via MediaPipe)
    ◦ Movement amplitude and direction (x, y, z)
    ◦ Wrist & finger movement patterns
Key Results
• Higher cybersickness corresponds to:
    ◦ ↑ Muscle activation (MAV, RMS) in forearm channels 3, 6, 7, 8
    ◦ ↑ Power spectral density
    ◦ ↑ Wrist movement (z-axis) and lateral displacement
    ◦ Finger clenching, thumb extension
• Strong correlations between FMS ratings and sEMG features (channels 6 & 8 strongest)
• Hand-movement patterns show consistent involuntary behaviour
• SSQ increases significantly between sessions 0→1 and 1→2","Fast Motion Sickness Scale,  MSSQ,  SSQ (Simulator Sickness Questionnaire)",Hand Movement,EMG (facial/muscle),,"Questionnaires,  Behavioural,  Physiological"
ID151,"The Effect of VR Gaming on Discomfort, Cybersickness, and Reaction Time","Vlahovic  Sara,  Suznjevic  Mirko,  Pavlin-Bernardic  Nina,  Skorin-Kapov  Lea",,10.1109/QoMEX51781.2021.9465470,2021,International Conference on Quality of Multimedia Experience,VR,User states: Cognitive & Affective Experience,,Entertainment and Gaming,"This paper examined how three popular room-scale VR games— a rhythm sword-slicing title, a cooking pick-and-place game, and a wave-shooter— influence post-play comfort, workload, and cognitive speed. After a 20-minute session with each game, players filled out the Simulator Sickness Questionnaire, Borg CR-10 pain and fatigue items, and the SIM-TLX workload scale, then completed simple and four-choice reaction-time tests. All three games raised cybersickness scores and arm fatigue, but the increases remained mild overall. The cooking game generated the highest workload and the biggest reaction-time slow-down, the rhythm game prompted the most arm fatigue and sweating, and the shooter caused the least reaction-time delay despite higher frustration. The authors conclude that physically active VR titles can tax muscles and slow responses even without artificial locomotion, so evaluations should track ergonomic strain alongside cybersickness.","
1. Participants:
    ◦ Number: 20 participants (10 females, 10 males).
    ◦ Age: 20 to 29 years (Mean = 24.45, SD = 2.58).
    ◦ VR Experience:
        ▪ 5 participants were inexperienced (never tried VR before).
        ▪ 10 participants were beginners (used VR a few times).
        ▪ 4 participants were intermediate users.
        ▪ 1 participant was an expert VR user.
    ◦ Gaming Experience:
        ▪ 4 participants were beginners.
        ▪ 8 participants were intermediate.
        ▪ 8 participants were experts.
2. Study Design:
    ◦ Type: Within-subjects design.
    ◦ Games Tested: Three different VR games were tested:
        ▪ Beat Saber (Rhythm game, slicing interaction).
        ▪ Order Up VR (Cooking simulator, pick and place interaction).
        ▪ Serious Sam VR: The Last Hope (Action/shooter, shooting interaction).
    ◦ Sessions:
 Each participant took part in three gaming sessions (one for each 
game), conducted on different days to avoid symptom accumulation.
    ◦ Randomization: The order of the games was randomized for each participant.
3. Procedure:
    ◦ Pre-Study Questionnaire: Participants provided demographic information and self-assessed their VR and gaming experience.
    ◦ Baseline Measurements:
 Before each VR session, participants filled out the Simulator Sickness 
Questionnaire (SSQ) and assessed baseline pain and muscle fatigue using 
the Borg CR-10 scale. Baseline reaction times were also collected using 
the Deary-Liewald Reaction Time (DLRT) task.
    ◦ Tutorial: Each session began with a short tutorial to familiarize participants with the game mechanics.
    ◦ VR Session: Participants engaged in a 20-minute gameplay session for each game.
    ◦ Post-Session Measurements:
 After each session, participants retook the DLRT test, completed the 
SSQ, and provided evaluations of pain, muscle fatigue, and discomfort 
(using the Borg CR-10 scale). They also evaluated workload using the 
SIM-TLX questionnaire.
4. Task:
    ◦ Beat Saber: Participants sliced incoming cubes with virtual sabers, dodging walls by squatting and moving laterally.
    ◦ Order Up VR: Participants picked up and placed virtual objects to prepare meals, requiring full-body rotation and bending.
    ◦ Serious Sam VR: The Last Hope:
 Participants shot at incoming enemies and protected themselves by 
lifting their arms, requiring quick reactions and arm movements.
5. Metrics Collected:
    ◦ Questionnaires:
        ▪ Simulator Sickness Questionnaire (SSQ): Measured cybersickness symptoms (nausea, oculomotor symptoms, disorientation).
        ▪ Borg CR-10 Scale: Assessed pain, muscle fatigue, and discomfort (e.g., headset weight, temperature, fit).
        ▪ SIM-TLX Questionnaire: Evaluated workload (physical demand, temporal demand, frustration, etc.).
    ◦ Performance Metrics:
        ▪ Reaction Time (RT): Simple Reaction Time (SRT) and Choice Reaction Time (CRT) were measured using the DLRT task.
    ◦ Behavioral Metrics:
        ▪ Physical Demand: Assessed through gameplay observations and participant feedback on physical exertion.
    ◦ Physiological Metrics:
        ▪ Localized Pain and Muscle Fatigue: Measured using the Borg CR-10 scale, focusing on arms, neck, upper back, and lower back.
Summary:
The
 study involved 20 participants who each experienced three different VR 
games in separate sessions. Baseline and post-session measurements were 
taken to assess cybersickness, discomfort, pain, muscle fatigue, and 
cognitive performance (reaction time). The tasks varied by game, 
requiring different physical interactions (slicing, pick and place, 
shooting). The study collected a wide range of metrics, including 
questionnaires (SSQ, Borg CR-10, SIM-TLX), performance metrics (reaction
 time), and behavioral observations (physical demand). The results 
provided insights into how different VR game mechanics affect user 
comfort and cognitive performance.","Borg scale,  SIM-TLX,  SSQ (Simulator Sickness Questionnaire)",,,Cognitive Task Performance,"Questionnaires,  Performance"
ID152,The Effects of Visual Realism on Spatial Memory and Exploration Patterns in Virtual Reality,"Huang  Jiawei,  Klippel  Alexander",,10.1145/3385956.3418945,2020,ACM Symposium on Virtual Reality Software and Technology,VR,Content & System Design,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"This study assessed the impact of visual realism on spatial memory, gaze, and locomotion patterns in VR. Results showed no significant effect of realism on memory performance, but individual differences (spatial ability and gender) accounted for significant variance. Participants in low-realism environments showed longer gaze durations and more clustered movement, indicating more effortful exploration. Despite equivalent task outcomes, users may benefit from high realism through reduced effort, suggesting nuanced learning affordances rather than performance boosts. Gaze and locomotion data aligned, showing behavioral patterns tied to visual conditions.

object location memory:
We measured the individual difference with object location memory test using the online version [13] developed based on the original version [37]. The test is in 2D on a computer screen, in which participants study an array of objects for a designated period of time, and the array of objects will then disappear. Next, the array of objects will reappear, but some of the objects will have exchanged positions. Participants then select the objects that have moved. Participants have five trials to complete the task. 
Spatial working
memory. We also collected spatial working memory through the Corsi block-tapping test that we developed on Unity based on the
original version [8]","1) Participants
• Total: 20 participants
• Demographics:
    ◦ Detailed demographics (e.g., age, gender) are not fully specified in the paper
    ◦ All participants completed 4 trials and used VR equipment with eye-tracking
    ◦ Participants had normal or corrected vision
2) Study Design
• Between-subjects design
    ◦ Participants were assigned to one of two visual realism conditions:
        1. High-Realism Virtual Environment
        2. Low-Realism Virtual Environment
• Independent Variable:
    ◦ Visual realism level of the VR scene
• Covariates (Control Variables):
    ◦ Gender
    ◦ Spatial working memory score from VSNA
• Dependent Variables:
    ◦ Object location memory (placement accuracy)
    ◦ Gaze behavior (fixations)
    ◦ Locomotion clustering
    ◦ Presence and usability ratings
3) Procedure
1. Pre-VR Phase:
    ◦ Participants completed a spatial working memory task (Virtual Spatial Navigation Assessment) to provide a baseline cognitive profile.
2. VR Exploration Phase (4 trials):
    ◦ In each trial, participants explored a virtual environment containing 12 target objects, which were moveable.
    ◦ Across all 4 trials, each participant encountered 48 total objects.
    ◦ Eye-tracking and movement (locomotion) data were recorded during this free exploration period.
3. Post-Exploration Phase (Recall):
    ◦ After each trial, participants used a 2D top-down interface to place the 12 objects in the positions they recalled.
4. Post-Experiment Questionnaire:
    ◦ Participants rated their sense of presence (SPES) and perceived usability of the VR experience.
4) Task
• Spatial Memory and Exploration Task:
    ◦ Participants were instructed to memorize object positions while freely navigating a VR environment.
    ◦ After each trial, they recalled and placed all 12 objects on a 2D map.
    ◦ The task assessed how visual realism influenced exploration strategies and memory accuracy.
5) Metrics Collected
• Performance Metrics:
    ◦ Object Location Memory: Euclidean distance error between recalled and actual object positions
    ◦ Spatial Working Memory Score: Assessed prior to VR exposure via VSNA
• Behavioral Metrics:
    ◦ Eye-Tracking: Gaze duration and fixations during object viewing
    ◦ Locomotion: Clustering of movement paths during exploration
• Questionnaires (Subjective UX):
    ◦ SPES (Spatial Presence Experience Scale)
    ◦ Usability Rating: Heuristic-based evaluation","Spatial Presence Experience Scale,  VR system evaluation","Movement Trajectories,  Gaze Analysis",,"Memory Performance,  Cognitive Task Performance","Questionnaires,  Behavioural,  Performance"
ID153,"The Impact of Avatar and Environment Congruence on Plausibility, Embodiment, Presence, and the Proteus Effect in Virtual Reality","Mal  David,  Wolf  Erik,  Döllinger  Nina,  Wienrich  Carolin,  Latoschik  Marc Erich",,10.1109/TVCG.2023.3247089,2023,Transactions on Visualization and Computer Graphics,VR,Embodiment Avatars & Social Presence,,Human-Computer Interaction (HCI),"Avatar plausibility is significantly influenced by avatar-environment congruence.Spatial presence and embodiment are NOT significantly affected by congruence.The Proteus effect (behavior change based on avatar characteristics) is only observed in participants who report high virtual body ownership (VBO).The study reinforces the importance of body ownership in XR user experience and avatar-related effects.

Four self-report questions captured participants’ assessment of whether they (1) attempted to do as many repetitions as possible in the given time, (2) put effort into the execution of the exercises (both commitment), (3) enjoyed performing the exercises (enjoyment), and (4) intend to do movement exercises more often in near future than they have done so far (future motivation). Participants were instructed to tick the extent to which the statements applied on a 7-point Likert scale (1 =does not apply at all). We averaged the first and second questions to calculate the value for commitment.
Avatar Plausibility: they assessed the avatars’ plausibility with the Virtual Human Plausibility Questionnaire (VHPQ) [1]. The measure consists of two dimensions: (1) The avatar’s appearance and behavior plausibility (ABP) and (2) the avatar’s match to the VE (MVE). All 11 questions were rated on a 7-point Likert scale (7 = highest plausibility).","### 1. Participants
- A total of 72 bachelor students from the University of Würzburg were recruited.
- After applying exclusion criteria, 59 valid data sets were analyzed.
- Participants' ages ranged from 19 to 27 years (M = 22.14, SD = 2.05), with 33 females (56%) and 26 males (44%).
- Participants had varying levels of VR experience: three had no experience, 48 had experienced VR between 1 and 10 times, and eight had more than ten experiences

### 2. Study Design
- The study utilized a 2 × 2 between-subjects design with two independent variables: avatar type (sport/business) and environment style (sport/business).
- Participants were assigned to one of four conditions in a counterbalanced manner

### 3. Procedure
- The experimental procedure lasted approximately 55 minutes.
- Participants provided informed consent and completed pre-questionnaires on demographic data, physical activity, and simulator sickness.
- Body height, weight, and interpupillary distance were measured.
- Participants were educated on using the VR equipment and could practice grasping virtual objects before the main tasks

### 4. Task
- Participants performed three lightweight exercises in VR:
  1. Knee Lifting: Alternately lifting each knee to hip level.
  2. Arm Lifting: Lifting arms sideways to shoulder height.
  3. Weight Lifting: Performing arm lifting with a virtual dumbbell.
- Each exercise had a two-minute time frame, and participants could stop at any time if they felt exhausted

### 5. Metrics Collected
- Behavioral Metrics: 
  - Number of repetitions and execution time for the knee and arm lifting exercises.
  - Weight selection for the weight lifting exercise.
- Self-Reported Metrics: 
  - Subjective measures including commitment, enjoyment, and future motivation assessed through questionnaires.
  - VR-related qualia such as avatar plausibility, sense of embodiment (SoE), and spatial presenc","Tromsø Study Physical Activity,  IPQ (Igroup Presence Questionnaire),  SSQ (Simulator Sickness Questionnaire),  VEQ,  VHPQ",,,"Completion Time,  exercise performance","Questionnaires,  Performance"
ID154,The Impact of Olfactory and Wind Stimuli on 360 Videos Using Head-mounted Displays,"Narciso  David,  Melo  Miguel,  Vasconcelos-Raposo  José,  Bessa  Maximino",,10.1145/3380903,2020,Transactions on Applied Perception,"Mulsemedia, VR",Content & System Design,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"Its main goal is to study the effect of additional sensory (tactile and olfactory) cues besides audio and video on participants’s sense of presence and cybersickness while watching a 360 video using an IVR setup.

The main outcomes of the study are:
- The addition of olfactory stimuli significantly increases the sense of presence in immersive virtual environments.
- Tactile stimuli (simulated wind) do not significantly affect presence or cybersickness.
- The study concludes that multisensory cues do not influence cybersickness, suggesting a positive outcome for user experience in XR","1) Participants
• Total: 48 participants (30 male, 18 female)
• Age Range: 18–51 years old (Mean = 24.50, SD = 6.624)
• Recruitment: University students and researchers

2) Study Design
• Type: Quasi-experimental, cross-sectional study with a between-subjects design
• Conditions: Three groups, each experiencing a different version of a 360 video VR scenario:
    1. Control group – 360 video with visual and auditory stimuli only
    2. Olfactory condition – 360 video with added basil smell
    3. Tactile condition – 360 video with simulated wind (air hose system)
• Independent Variable: Sensory stimuli (visual/audio-only, olfactory, tactile)
• Dependent Variables:
    ◦ Presence (Measured via IPQ)
    ◦ Cybersickness (Measured via SSQ)

3) Procedure
1. Participants received a briefing about the experiment and were instructed not to walk during the VR experience.
2. Participants signed a consent form and completed a sociodemographic questionnaire.
3. The HMD (Oculus Rift DK2) and noise-canceling headphones were placed on the participant.
4. For the olfactory condition, a basil scent was released five times at fixed intervals (every ~30 sec).
5. For the tactile condition, air bursts simulating wind were delivered at varying intensities and durations.
6. Participants watched the 360 video for 3 minutes while standing.
7. After removing the HMD, participants completed the presence (IPQ) and cybersickness (SSQ) questionnaires.

4) Task
• Task: Passive observation of a pre-recorded 360 video of an outdoor urban scene (Praça da Batalha, Porto, Portugal).
• Duration: 3 minutes
• Interaction: None (participants could rotate their heads but could not move in the environment).

5) Metrics Collected
• Questionnaires:
    ◦ IPQ (Igroup Presence Questionnaire) – Measures spatial presence, realness, involvement, and overall presence.
    ◦ SSQ (Simulator Sickness Questionnaire) – Measures nausea, oculomotor discomfort, disorientation, and overall cybersickness.
• Other Recorded Variables:
    ◦ Wind intensity & duration (for tactile condition)
    ◦ Olfactory exposure frequency (for olfactory condition)

Key Findings from the Experiment
• Olfactory stimulation significantly increased presence (spatial & overall).
• Tactile stimulation (wind) had no significant impact on presence.
• Neither olfactory nor tactile stimuli affected cybersickness.","IPQ (Igroup Presence Questionnaire),  SSQ (Simulator Sickness Questionnaire)",,,,Questionnaires
ID155,The Influence of Avatar Representation on Interpersonal Communication in Virtual Social Environments,"Aseeri  Sahar,  Interrante  Victoria",,10.1109/TVCG.2021.3067783,2021,Transactions on Visualization and Computer Graphics,VR,Embodiment Avatars & Social Presence,Behavioural Dynamics & Exploration.,"Human-Computer Interaction (HCI), Telecommunications and Collaboration","
- Participants reported higher levels of trust and attentional focus on facial expressions in the Real Avatar condition compared to the other conditions.
- A significant majority of participants preferred the Real Avatar for its effectiveness in facilitating communication.
- The study provides insights into how avatar representation can enhance interpersonal communication in immersive environments

Virtual hand representation affects motor performance
• The study found that different hand representations (e.g., realistic vs. abstract hands) significantly impact user performance in VR tasks.
• Users performed better when using more realistic hand models, as they provided more intuitive control and feedback.Impact on user experience and interaction accuracy
• Realistic hands improved task accuracy and reduced movement errors compared to abstract representations.
• However, abstract hand models may still be useful in certain applications, such as stylized or game-like VR environments.Presence and engagement were influenced by hand realism
• More realistic virtual hands increased immersion and user engagement in VR experiences.
• Users felt a stronger sense of embodiment and presence when the virtual hands closely resembled their real hands.Findings contribute to VR interface design
• The study’s results provide insights for designing VR interfaces that optimize motor performance, accuracy, and user experience.
• The findings suggest that customizing hand representation based on the task type could improve user interactions in VR.

Mutual Gaze: When head orientation was directed toward their partner’s head position.
Unique Words Count: fluency. A higher unique word count was associated with more fluent communication.
Attention to Behavioural Cues: Observed attention to nonverbal cues (such as gesture, posture, and facial expressions).","### 1. Participants
- Number: 36 participants (16 females, 20 males)
- Age Range: 18 to 27 years (Mean = 20.94, SD = 2.67)
- Recruitment: Graduate and undergraduate students recruited via flyers
- Compensation: Each participant received a $30 gift card for participation

### 2. Study Design
- Type: Within-subjects design
- Conditions: Three avatar types (No Avatar, Scanned Avatar, Real Avatar) and three tasks (Conversation Cards, Survival Items, Charades)
- Counterbalancing: Each participant experienced each avatar and task exactly once, with the assignment of avatar to task counterbalanced using a 3x3 Latin Square

### 3. Procedure
- Participants underwent screening tests for visual acuity and stereo vision, signed a consent form, and completed a demographic survey.
- They filled out the Eysenck Personality Inventory (EPI) to assess extroversion/introversion traits.
- Participants prepared questions for the Conversation Cards task and words for the Charades task.
- The experiment lasted approximately 1.5 hours, including 20 minutes for each condition and 10-minute breaks in between

### 4. Task
- Conversation Cards: Participants shared personal information by answering general life questions.
- Survival Items: Participants chose five items from a set to survive a given scenario, with the experimenter trying to persuade them to change their choices.
- Charades: Participants acted out verbs while their partner guessed the actions

### 5. Metrics Collected
- Subjective Measures: Surveys on social presence, interpersonal trust, communication satisfaction, and attention to behavioral cues (using a 7-point Likert scale).
- Objective Measures:
  - Mutual Gaze: Percentage of time spent attempting to engage in mutual gaze with the experimenter.
  - Unique Words: Count of distinct words spoken during conversations","Behavioural Cues,  Social Presence","Head Analysis,  Joint Attention Behaviour,  Communication Analysis",,,"Questionnaires,  Behavioural"
ID156,The Potential Disconnect between Time Perception and Immersion: Effects of Music on VR Player Experience,"Rogers  Katja,  Milo  Maximilian,  Weber  Michael,  Nacke  Lennart E.",,10.1145/3410404.3414246,2020,Symposium on Computer-Human Interaction in Play,VR,User states: Cognitive & Affective Experience,Content & System Design.,Entertainment and Gaming,"The paper discusses how the presence of music affects time perception in XR, indicating that music can lead to a significant underestimation of time spent in VR.

### Main Outcomes
The main outcomes of the study indicate that:
- Music significantly affects time perception in VR, leading to greater underestimation of time spent.
- There is no significant effect of music on immersion or other psychometric measures of player experience.
- The findings suggest a need for further exploration of how music can be intentionally used in VR game design to influence player experience and time perception

We used the self-assessment manikin (SAM) by Bradley and Lang [5] to measure affective state in the form of arousal, valence, and dominance on 7-point pictorial scales.","### 1. Participants
- Total Participants: 64
- Median Age: 22 years (IQR = 20–25)
- Gender Distribution: Balanced across two sound conditions (1 non-binary, 12 female, 19 male per group

### 2. Study Design
- Design Type: Between-subjects design
- Independent Variable: Presence of music (with-music vs. no-music)
- Duration of Study: Approximately 30 minutes per session

### 3. Procedure
- Participants provided informed consent and completed a brief VR tutorial on game controls and mechanics.
- After the tutorial, participants filled out a demographic questionnaire to assess prior VR experience.
- Participants then played the VR game without being informed that they would later estimate the duration of play.
- After gameplay, participants completed a post-game questionnaire assessing their emotional state and level of immersion

### 4. Task
- Game Type: A bow-and-arrow tower defense VR game.
- Gameplay Duration: Approximately 4.5 minutes per playthrough.
- Game Mechanics: Players defended a portal against waves of enemies (orcs and pegators) using a bow and arrows. An enhancer mechanic appeared at intervals, allowing players to power up their arrows","IEQ (Immersion Experience Questionnaire),  PXI,  SAM (Self-Assessment Manikin)",,,"Retrospective Time Estimation,  Gameplay Performance","Questionnaires,  Performance"
ID157,"The Relationship Between Cybersickness, Sense of Presence, and the Users’ Expectancy and Perceived Similarity Between Virtual and Real Places","Magalhaes  Mariana,  Melo  Miguel,  Bessa  Maximino,  Coelho  Antonio Fernando",,10.1109/ACCESS.2021.3084863,2021,IEEE Access,VR,User states: Cognitive & Affective Experience,,Tourism and Cultural Heritage,"This paper investigated whether users who feel stronger presence or cybersickness during a virtual tour subsequently judge the real site as more similar to the virtual one and feel their expectations were met. Forty-three adults first watched a 360-video on a laptop, then through an HMD, and later visited both real locations. After each virtual session they completed the IPQ (presence) and SSQ (cybersickness); after the real visits they rated perceived similarity and expectation fulfilment. In the laptop condition, higher presence—especially the realism subscale—was linked to greater perceived similarity and stronger fulfilment of expectations, while cybersickness showed no relation. In the HMD condition the pattern flipped: higher cybersickness (nausea, oculomotor discomfort, disorientation) was positively related to similarity and expectations, and presence showed no correlation. The authors conclude that immersive VR can still boost destination credibility even when it induces mild discomfort, whereas presence is the key driver in low-immersion desktop tours.","1) Participants
• Number: 45 participants (2 later removed as outliers, final N = 43)
• Gender: 21 male, 24 female
• Age: 18 to 79 years (Mean = 42.27, SD = 17.567)
• Vision: All had normal or corrected-to-normal vision
• Recruitment: Convenience sampling (volunteers)
• Language: Portuguese (all instruments adapted accordingly)
2) Study Design
• Design: Within-subjects experimental design
• Conditions:
    ◦ Non-immersive VR using a laptop
    ◦ Immersive VR using an Oculus Rift DK2
• Counterbalancing: Two groups saw different locations in immersive vs. non-immersive settings to prevent boredom/order effects
• Environments: Two real-world tourism locations in Portugal
    ◦ São Leonardo de Galafura
    ◦ Capela Nova
3) Procedure
1. Participants received instructions and gave informed consent.
2. Completed a demographic questionnaire.
3. Non-immersive VR experience: Viewed a 360° video on a laptop.
    ◦ Post-experience: Completed SSQ and IPQ.
4. Immersive VR experience: Viewed another 360° video using Oculus Rift DK2.
    ◦ Post-experience: Completed SSQ and IPQ again.
5. Participants visited both real locations physically.
6. After each visit, completed a custom questionnaire assessing:
    ◦ Perceived similarity between virtual and real experiences
    ◦ Whether their expectations were fulfilled
4) Task
• VR task: Explore a 360° video of a tourism location
    ◦ Non-immersive: Used a laptop and mouse to control the view
    ◦ Immersive: Used Oculus Rift DK2 headset for immersive viewing
    ◦ Maximum duration: 2 minutes per VR session
• Post-VR task: Complete self-report questionnaires
• Real-world task: Visit the actual location and compare the real vs. virtual experiences
5) Metrics Collected
a) Questionnaires (Self-Report)
• Sense of Presence: Igroup Presence Questionnaire (IPQ-PT)
• Cybersickness: Simulator Sickness Questionnaire (SSQ - Portuguese version)
• Perceived Similarity & Expectation Fulfillment: Custom-developed 10-item questionnaire
    ◦ 6 items on perceived similarity
    ◦ 4 items on user expectations
b) Internal Consistency Testing
• Cronbach's alpha:
    ◦ Similarity scale: α = 0.734
    ◦ Expectancy scale: α = 0.705","Custom made,  IPQ (Igroup Presence Questionnaire),  SSQ (Simulator Sickness Questionnaire)",,,,Questionnaires
ID158,The Role of Haptic Feedback and Gamification in Virtual Museum Systems,"Ceccacci  Silvia,  Generosi  Andrea,  Leopardi  Alma,  Mengoni  Maura,  Mandorli,  Ferruccio",,10.1145/3453074,2021,Journal of Computing and Cultural Heritage,VR,Interaction Techniques & input Modalities,,Tourism and Cultural Heritage,"This paper investigated whether force-feedback and a light game layer improve user experience in virtual museums. Sixty-five participants first compared the tactile realism of a 3-D-printed statuette manipulated with a stylus to its virtual model handled with a six-DOF haptic device, rating the two as closely similar in both look and feel. A second, between-groups study then had visitors explore artifacts with either a mouse-driven viewer, the same haptic viewer, or a gamified “dig-and-reveal” haptic game. The haptic interfaces matched the mouse for perceived usability (SUS ≈ 76–82) but scored far higher on AttrakDiff hedonic quality and attractiveness; users also spent roughly twice as long with the haptic systems, signalling greater engagement. Adding the game did not further raise usability or time-on-task, suggesting that the novelty and tactile richness of force feedback itself—not simple game elements—drove the UX gain.","1. Participants
• Comparative Study: 20 participants (9 females, 11 males), aged 16–24 (mean age 18).
• Parallel Study: 45 participants (24 males, 21 females), aged 16–23 (mean age 19), divided into three groups (A, B, C).
• All participants were familiar with mouse-based computer interaction but had no prior experience with haptic devices.
2. Study Design
• Comparative Study: Single-group design where participants interacted with both:
    ◦ A 3D printed replica of an artifact (Venus of Frasassi) using a stylus.
    ◦ A virtual replica via a haptic interface (Omega 6 device) and PC monitor.
• Parallel Study: Three-group design comparing:
    ◦ Group A: Traditional mouse-based 3D rendering interface.
    ◦ Group B: Haptic interface (non-gamified).
    ◦ Group C: Gamified haptic interface (nonogram puzzle game).
3. Procedure
• Comparative Study:
    1. Training on stylus-based interaction with the 3D printed replica.
    2. Training on haptic interface interaction with the virtual model.
    3. Free exploration of both physical and virtual models (switching allowed).
    4. Participants rated similarity between physical and virtual experiences on a 7-point Likert scale.
• Parallel Study:
    1. Group-specific training (mouse, haptic, or gamified haptic interface).
    2. Free exploration (or gameplay for Group C) of three virtual artifacts (Venus of Frasassi, Augusto Capite Velato, Pyx).
    3. Time spent interacting was recorded.
    4. Post-experiment questionnaires:
        ▪ System Usability Scale (SUS) for usability.
        ▪ AttrakDiff2 for UX (pragmatic/hedonic quality, attractiveness).
4. Task
• Comparative Study: Compare tactile/visual experiences between physical and virtual artifacts.
• Parallel Study:
    ◦ Groups A & B: Freely explore virtual artifacts.
    ◦ Group C: Complete a gamified task (sculpting a stone block to reveal an artifact) before exploration.
5. Metrics Collected
• Comparative Study:
    ◦ Self-reported similarity (7-point Likert scale) between physical and virtual experiences (haptic/visual).
• Parallel Study:
    ◦ Behavioral: Time spent interacting with the interface.
    ◦ Questionnaires:
        ▪ SUS scores (usability).
        ▪ AttrakDiff2 scores (Pragmatic Quality, Hedonic Quality-Stimulation/Identity, Attractiveness).
    ◦ Qualitative feedback from participants.
Key Findings:
• Haptic interfaces scored similarly to mouse-based interfaces in usability (SUS) but significantly increased engagement (longer interaction times).
• Gamification did not significantly enhance engagement over haptics alone.
• AttrakDiff2 showed haptic interfaces outperformed mouse-based ones in hedonic quality (novelty, stimulation) and attractiveness.
This study provides insights into the role of haptic feedback in XR user experience, though it notes limitations (e.g., gamification design, participant demographics).","AttrakDiff2,  System Usability Scale (SUS)",Interaction Time,,,"Questionnaires,  Behavioural"
ID159,The Role of Physiological Responses in a VR-Based Sound Localization Task,"Moraes  Adrielle N.,  Flynn  Ronan,  Hines  Andrew,  Murray  Niall",,10.1109/ACCESS.2021.3108446,2021,IEEE Access,VR,Interaction Techniques & input Modalities,User states: Cognitive & Affective Experience.,Education and Training,"1. Physiological Metrics as Indicators: The study found that physiological metrics, such as electrodermal activity (EDA) and pupillary response, are effective indicators of cognitive load and immersion during sound localization tasks in VR. As the task progressed, both EDA values and pupillary responses increased, suggesting that the task was challenging and required higher mental demand

2. Impact of Interaction Methods: The research compared two interaction methods (pointer vs. gaze) and found that the pointer group performed better in terms of the number of correct selections. This indicates that the method of interaction with the virtual environment significantly affects user performance during sound localization tasks

General Implications for XR Applications: The findings emphasize the importance of user interaction methods and physiological responses in designing VR applications aimed at improving auditory localization abilities. The study suggests that future applications should consider these factors to enhance user experience and performance","### 1. Participants
- A total of 20 participants took part in the experiment, with 10 in each interaction group (Gaze Pointing and Pointer Pointing). 

### 2. Study Design
- The study employed a within-subjects design where participants were assigned to one of two interaction methods: Gaze Pointing (GP) or Pointer Pointing (PP). The experiment consisted of a tutorial phase followed by three main testing phases that varied in complexity

### 3. Procedure
- The experiment began with a screening phase to check for hearing and visual impairments. Participants then wore an E4 Empatica wristband to collect physiological data during the experiment. After a 5-minute baseline data collection, participants watched tutorial videos on how to interact with the VR environment

- The tutorial phase involved selecting spheres in the environment to familiarize participants with the interaction method. This was followed by the sound localization task, which included three phases: 
  1. Target-only: Participants localized a sound source without distractions.
  2. One distractor: A competing sound was introduced at the same location.
  3. Two distractors: Two competing sounds were presented at ±90 degrees from the target

### 4. Task
- The primary task was to localize sound sources in a VR environment. Participants were instructed to select the location of the sound source as quickly as possible, with a total of 72 selection attempts (24 for each phase). The task aimed to assess the participants' ability to identify the correct location of auditory stimuli

",NASA-TLX,Head Analysis,"EDA / GSR (Skin Conductance),  Pupil Analysis","Accuracy,  Completion Time","Questionnaires,  Behavioural,  Physiological,  Performance"
ID160,The Stare-in-the-Crowd Effect in Virtual Reality,"Raimbaud  Pierre,  Jovane  Alberto,  Zibrek  Katja,  Pacchierotti  Claudio,  Christie  Marc,  Hoyet  Ludovic,  Pettre  Julien,  Olivier  Anne-Helene",,10.1109/VR51125.2022.00047,2022,Conference on Virtual Reality and 3D User Interfaces,VR,Behavioural Dynamics & Exploration,Embodiment Avatars & Social Presence.,Human-Computer Interaction (HCI),"This paper investigated how people’s eye movements change when virtual crowd members stare at them in VR. Thirty users viewed an audience of seated agents whose gaze was either always averted, always direct, or switched from averted to direct (and vice-versa) once looked at. Eye-tracking showed reliably longer dwell times, more fixations and longer first-fixation durations on agents whose gaze was direct, confirming the stare-in-the-crowd effect in a head-mounted display; the effect diminished for agents positioned on the far left, hinting at a natural leftward viewing bias. Dynamic gaze shifts (being caught staring / catching someone stare) produced intermediate attention patterns. Importantly, higher social-anxiety scores correlated with shorter dwell times on staring agents, indicating that anxious users avoid mutual gaze even in VR crowds.","1) Participants
• Number: 30 human users.
• Details: The participants' social anxiety levels were measured using a post-experiment questionnaire (Liebowitz Social Anxiety Scale). The sample was not gender-balanced, which was noted as a limitation.
2) Study Design
• Type: Within-subject experiment.
• Conditions: Four gaze behaviors of virtual agents in a virtual crowd:
    ◦ Averted (A): No agent looks at the user.
    ◦ Directed (D): One agent (""active agent"") stares at the user throughout the task.
    ◦ Averted-then-Directed (AD): The active agent starts with an averted gaze and switches to directed when the user looks at them.
    ◦ Directed-then-Averted (DA): The active agent starts with a directed gaze and switches to averted when the user looks at them.
• Trials: 72 trials per participant (4 gaze behaviors × 9 agent positions × 2 repetitions).
3) Procedure
1. Preparation: Participants were seated, equipped with a FOVE VR headset (with eye-tracking), and calibrated.
2. Training Phase: Participants familiarized themselves with the virtual environment, where agents had neutral gazes.
3. Main Experiment: Participants completed 72 trials (divided into 3 blocks of 24 trials each). Each trial lasted 16 seconds, with a 3-second break between trials.
4. Post-Experiment: Participants filled out a social anxiety questionnaire and provided demographic information.
4) Task
• Primary Task: Observe a virtual audience without specific instructions to search for gaze directions (free-viewing task).
• Context: The virtual scene simulated a classroom with 11 agents seated as an audience and a silent speaker. Participants were asked to face the audience and avoid looking at the speaker after the training phase.
5) Metrics Collected
• Eye-Tracking Metrics:
    ◦ Dwell time: Total time spent looking at the active agent.
    ◦ Fixation count: Number of fixations on the active agent.
    ◦ First fixation time/duration: Time and length of the first fixation on the active agent.
    ◦ Second fixation time/duration: Time and length of the second fixation.
• Questionnaire:
    ◦ Social anxiety score: Measured using the Liebowitz Social Anxiety Scale (range: 0–144).
Additional Notes
• Analysis: The study compared gaze behaviors (e.g., A vs. D, A vs. AD) and examined correlations between gaze metrics and social anxiety scores.
• Key Findings:
    ◦ The stare-in-the-crowd effect was confirmed in VR (longer dwell time/fixations on directed gazes).
    ◦ Social anxiety negatively correlated with dwell time on directed gazes, especially in dynamic conditions (e.g., AD).
This summary captures the core experimental setup and methodology used to investigate the stare-in-the-crowd effect in VR.",Liebowitz Social Anxiety Scale,"Gaze Analysis,  Fixation Analysis",,,"Questionnaires,  Behavioural"
ID161,The Stare-in-the-Crowd Effect When Navigating a Crowd in Virtual Reality,"Raimbaud  Pierre,  Jovane  Alberto,  Zibrek  Katja,  Pacchierotti  Claudio,  Christie  Marc,  Hoyet  Ludovic,  Pettré  Julien,  Olivier  Anne-Hélène",,10.1145/3605495.3605796,2023,Symposium on Applied Perception,VR,Behavioural Dynamics & Exploration,Embodiment Avatars & Social Presence.,Human-Computer Interaction (HCI),"The study focuses on how users interact with virtual agents in a virtual reality environment, specifically examining the effects of gaze behavior on navigation and social interactions. 
1. Stare-in-the-Crowd Effect: The study confirmed that the stare-in-the-crowd effect persists in dynamic situations, where users navigate through both idle and moving crowds of virtual agents. Participants spent more time looking at agents with directed gazes compared to those with averted gazes, indicating that gaze direction significantly influences attention

2. Impact of Social Anxiety: The results showed that social anxiety negatively affects gaze interaction time. Participants with higher levels of social anxiety exhibited reduced attention to the gazes of virtual agents, which also influenced their proximity behaviors when interacting with these agents

3. Locomotion Behaviors: Interestingly, the gaze of virtual agents did not lead to significant changes in the participants' locomotion behaviors. While the study anticipated that directed gazes would affect path decisions and local navigation behaviors, the results indicated that the predictable nature of the crowd's movement limited the impact of gaze on navigation","### 1. Participants
- Number of Participants: 21 (5 females, 16 males)
- Age: Average age of 31.8 years (SD: 6.9)
- VR Experience: Average score of 3.2 (on a scale from 1 to 5)
- Computer Games Experience: Average score of 3.7 (on a scale from 1 to 5)
- Vision: Normal or corrected-to-normal vision

### 2. Study Design
- Type of Study: Within-subjects design with multiple gaze behavior conditions.
- Conditions: Four gaze behavior conditions were tested: 
  - Averted gaze (A)
  - Directed gaze (D)
  - Averted-then-directed gaze (AD)
  - Directed-then-averted gaze (DA)
- Crowd Types: Two types of crowds were created: idle (standing) and moving (walking in the opposite direction)

### 3. Procedure
- Participants were informed about the study and signed consent forms.
- They were equipped with a backpack computer, HTC Vive Pro Eye VR headset, controllers, and trackers.
- Calibration of the eye-tracking system and participant height was performed.
- A training phase included 6 trials to familiarize participants with the virtual environment.
- Participants completed 64 trials under various gaze behavior conditions, with breaks to minimize fatigue

## 4. Task
- The primary task was to navigate a populated virtual street and reach a tree placed 8 meters away while following natural behaviors in social situations. Participants were not informed about the gaze behaviors of the virtual agents","Social Anxiety,  Social Presence","Gaze Analysis,  Movement Trajectories",,,"Questionnaires,  Behavioural"
ID162,The User Experience of Journeys in the Realm of Augmented Reality Television,"Pamparău  Cristian,  Vatavu  Radu-Daniel",,10.1145/3505284.3529969,2022,Conference on Interactive Media Experiences,"AR, AR Television",Behavioural Dynamics & Exploration,User states: Cognitive & Affective Experience.,Entertainment and Gaming,"User Experience (UX) Characterization: The study reveals that the user experience of Augmented Reality Television (ARTV) is characterized by:
   - High Perceived Usability: The average System Usability Scale (SUS) score was 80.2, indicating a high level of usability 
   - Low to Medium Workload: The perceived task load was overall low, with specific subscales indicating low frustration and moderate mental demand
   - High Captivation and Comprehension: Participants reported high levels of captivation (M=72.7) and comprehension (M=71.9) during their ARTV experiences
   - Moderate Transportation and Dissociation: Users experienced a moderate feeling of being transported into the augmented world, indicating a balance between immersion and awareness of their physical surroundings

Lean-Back and Lean-Forward Behaviour: leaning back for passive viewing or leaning forward to interact with or explore ARTV content. Recorded through direct observation 

Walking and Spatial Interaction: within the 4x4 metre experimental space as they explored augmented content using HoloLens tracking data.
    
Interaction with Content: exploring scenes or focusing on specific content areas, by direct observation and recorded interactions using system logs and researcher notes.

Number of Transitions: between different ARTV scenarios.

Transition Behaviour: patterns of transitions, such as linear transitions (progressing sequentially through scenarios) or primary transitions (frequently returning to a preferred scenario).","### 1. Participants
- Number of Participants: 14 volunteers (9 men, 5 women)
- Age Range: 19 to 34 years (M=25.9, SD=5.4)
- Daily TV Watching Time: Self-reported average between 1 and 7 hours (M=3.1, SD=2.0)

### 2. Study Design
- Design Type: Within-subjects design
- Independent Variable: ARTV-Scenario, with five conditions representing varying levels of augmentation from no augmentation (control condition) to fully immersive ARTV

### 3. Procedure
- Participants signed a consent form and completed a demographic questionnaire.
- They first watched a 90-second movie on a conventional TV screen (control condition).
- Afterward, participants used a HoloLens to rewatch the movie with augmented content according to different ARTV scenarios, performing transitions between these scenarios at will

### 4. Task
- The task involved watching the augmented movie and engaging with the content by transitioning between different ARTV scenarios, which included:
1. Control condition (conventional TV)
2. Virtual-screen (virtual TV)
3. Off-screen-augmentation (content around the virtual TV)
4. Off-screen-augmentation-more (more content displayed in the room)
5. Augmented-living-room (fully immersive AR content)

### 5. Metrics Collected
- User Experience Metrics:
  - Usability: Measured using the System Usability Scale (SUS).
  - Task Load: Measured with the NASA TLX.
  - Immersion: Assessed using the Immersive Experience Questionnaire for Film and TV.
  - Presence: Measured with Witmer and Singer’s Presence Questionnaire (PQ v2.0).","Immersive Experience Questionnaire for Film and TV,  NASA-TLX,  Presence – General,  System Usability Scale (SUS)","Body Analysis,  Number of Transitions between ARTV Scenarios,  Movement Trajectories",,,"Questionnaires,  Behavioural"
ID163,There Is More to Avatars Than Visuals: Investigating Combinations of Visual and Auditory User Representations for Remote Collaboration in Augmented Reality,"Fink  Daniel Immanuel,  Skowronski  Moritz,  Zagermann  Johannes,  Reinschluessel  Anke Verena,  Reiterer  Harald,  Feuchtner  Tiare",,10.1145/3698148,2024,Proceedings of the ACM on Human-Computer Interaction,AR,Content & System Design,User states: Cognitive & Affective Experience.,Telecommunications and Collaboration,"This paper investigates how visual richness of avatars (Simple vs. Rich) and auditory representation (Mono vs. Spatial Audio) jointly influence user experience, social presence, attention, and performance in symmetric remote AR collaboration. Forty-eight participants (24 dyads) completed a collaborative sticky-note sorting task in a shared AR workspace while interacting with avatar variations and audio conditions.
Results show that Spatial Audio improves task efficiency in parallel tasks (e.g., responding to smartphone interruptions) and enhances spatial awareness, while Rich Avatars significantly improve hedonic user experience and increase visual attention to the avatar's head. Simple avatars are perceived as less distracting and may benefit performance-oriented scenarios. Social presence was high in all conditions, but richer avatars enhanced the subjective feeling of interpersonal connection. The study proposes design guidelines for selecting avatar/audio combinations depending on collaboration goals. The work offers strong, generalizable insights into multimodal representation and social UX in AR.","The study involved 48 participants forming 24 dyads who previously knew each other. Each dyad collaborated remotely from two matched physical labs, wearing HoloLens 2 devices. A within-subjects 2×2 design compared:
• Visual representation: Simple Avatar vs. Rich Avatar
• Auditory representation: Mono Audio vs. Spatial Audio
Participants performed a collaborative sticky-note classification task in AR, alongside an interruption task requiring rapid responses to ringing smartphones. Each session included a tutorial, calibration, four experimental conditions in Latin-square order, condition-specific questionnaires (UEQ-S, NASA-TLX, Networked Minds Social Presence Inventory), and a semi-structured interview.
Behavioural measures were logged automatically (gaze, proxemics, completion time, call response time).","NASA-TLX,  Social Presence,  UEQ (User Experience Questionnaire),  Semi-structured Interviews","Interpersonal Distance,  Gaze Analysis",,Completion Time,"Questionnaires,  Behavioural,  Performance"
ID164,Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms,"Saffo  David,  Batch  Andrea,  Dunne  Cody,  Elmqvist  Niklas",,10.1145/3544548.3581093,2023,Conference on Human Factors in Computing Systems,"Cross-Virtuality, VR, desktop users",Embodiment Avatars & Social Presence,Visualization Techniques.,"Human-Computer Interaction (HCI), Telecommunications and Collaboration","The study focused on understanding how different levels of group awareness (interaction vs. perspective sharing) impact collaborative behavior and effectiveness in cross-virtuality analytics (XVA).

The study examines group awareness techniques in cross-virtuality collaboration (VR + desktop users) and identifies the following key outcomes:
1. Perspective Sharing Enhances Collaboration
    ◦ Perspective-sharing techniques (allowing users to see through their teammate’s eyes) improved coordination and collaborative coupling compared to interaction-sharing techniques.
    ◦ VR users were more engaged when they could see what their desktop partner was viewing, leading to more effective decision-making.
2. Leadership Emerges Based on Awareness Features
    ◦ Perspective-sharing led to clearer leadership roles, with one participant taking initiative in guiding the collaboration.
    ◦ When using interaction sharing (where only actions were visible), leadership roles were less defined.
3. Cross-Virtuality Awareness Affects Engagement & Communication
    ◦ Awareness techniques impacted engagement, with VR users feeling more immersed when perspective-sharing was available.
    ◦ Communication styles adapted depending on how much information was shared about the partner’s actions and viewpoint.
4. Abstract vs. Natural Visualization Contexts Influence Collaboration
    ◦ Participants collaborated differently when working with parallel coordinates (abstract visualization) vs. pitch trajectories (natural spatial mapping).
    ◦ The type of visualization affected how awareness techniques were used and how users structured their workflow.
Conclusion
The study demonstrates that perspective-sharing significantly improves collaboration, engagement, and leadership emergence in cross-virtuality teamwork. The findings suggest that awareness techniques can be optimized to enhance user experience and coordination in XR-based collaborative environments.","1) Participants
6 pairs of participants (12 total), each pair consisting of one VR user and one desktop user. Participants were recruited via Prolific and screened for knowledge of baseball and VR headset ownership.
2) Study Design
Synchronous remote user study with a between-subjects factor (two group awareness techniques: L3 - Interaction Sharing and L4 - Perspective Sharing) and a within-subjects factor (two visualization contexts: Abstract Visualization - Parallel Coordinates & Natural Spatial Mapping - Pitch Trajectories).
3) Procedure
Participants joined a Zoom call and accessed the VRxD platform for data visualization. Sessions lasted 90 minutes (20 minutes setup, ~35 minutes per visualization condition). The researcher guided participants, observed collaboration, and conducted an exit interview.
4) Task
Participants collaborated to analyze baseball pitching data using the VRxD system. They explored datasets, identified patterns, and summarized their findings in a report template. The two visualization contexts included: - Parallel Coordinates (PC): Clustering and correlating statistics - Pitch Trajectories (PT): Extracting pitch characteristics and summarizing findings.5) Metrics Collected- Behavioral Metrics: Interaction logs (collaborative behaviors, group awareness usage, leadership roles) - Collaborative Coupling Behavior: Tight vs. loose collaboration - Qualitative Data: Exit interviews coded for engagement, communication, and collaboration patterns","NASA-TLX,  System Usability Scale (SUS)","Group Behaviour / Awareness,  Movement Trajectories",,"Accuracy,  Completion Time","Questionnaires,  Behavioural,  Performance"
ID165,Touching Virtual Humans: Haptic Responses Reveal the Emotional Impact of Affective Agents,"Ahmed  Imtiaj,  Harjunen  Ville J.,  Jacucci  Giulio,  Ravaja  Niklas,  Ruotsalo  Tuukka,  Spapé  Michiel M.",,10.1109/TAFFC.2020.3038137,2023,Transactions on Affective Computing,VR,Embodiment Avatars & Social Presence,User states: Cognitive & Affective Experience.,Human-Computer Interaction (HCI),"1. First Study on Emotional Perception and Haptic Responses: The research represents the first investigation into how emotional expressions displayed by virtual agents affect users' haptic responses during interactions in a social VR setting. It demonstrates that the emotional expressions of agents significantly influence users' affective states, as indicated by both self-reports and physiological responses

2. Impact of Emotional Expressions on Haptic Responses: The study found that perceiving the emotional expressions of virtual agents affects users' haptic responses, specifically the duration and intensity of their touch. Negative and high-arousal emotional states elicited longer response durations and higher maximum pressure during haptic interactions compared to positive or low-arousal states

3. Link Between User and Agent Affect: The research highlights that both the users' own emotional states and their perceptions of the agent's emotional states predict the duration and intensity of haptic responses. This indicates a complex interplay between perceived and experienced emotions in shaping haptic interactions

4. Methodological Contributions: The paper proposes and validates a range of user metrics, including self-reported questionnaires, physiological measures (such as electrodermal activity and facial electromyography), and haptic response metrics. This methodological framework can be utilized in future studies to explore affective interactions in XR environments

5. Implications for Affective Computing: The findings suggest that haptic responses can serve as implicit measures of users' emotional experiences in social VR, providing valuable insights for the development of adaptive systems that respond to users' emotions based on haptic interactions","### 1) Participants
- Number: 36 university students (21 female, 15 male)
- Average Age: 29 years (SD = 4.37)
- Criteria: All participants were right-handed and had normal or corrected-to-normal vision. Participation was voluntary, and they received a movie ticket as compensation

### 2) Study Design
- The study involved a within-subjects design where participants engaged in 168 trials divided into two blocks of 84 trials each. Each trial involved interactions with four virtual agents (two male and two female) displaying seven emotional expressions (anger, disgust, fear, happiness, neutral, sadness, surprise

### 3) Procedure
- Participants were equipped with physiological sensors, a head-mounted display (HMD), and headphones. They held a haptic input device while their gaze was tracked using an integrated eye-tracking system. Each trial began with a view of the participant's hand holding the arm of a virtual agent, followed by an emotional expression animation. Participants were instructed to squeeze the haptic device in response to an auditory cue and maintain pressure for 1 second

### 4) Task
- The primary task involved participants squeezing a pressure-sensing tube (representing the agent's arm) in response to the emotional expressions displayed by the virtual agent. They were instructed to apply the same amount of force as they would when touching a real person, regardless of the emotion expressed by the agent

### 5) Metrics Collected
- Self-Report Measures: Participants rated their own affective state (valence and arousal) and the perceived affect of the agent using 5-point Likert scales after each trial.
- Physiological Measures: Data were collected on facial electromyography (fEMG), electrodermal activity (EDA), and electrocardiography (ECG) to assess physiological responses to the emotional expressions.
- Haptic Responses: Metrics included response onset, response duration, and maximum pressure applied during the squeeze",Social Presence,Haptic Response,"ECG,  EDA / GSR (Skin Conductance),  EMG (facial/muscle)",,"Questionnaires,  Behavioural,  Physiological"
ID166,Toward Understanding the Effects of Virtual Character Appearance on Avoidance Movement Behavior,"Mousas  Christos,  Koilias  Alexandros,  Rekabdar  Banafsheh,  Kao  Dominic,  Anastaslou  Dimitris",,10.1109/VR50410.2021.00024,2021,Conference on Virtual Reality and 3D User Interfaces,"6-DoF, VR",Embodiment Avatars & Social Presence,Behavioural Dynamics & Exploration.,Human-Computer Interaction (HCI),"This paper extends the general understanding of avoidance behaviour when interacting with virtual characters. It presents a user study that investigates the effect of virtual character appearance on a user's behaviour and correlates the movement behaviour with self-reported ratings. The study was conducted in a 6-DoF VR setting.

The study found that virtual character appearance significantly influences user avoidance movement behavior in VR. Specifically:
1. Users adjusted their paths when approaching virtual characters, indicating that proxemic behavior (personal space) is affected by visual design elements.
2. More human-like virtual characters led to greater social presence, making users more likely to adjust their navigation patterns compared to abstract or robotic avatars.
3. The results highlight the importance of avatar realism in shaping user behavior in immersive environments, providing insights into how virtual agents affect movement-based interactions in XR.

Avoidance Movement Behaviour: a measure to evaluate a user's reaction to stylised virtual characters. Using a mocap tracking system, this was measured according to three features of avoidance movement:
- Length of the trajectory: The length of the extracted root trajectory between the starting and ending point (goal position), measured in meters.
- Clearance distance: The shortest distance between the participant and the virtual character during the avoidance task, measured in meters.
- Walking speed: The average speed of the participants’ walking motion from the start to the goal position, measured in meters/second. ","1) Participants
The study recruited 30 participants

2) Study DesignWithin-subjects design: 
Each participant experienced different conditions where virtual characters varied in appearance (e.g., human-like, robotic)

3) Procedure
Participants were immersed in a virtual environment using a VR headset and instructed to walk through the space where virtual characters were present. The experiment recorded their movement behavior as they navigated past these virtual agents.

4) Task
Participants walked through a virtual corridor while encountering different virtual characters. The goal was to observe how their path and navigation behavior changed based on the appearance of the virtual characters.

5) Metrics Collected- Behavioral Metrics: Avoidance movement patterns, proxemic distance (path deviation when passing virtual characters). - Questionnaires: Social Presence Questionnaire (SPQ) to assess perceived presence of virtual characters.
","Custom made,  Emotional Reactivity,  IEQ (Immersion Experience Questionnaire),  Presence – General,  Social Presence,  UEQ (User Experience Questionnaire)",Avoidance Behaviour,,,"Questionnaires,  Behavioural"
ID167,Towards a Symmetrical Definition of QoE: An Evaluation of Emotion Semantics in Augmented Reality Training,"Hynes  Eoghan,  Flynn  Ronan,  Lee  Brian,  Murray  Niall",,10.1109/QoMEX58391.2023.10178564,2023,International Conference on Quality of Multimedia Experience,AR,User states: Cognitive & Affective Experience,Behavioural Dynamics & Exploration.,Education and Training,"This paper investigated whether emotion labels such as happy or excited communicate VR quality of experience as reliably as underlying physiological, facial-expression and gaze data during an AR GoCube® training task. Sixty participants learned the cube manipulation either with text-only steps or with a combined text + animated example shown on a HoloLens 2; eye gaze, electrodermal activity, heart rate, skin temperature and lower-face action units were recorded. Text-only training led female participants to complete both practice and recall faster and with fewer errors than the example format, whereas males showed no timing difference. Across the whole sample many significant correlations emerged among physiological signals, facial action units, SAM valence/arousal scores and NASA-TLX workload—but none linked to the open-ended or 2-D emotion terms participants selected. The authors conclude that everyday emotion words are too vague for QoE research and that future work should rely on dimensional scales or directly measurable signals instead of discrete labels like delight and annoyance.","1) Participants
• A total of 60 participants were involved in the study, secured through convenience sampling.
• The participants were gender-balanced, with 30 males and 30 females, and their ages ranged from 19 to 62 years, with a mean age of 32. 
• The sample included participants from 12 different nationalities https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=14418,15209, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=13513,14417.
2) Study Design
• The study utilized a self-paced between-groups design.
• Participants were divided into two independent groups: 
    ◦ Test Group (TG): Used a combined text and interactive animated 3D model instruction format.
    ◦ Control Group (CG): Used a text-only instruction format https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=10204,11167, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=9315,10203.
3) Procedure
• The evaluation consisted of ten distinct phases:
    1. Sampling and Information Sharing
    2. Screening (visual acuity and color perception tests)
    3. Instruction and Calibration
    4. Baseline Measurement
    5. Practice Phase
    6. Training Phase
    7. Waiting Phase
    8. Recall Phase
    9. Transfer Phase (re-taking the mental rotation test)
    10. Questionnaires (emotional state and cognitive load assessments) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=13513,14417, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=4208,4812.
4) Task
• Participants were trained on a GoCube™ manipulation procedure. They were required to follow instructions to rotate the Cube faces in specified directions (90° clockwise or anti-clockwise) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=18644,19499, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=10204,11167.
5) Metrics Collected
• Objective Metrics: 
    ◦ Instruction response times, quantity of errors, and durations were captured in real-time.
    ◦ Baseline and post-experience mental rotation abilities were assessed using the Vandenberg mental rotation test https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=16194,16999, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=33938,34335.
• Implicit Metrics: 
    ◦ Eye gaze features, physiological ratings (skin temperature, blood 
volume pulse, heart rate, interbeat interval, electrodermal activity), 
and facial expressions were recorded using various sensors https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=11948,12717, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=17752,18643.
• Explicit Metrics: 
    ◦ Participants completed an affect questionnaire post-experience, which included:
        ▪ Open-ended emotional state description.
        ▪ Self-Assessment Manikin (SAM) questionnaire.
        ▪ Selection of an emotion label from a 2D emotion space.
        ▪ Likert scale and NASA-TLX questionnaires to assess quality of experience, subjective task load, and cognitive load https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=13513,14417, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/6811f7917cb155bf8bc38be4/?range=22017,22877.
This summary encapsulates the key aspects of the experiments 
conducted, providing a clear overview of the participants, study design,
 procedure, task, and metrics collected.","Likert Scale (5-point) for QoE,  NASA-TLX,  SAM (Self-Assessment Manikin)",Gaze Analysis,"BVP / PPG,  EDA / GSR (Skin Conductance),  Heart Rate,  Facial Expressions,  HRV / IBI",,"Questionnaires,  Behavioural,  Physiological"
ID168,Towards Engaging Intangible Holographic Public Displays,"De Angeli  Daniela,  Frangoudes  Fotos,  Avraam  Savvas,  Neokleous  Kleanthis,  O'Neill  Eamonn",,10.1109/IMET54801.2022.9929788,2022,International Conference on Interactive Media Smart Systems and Emerging Technologies (IMET),AR,Interaction Techniques & input Modalities,,Telecommunications and Collaboration,"This paper investigated whether a touchable 3-D-printed drum or a touch-free Kinect gesture offers the more natural and engaging way to play a holographic drum on a public display. Twenty-three visitors performed 24 rhythm-matching tasks with each input while task accuracy, NASA-TLX workload, meCUE usability, IBM-ASQ satisfaction, Audience-Engagement scores, interview comments and video-coded behaviours were collected. The tangible replica proved more natural: users learned it faster, achieved higher Dynamic-Time-Warping accuracy and reported lower workload and higher usability. The gesture input was technically glitchier, required more help and produced signs of confusion, yet both inputs were rated equally engaging and held visitors’ attention for similar durations. Interviews suggested combining minimal tangible props with mid-air gestures to balance intuitiveness and hygiene for public holographic installations.","1. Participants
• Number of Participants: 23 (13 male, 10 female)
• Age Distribution: 52.2% aged 25-34, 39.1% aged 35-44, and 8.7% aged 45-54 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=13352,13892 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=28856,29216.
2. Study Design
• Design Type: Within-subjects design, where each 
participant interacted with both tangible and intangible inputs in a 
counter-balanced order https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=16888,17715, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=25818,26601.
3. Procedure
• Participants were invited to a large open space at CYENS – Centre of Excellence.
• A pre-session survey captured demographic data.
• Participants interacted with a holographic display (HoD) using 
either tangible or intangible inputs, followed by a series of tasks.
• After completing the tasks, participants filled out a post-session survey and participated in a semi-structured interview https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=16888,17715, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=13352,13892.
4. Task
• Participants were tasked with playing a holographic drum by 
following rhythmic patterns of pauses and beats presented visually above
 the drum.
• They started with 4 familiarization tasks and then performed 20 predefined tasks, which varied in complexity https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=16181,16887, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=18574,19411.
5. Metrics Collected
• Usability: Measured using the usability subscale from the meCUE questionnaire.
• Task Accuracy: Assessed using the Dynamic Time Warping (DTW) algorithm to compare user performance against expected patterns.
• Perceived Workload: Evaluated using the NASA Task Load Index, which measures mental, physical, and temporal demand.
• Engagement and Satisfaction: Measured using the Audience Engagement questionnaire and the IBM After-Scenario Questionnaire (ASQ) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=19412,20223 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67ea95e2bb7049c45c28dede/?range=20224,21112.
This summary encapsulates the key aspects of the experimental study, 
providing a clear overview of the participants, design, procedure, 
tasks, and metrics collected.","Audience Engagement,  IBM AfterScenario,  NASA-TLX,  meCUE",,,"Accuracy,  Completion Time","Questionnaires,  Performance"
ID169,Understanding and Designing Avatar Biosignal Visualizations for Social Virtual Reality Entertainment,"Lee  Sueyoon,  El Ali  Abdallah,  Wijntjes  Maarten,  Cesar  Pablo",,10.1145/3491102.3517451,2022,Conference on Human Factors in Computing Systems,"VR, social VR",Embodiment Avatars & Social Presence,Visualization Techniques.,Entertainment and Gaming,"Semi-structured interviews to gather insights on overall experience, biosignal interpretations, and other related topics

The paper discusses the use of physiological metrics (heart rate and breathing rate) and questionnaires to assess user perceptions and experiences in social VR.
The focus on user perceptions of avatars and biosignal visualizations directly relates to understanding user experience in VR. The study aims to enhance communication and emotional expression in social VR, which is a critical aspect of user experience","### 1. Participants
- Number of Participants: 32 participants (18 female, 13 male, 1 non-binary)
- Age Range: 19-33 years (M = 26.7, SD = 3.6)
- Background: Primarily undergraduate/graduate students, with varied nationalities and backgrounds. 90.6% had tried HMD-based VR at least once, while only 21.8% had experience with social VR platforms

### 2. Study Design
- Type: Within-subjects design
- Conditions: The study involved a 4 (Visualization: Skeuomorphic, Particles, Creature, Environment) x 2 (Biosignal Type: Heart Rate, Breathing Rate) x 3 (Signal Rate: Low, Rest, High) design, resulting in 24 different conditions

### 3. Procedure
- Duration: Approximately one hour
- Initial Steps: Participants signed consent forms and filled out demographic details. They received an overview of the study and a tutorial on navigating within VR and selecting questionnaire responses
- Quantitative Part: Participants observed their agent companion avatar and the environment, responding to Likert-scale questions presented on a panel
- Qualitative Part: After a break, participants engaged in a paired interview within the VR environment, discussing their experiences and perceptions of the biosignal visualizations

### 4. Task
- Main Task: Participants were instructed to observe their companion avatar and the jazz bar environment, reflecting on their experiences and responding to questions about perceived avatar arousal, distraction, and preferences for the visualizations

### 5. Metrics Collected
- Quantitative Metrics:
  - Perceived avatar arousal (Likert scale from 1 to 9)
  - Perceived visualization distraction (Likert scale from 1 to 9)
  - Overall visualization preference (forced-choice task)
  - Igroup Presence Questionnaire (IPQ) scores

- Qualitative Metrics:
  - Semi-structured interviews to gather insights on overall experience, biosignal interpretations, and other related topics","IPQ (Igroup Presence Questionnaire),  Likert Scake (7-Point),  Likert Scake - Avatar Arousal,  Likert Scake - Emotion Inference,  Semi-structured Interviews",,"Respiration,  Heart Rate",,"Questionnaires,  Physiological"
ID170,"Understanding Multi-user, Handheld Mixed Reality for Group-based MR Games","Bautista Isaza  Carlos Augusto,  Enriquez  Daniel,  Moon  Hayoun,  Jeon  Myounghoon,  Lee  Sang Won",,10.1145/3653688,2024,Proceedings of the ACM on Human-Computer Interaction,MR,Interaction Techniques & input Modalities,Embodiment Avatars & Social Presence.,Human-Computer Interaction (HCI),"This paper investigated how group size (two, four or eight players) and interaction style (touching holographic balloons with the tablet versus shooting virtual projectiles) shape experience in a handheld MR game. Larger teams felt less challenged and expended less effort but also reported lower positive affect, while proximity-based touch produced higher social presence yet higher physical and temporal workload; pointing-based shooting demanded more mental effort and caused more frustration. Despite these contrasts, both input methods were equally engaging and sustained attention for similar periods, and players naturally avoided collisions even in the most crowded condition. Preferences split: some favoured the fast, physical two-player touch mode, others the calmer eight-player shooting mode, highlighting divergent design needs in multi-user handheld MR.","1) Participants
• Total Participants: 40 were recruited, but data 
from 2 participants were excluded due to late arrival, resulting in 38 
participants (26 male, 12 female).
• Average Age: 25.79 years (std. dev. = 5.01).
• Experience: 27 participants had prior experience with VR, and 26 had experience with multiplayer games.
• Compensation: Each participant received a $10 electronic gift card https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=37484,38245, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=39025,39771.
2) Study Design
• Type: A within-subjects design with two independent variables:
    ◦ Interaction Type: Two types (Poke - proximity-based, and Shoot - pointing-based).
    ◦ Group Size: Three conditions (1:1, 2:2, and 4:4).
• Total Conditions: Each participant experienced six different conditions, combining the interaction types and group sizes https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=33632,34372, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=38246,39024.
3) Procedure
• Participants were briefed on the study's purpose and signed a consent form.
• They completed a demographic questionnaire and were assigned tracker IDs.
• Participants were trained on the interaction methods until comfortable.
• Each game experience lasted two minutes, and the order of conditions was counterbalanced.
• After each game, participants completed a post-game questionnaire and provided open-ended feedback https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=37484,38245 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=38246,39024.
4) Task
• The task involved a competitive multiplayer game where participants 
interacted with virtual balloons to change their colors to match their 
team (red or blue). The goal was to claim more balloons than the 
opposing team https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=26890,27671, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=26115,26889.
5) Metrics Collected
• Perceived Workload: Measured using NASA-TLX, assessing mental demand, physical demand, temporal demand, performance, effort, and frustration.
• Perceived Connectivity: Participants rated their perceived connectivity on a scale from 0 (Never) to 4 (Always).
• Preference: Participants ranked the six conditions from most enjoyable (Rank 1) to least enjoyable (Rank 6).
• Post-Game Questionnaire: Included questions on presence, social presence, engagement, and workload https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=35879,36525, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/681494441c3af2fd9192606d/?range=35134,35878.
This structured approach allowed the researchers to explore the 
effects of group size and interaction methods on user experiences in 
handheld mixed reality environments.","NASA-TLX,  Presence – General,  Social Presence,  Game Experience Questionnaire (GEQ),  Unstructured Interviews",,,,Questionnaires
ID171,Understanding Perspectives for Single- and Multi-Limb Movement Guidance in Virtual 3D Environments,"Elsayed  Hesham,  Kartono  Kenneth,  Schön  Dominik,  Schmitz  Martin,  Mühlhäuser  Max,  Weigel  Martin",,10.1145/3562939.3565635,2022,ACM Symposium on Virtual Reality Software and Technology,VR,Interaction Techniques & input Modalities,Behavioural Dynamics & Exploration.,"Education and Training, Healthcare and Sports","The paper investigates how different perspectives (first-person and third-person) influence the accuracy of movement guidance, which is a critical aspect of interaction techniques in immersive environments.
It evaluates the effectiveness of these perspectives in guiding users through single- and multi-limb movements, which is essential for applications in physical therapy, sports training, and other domains.
The study also explores feedback mechanisms (visual and haptic) that enhance user interaction and performance during movement tasks.

### Key Contributions:
1. Evaluation of Movement Accuracy: The study provides empirical evidence on how perspective affects the accuracy of single- and multi-limb movements, demonstrating that third-person perspectives can significantly enhance movement accuracy compared to first-person perspectives

2. Insights into User Preferences: It explores user preferences for feedback types during movement tasks, revealing that participants favored haptic feedback despite it not showing a significant performance improvement

3. Implications for Application Domains: The findings have practical implications for fields such as physical therapy and sports training, where accurate movement guidance is crucial for effective training and rehabilitation","### 1. Participants
- Number of Participants: 18 (13 male, 5 female)
- Age Range: 21 to 27 years (Mean = 23.89, SD = 1.94)

### 2. Study Design
- The study employed a controlled user study design with a total of 162 conditions based on the following independent variables:
  - Perspective: First-person (1pp), Third-person (3pp), Multi-third-person (Multi3pp)
  - Movement Complexity: One arm, two arms, two arms + leg
  - Movement Direction: Forward, sideways, backward
  - Movement Speed: Slow and fast
  - Feedback Type: None, haptic, color

### 3. Procedure
- Participants provided informed consent and demographic data.
- They were instructed to replicate a movement after observing it in VR.
- Each trial began with participants in a neutral pose, followed by visualizing a movement. After pressing a trigger button, they replicated the movement and indicated completion by pressing the button again.
- Participants completed all movements for a specific perspective before taking a short break

### 4. Task
- The primary task was to replicate movements demonstrated in VR, which varied in complexity and direction. Participants were instructed to perform the movements at the same speed as displayed

### 5. Metrics Collected
- Joint Angle Errors: Measured frame by frame for all joints involved in the movements (shoulders, elbows, hips, knees) to calculate the average error per movement.
- Feedback Preferences: Qualitative feedback was collected through a survey post-experiment, asking participants about their experiences and preferences regarding the VR system and feedback types",Feedback Prefernces,,,Error Rate,"Questionnaires,  Performance"
ID172,Understanding the Effects of Perceived Avatar Appearance on Latency Sensitivity in Full-Body Motion-Tracked Virtual Reality,"Halbhuber  David,  Kocur  Martin,  Kalus  Alexander,  Angermeyer  Kevin,  Schwind  Valentin,  Henze  Niels",,10.1145/3603555.3603580,2023,Mensch Und Computer,VR,Embodiment Avatars & Social Presence,Content & System Design.,Human-Computer Interaction (HCI),"It explores how the visual appearance of avatars can influence user experience and performance, particularly in relation to latency.
The outcomes of the study indicate that embodying avatars perceived as more fit enhances physical performance and user experience, contributing to the understanding of quality in XR.
The authors concluded that while the avatar's visual appearance significantly affects user experience and performance, it does not mitigate the adverse effects of latency in full-body motion-tracked VR settings. They suggest that future research should explore different avatar characteristics, such as clothing or accessories, to further understand their potential impact on user experience in relation to latency","### 1. Participants
- Total Participants: 16 participants (8 male, 8 female) were recruited for the main study.
- Age Range: Participants' ages ranged from 18 to 29 years (M = 22.38 years, SD = 2.87 years)

### 2. Study Design
- Type: Within-subjects design.
- Independent Variables: 
  - Visual Appearance (perceived as more fit vs. perceived as less fit).
  - Latency (system latency vs. high latency)

### 3. Procedure
- Participants were briefed about the study's purpose and provided informed consent.
- They completed a demographic questionnaire and self-rated their experience with VR.
- Participants were equipped with a motion tracking suit and a head-mounted display (HMD).
- They performed tasks in a virtual environment designed to mimic a real-world laboratory, including virtual mirrors for avatar familiarization

### 4. Task
- Task 1: Reaction Wall Task: Participants aimed to touch flashing fields on a virtual wall as quickly as possible for 60 seconds. Points were awarded for correct touches.
- Task 2: Barbell Task: Participants performed two-handed bicep curls with a real 5 kg barbell for 40 seconds, aiming to complete as many repetitions as possible

### 5. Metrics Collected
- Performance Metrics:
  - Reaction Wall Task: Total score (number of correctly touched fields) and error rate (number of incorrectly touched fields).
  - Barbell Task: Total number of repetitions completed.
- Physiological Metrics: Heart rate measured using a Polar OH1 sensor.
- Subjective Measures: 
  - Borg Scale for perceived exertion (assessed every 20 seconds).
  - Body Representation Questionnaire (BRQ) for body ownership.
  - Igroup Presence Questionnaire (IPQ) for general presence and experienced realism.
  - Intrinsic Motivation Inventory (IMI) for enjoyment and perceived competence","BRQ,  Borg scale,  IMI,  IPQ (Igroup Presence Questionnaire),  SPF",,Heart Rate,"Error Rate,  Task Success / Completion","Questionnaires,  Physiological,  Performance"
ID173,Understanding User Experiences Across VR Walking-in-place Locomotion Methods,"Tan  Chek Tien,  Foo  Leon Cewei,  Yeo  Adriel,  Lee  Jeannie Su Ann,  Wan  Edmund,  Kok  Xiao-Feng Kenan,  Rajendran  Megani",,10.1145/3491102.3501975,2022,Conference on Human Factors in Computing Systems,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This paper investigated how users experience four easy-to-set-up WIP locomotion methods while exploring a large VR commuting scene. Forty participants walked the same guided route with each method, thinking aloud and later being interviewed; after every condition they completed the Virtual-Reality Sickness Questionnaire, Igroup Presence Questionnaire and Flow Short Scale. Leg-lifting delivered the most positive comments, felt the most natural and brought the lowest sickness, whereas arm-swinging caused the highest sickness yet required the least effort. Head-bob and full-body fusion were often rated fatiguing and sometimes disorienting because exaggerated bobbing impaired vision. Across methods, users adapted idiosyncratic walking gestures based on tracker placement, craved finer speed control, and kept mental track of room boundaries, which cut involvement. The authors distil five design insights, recommending leg-lift as a default, motivating exertion through tracker affordances, accommodating diverse gait styles and integrating WIP with spatial layout and subtle guidance cues.","1. Participants
• Total: 42 participants recruited, 40 completed the study (25 male, 15 female).
• Age: 21 to 45 years old (Mean = 26.2, SD = 5.1).
• Experience: Majority had minimal VR exposure (36/40 had under 2 hours prior experience).
2. Study Design
• Design Type: Within-subjects design.
• Conditions: Each participant tested four different VR walking-in-place (WIP) locomotion methods:
    ◦ Head-bob
    ◦ Arm-swing
    ◦ Leg-lift
    ◦ Full-body (combination of the above three)
• Order of Conditions: Randomized for each participant to counterbalance order effects.
• Duration: Each condition lasted ~10 minutes. Total session time ~2 hours including breaks and interviews.
3. Procedure
1. Consent and Demographics Survey
2. VR Familiarization: Participants practiced each locomotion method in a test environment.
3. Task Execution: Each locomotion condition took place in a VR commuting simulation.
4. Think-aloud: Participants verbalized thoughts during the task.
5. Observation and Video Recording: Researchers took notes and recorded participants.
6. Post-condition Surveys: Administered after each locomotion method.
7. Break: 10 minutes rest between conditions.
8. Final Interview: Semi-structured interview to gather qualitative insights and preferences.
4. Task
• Simulation Type: VR commuting scenario simulating an urban environment (streets, sidewalks, bus stops).
• Objective: Navigate using each WIP method through guided paths with visual UI markers.
• Focus: Experience natural locomotion (walking-like movement) using the given method in each condition.
5. Metrics Collected
Qualitative Data:
• Think-aloud Protocols
• Semi-structured Interviews
• Observational Notes
Quantitative Data (Self-Report Questionnaires):
• Virtual Reality Sickness Questionnaire (VRSQ): Assesses cybersickness symptoms.
• Igroup Presence Questionnaire (IPQ): Measures immersion/presence.
• Flow Short Scale (FSS): Assesses flow state and perceived fluency.
Additional:
• Code Frequencies: Derived from thematic analysis of qualitative data.
• Preference Rankings: Participants ranked methods based on comfort and perceived naturalness.","Flow Short Scale,  Group Presence Questionnaire (IPQ),  VRSQ (Virtual Reality Sickness Questionnaire)",,,,Questionnaires
ID174,Universal Access for Social XR Across Devices: The Impact of Immersion on the Experience in Asymmetric Virtual Collaboration,"Latoschik  C. Merz,  C. Göttfert,  C. Wienrich,  M. E.",,10.1109/VR58804.2024.00105,2024,Conference on Virtual Reality and 3D User Interfaces,VR,Embodiment Avatars & Social Presence,Content & System Design.,Telecommunications and Collaboration,"The main outcomes of the paper indicate that while the level of immersion significantly affects self-perception and the sense of presence, it does not impact the perception of others or overall user satisfaction in asymmetric collaborative social XR environments. Here are the key findings:


1. Self-Perception and Presence: Higher immersion leads to a better quality of self-perception, confirming that immersion affects the sense of embodiment and presence
2. Other-Perception: The study found no significant differences in the perception of others, co-presence, or social presence, regardless of the immersion level of oneself or the other participant
3. User Satisfaction and Task Load: User satisfaction and task enjoyment were not significantly affected by the level of immersion. The only notable difference was in physical demand, which was higher for the desktop setup with controllers compared to the desktop with a mouse
4. Implications for Asymmetric Collaboration: The findings suggest that asymmetric interaction can be effective for collaboration, allowing users with different devices to participate without compromising the quality of the interaction

We measured co-presence and social presence with the networked mind measurement (NMM). It consists of four subscales: perception of self and of the other co-presence and perception of self and of the other psychological engagement (PE), consisting of 34 items on a scale from 1 to 7.","The authors developed a social XR environment that enables asymmetric interaction and collaboration between users. Two physically remote users collaborate in one virtual space with different interaction devices: (i) desktop screen with mouse, (ii) desktop screen with VR controllers as in- put, and (iii) HMD with VR controllers. ","IPQ (Igroup Presence Questionnaire),  NASA-TLX (RTLX),  SSQ (Simulator Sickness Questionnaire),  Social Presence,  VEQ",Movement Trajectories,,,"Questionnaires,  Behavioural"
ID175,User Behaviour Analysis of Volumetric Video in Augmented Reality,"Zerman  Emin,  Kulkarni  Radhika,  Smolic  Aljosa",,10.1109/QoMEX51781.2021.9465456,2021,International Conference on Quality of Multimedia Experience,AR,Behavioural Dynamics & Exploration,,Human-Computer Interaction (HCI),"This paper investigated how users move around volumetric videos in mobile augmented reality. With a marker-based Android app, device position and rotation were recorded while viewers freely inspected two human holograms. Analysis showed that users spent roughly 70 % of the viewing time within a ±60° frontal arc and about 45 % inside a tighter ±20° cone, staying on average 1.6 heights away on the floor plane and 2.3 heights in 3-D space. They also favoured eye-level vertical angles between 30° and 60°. The authors release the trajectory dataset and argue that these patterns can guide viewport-prediction, mesh-resolution choices, and adaptive streaming for future AR volumetric-video services.",The participants watched the volumetric video in the AR application “however they wish” without any specific instructions. They were not limited in duration so that their movement behaviour was recorded in a natural way without forcing them into a time pressure or uncomfortable situation while they are consuming the VV,,Movement Trajectories,,,Behavioural
ID176,"User Experience of Reading in Virtual Reality — Finding Values for Text Distance, Size and Contrast","Kojic  Tanja,  Ali  Danish,  Greinacher  Robert,  Moller  Sebastian,  Voigt-Antons  Jan-Niklas",,10.1109/QoMEX48832.2020.9123091,2020,International Conference on Quality of Multimedia Experience,VR,Visualization Techniques,,"Education and Training, Human-Computer Interaction (HCI)","The main outcome of the study is the identification of optimal text parameters for readability in virtual reality (VR). The research demonstrates how different text characteristics, such as size, distance, and contrast, affect user experience and comprehension in XR environments. By allowing participants to manipulate these parameters and report their experiences, the study provides insights into how these factors can enhance the overall quality of user experience in XR","### 1. Participants
- A total of 22 participants (54.5% male and 45.5% female).
- Average age: 28.41 years (SD = 9.56, min = 19, max = 62).
- Most participants had some experience with virtual environments (86.4%), while only 3 participants (16.6%) had never tried virtual reality before

### 2. Study Design
- The study utilized a within-subjects design with two different standalone VR devices (Oculus Go and Oculus Quest) and three different text lengths (short: 2 words, medium: 21 words, long: 51 words).
- Participants were tasked with setting text parameters to achieve both the best and worst readability

### 3. Procedure
- Participants were welcomed and given an introduction to the study, followed by signing a consent form.
- They completed a pre-questionnaire regarding demographics, VR experience, and reading habits.
- Participants underwent a training session to familiarize themselves with the VR devices and controllers.
- After training, participants set text parameters for each condition, completing two tasks: one to set the best readability and another to set the worst readability

### 4. Task
- The main tasks involved:
  - Best readability task: Participants adjusted text parameters (distance, font size, contrast) to achieve optimal readability.
  - Worst readability task: Participants adjusted the same parameters to create the least readable text

### 5. Metrics Collected
- Angular Size: Measured as a combination of font size and distance, reported in distance-independent millimeters (dmm).
- Contrast Ratio: Calculated between the text color and background color.
- Self-Assessment Manikin (SAM): Used to assess emotional responses (pleasure, arousal, dominance) during the tasks.
- System Usability Scale (SUS): Participants rated the usability of reading in VR after completing the tasks","SAM (Self-Assessment Manikin),  System Usability Scale (SUS)",,,,Questionnaires
ID177,User Experience of Stereo and Spatial Audio in 360° Live Music Videos,"Holm  Jukka,  Väänänen  Kaisa,  Battah  Anas",,10.1145/3377290.3377291,2020,International Conference on Academic Mindtrek,"360 video, VR, spatial audio",Content & System Design,,Entertainment and Gaming,"Participants rate their experiences using a 7-point Likert scale for various metrics, including perceived audio quality, presence, and overall listening experience.

The combination of HMD and spatial audio scored highest in perceived audio quality, presence, and overall listening experience, indicating a strong user preference for this setup","### 1. Participants
- Number of Participants: 20 (15 male, 5 female)
- Age Range: 22-34 years (mean age = 26)
- Musical Background: 9 participants played a musical instrument

### 2. Study Design
- The study utilized a mixed-method laboratory experiment design, where participants experienced different audio formats (stereo and spatial audio) in a 360° music video setting

### 3. Procedure
- Participants watched four versions of the 360° video:
1. Flat computer display, 2D video, stereo audio
2. Flat computer display, 2D video, spatial audio
3. Head-mounted display (HMD), 3D video, stereo audio
4. Head-mounted display (HMD), 3D video, spatial audio
- The flat display variations were presented first, followed by the HMD conditions to avoid novelty effects
- After each video, participants filled out a questionnaire focusing on their immediate experiences using a 7-point Likert scale. A semi-structured interview was conducted after all videos to gather qualitative insights

### 4. Task
- The primary task involved watching and evaluating the 360° music videos, which included assessing audio quality, presence, and overall listening experience

### 5. Metrics Collected
- Quantitative Metrics:
  - Questionnaires: Participants rated their experiences on a 7-point Likert scale for audio quality, presence, and overall listening experience.
- Qualitative Metrics:
  - Semi-Structured Interviews: Gathered insights on user preferences and experiences regarding the audio formats and video conditions

","Likert Scake (7-Point),  Semi-structured Interviews",,,,Questionnaires
ID178,Using gaze transition entropy to detect classroom discourse in a virtual reality classroom,"Stark  Philipp,  Jung  Alexander J.,  Hahn  Jens-Uwe,  Kasneci  Enkelejda,  Göllner  Richard",,10.1145/3649902.3653335,2024,symposium on eye tracking research and applications,VR,Behavioural Dynamics & Exploration,,Education and Training,"This paper investigates gaze transition entropy as a quantitative metric to detect classroom discourse events (teacher questions, student hand-raising, and answers) in an immersive VR classroom. Using eye-tracking data from 240 secondary school students, the authors computed transition entropy (reflecting unpredictability of gaze shifts between AOIs) and stationary entropy (reflecting uniformity of gaze distribution) across different levels of student participation(20%, 35%, 65%, 80% of virtual peers raising hands).
Results show that both entropy measures successfully differentiated between teacher-centered discourse events and teacher explanations, reaching a classification accuracy of 67% using logistic regression. Transition entropy increased during interactive classroom events, reflecting more dynamic visual exploration, while stationary entropy increased with higher levels of virtual student participation. The study validates entropy-based gaze metrics as robust indicators of attentional engagement and social perception in VR educational contexts.","1) Participants
• N = 240 (secondary school students, sixth grade, Germany).
• Originally N = 381, reduced after eye-tracking data quality filtering (>90% tracking ratio).
• Randomly assigned to one of four hand-raising conditions (20%, 35%, 65%, 80%).
2) Study Design
• Between-subjects design based on hand-raising level (student participation).
• Events analyzed:
– Teacher-centered discourse: teacher questions, student hand-raising, and answers.
– Teacher explanation: teacher lecturing without student participation.
• Binary event coding (1 = discourse, 0 = explanation) used for prediction analysis.
3) Apparatus & Setup
• Hardware: HTC Vive Pro Eye (Tobii integrated eye tracker, 0.5°–1.1° accuracy).
• Software: Unreal Engine 4.23.1.
• Scene: 15-min simulated lesson on computational thinking in a VR classroom with virtual teacher and animated peer learners.
• 26 AOIs defined (teacher, board, 24 virtual students).
• Gaze data aggregated in 30-second intervals with sliding window (10 s).
4) Metrics Collected
Physiological / Behavioral Metrics:
• Transition Entropy (TE): unpredictability of gaze transitions between AOIs (visual exploration).
• Stationary Entropy (SE): uniformity of gaze distribution (attention dispersion).
• AOI duration & transition matrices for 30s windows.
• Hand-raising condition (proxy for classroom participation intensity).
Predictive Modeling Metrics:
• Multi-level linear regression (entropy vs. event type and participation level).
• Logistic regression (TE & SE → event prediction).
Main Findings
• Entropy measures differentiated discourse vs. explanation events:
– Transition entropy higher during interactive discourse (β_event = 0.20, p < .001).
– Stationary entropy higher during discourse (β_event = 0.63, p < .001).
• Participation level effects:
– TE predictive power stronger at extreme hand-raising levels (20% & 80%).
– SE predictive power increased mainly in 80% condition (more uniform gaze spread).
• Model accuracy: Logistic regression correctly classified 67% of events (F1 = 0.67).
• Reduced AOI model (3 AOIs: teacher, students, board): Accuracy dropped to 61%.
• Interpretation: Gaze entropy reliably captures attention modulation and engagement during social interactions in VR classrooms.",,"Head Analysis,  Gaze Analysis,  Entropy Analysis",,,Behavioural
ID179,Using Heart Rate Variability for Comparing the Effectiveness of Virtual vs Real Training Environments for Firefighters,"Narciso  David,  Melo  Miguel,  Rodrigues  Susana,  Cunha  João Paulo,  Vasconcelos-Raposo  José,  Bessa  Maximino",,10.1109/TVCG.2022.3156734,2023,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,,Healthcare and Medical Training,"This paper compares the effectiveness of a Virtual Environment (VE) and Real Environment (RE) for firefighter training by measuring Heart Rate Variability (HRV), perceived fatigue, stress, presence, and knowledge transfer. It introduces a novel methodology using HRV to quantify physiological stress as an indicator of training effectiveness. The results show that both environments induced physiological stress, but the RE produced higher stress levels. Despite this, the VE was still able to induce comparable stress, good presence, effective knowledge transfer, and no cybersickness—supporting its potential as a viable training tool, though slightly less effective than RE.

- ""Perceived Stress Scale (PSS)""
- ""Checklist Individual Strength (CIS)""
- ""Igroup Presence Questionnaire (IPQ)""
- ""Simulator Sickness Questionnaire (SSQ)”
- Visual Analogue Scales (VAS) for perceived fatigue and stress. A VAS consists of a one-item scale where the user evaluates the amount of a given characteristic/attitude on a level of one to ten [48], [49]. VASs were used to measure the perception of fatigue and stress, in addition to the questionnaires specific for that purpose (CIS and PSS), because they are a quick and easy way to obtain several measurements in an experiment and make a before and after comparison to assess the influence of a particular environment


All questionnaires were completed in Portuguese.","### 1. Participants
- Total Participants: 12 firefighter trainees (5 males, 7 females)
- Final Sample for VE: 11 participants (after one dropped out due to discomfort)
- Final Sample for RE: 5 participants (due to refusals and scheduling conflicts).


### 2. Study Design
- Type: Quasi-experimental study with a within-subjects design.
- Independent Variable: Environment (baseline, virtual environment (VE), real environment (RE)).
- Dependent Variables: Physiological stress (measured via HRV), perceived fatigue, perceived stress, knowledge transfer, sense of presence, and cybersickness.


### 3. Procedure
- Day 1: Baseline HRV measurement and VE exercise.
- Day 2: RE exercise.
- Participants completed consent forms, pre-questionnaires, and underwent HRV baseline measurement while relaxing.
- In the VE, participants used VR equipment (HTC Vive HMD, Vital Jacket for HRV monitoring, etc.) and completed a series of tasks.


### 4. Task
- VE Task: Participants engaged in a firefighting training exercise simulating conditions of smoke and heat in a virtual environment.
- RE Task: Participants performed the same firefighting training exercise in a real environment.


### 5. Metrics Collected
- Physiological Metrics: Heart Rate Variability (HRV) parameters (AVNN, SDNN, RMSSD, pNN20, LF/HF).
- Questionnaires: 
  - Perceived Stress Scale (PSS)
  - Checklist Individual Strength (CIS)
  - Igroup Presence Questionnaire (IPQ)
  - Simulator Sickness Questionnaire (SSQ)
  - Visual Analogue Scales (VAS) for perceived fatigue and stress","CIS,  IPQ (Igroup Presence Questionnaire),  PSS,  SSQ (Simulator Sickness Questionnaire),  VAS",,HRV / IBI,,"Questionnaires,  Physiological"
ID180,Vicarious: Context-aware Viewpoints Selection for Mixed Reality Collaboration,"Zaman  Faisal,  Anslow  Craig,  Rhee  Taehyun James",,10.1145/3611659.3615709,2023,ACM Symposium on Virtual Reality Software and Technology,"AR, VR",Visualization Techniques,Embodiment Avatars & Social Presence.,Telecommunications and Collaboration,"This paper investigated whether letting a system guide or auto-switch between first- and third-person views improves remote MR collaboration. Twenty-seven triads (one AR “local” user, two VR “remote” users) built small Lego-and-domino models under four conditions: no extra view, manual view choice, guided view highlighting, and fully automatic view switching. Guided and Automatic selection gave remote users a clearer grasp of the workspace, cut overall task time, and reduced NASA-TLX workload compared with manual choice or no view at all; the Guided mode also raised both spatial and social-presence ratings and was the most preferred. Automatic switching sometimes distracted AR wearers and increased workload relative to Guided. Simulator-sickness remained low in every condition. The authors recommend combining lightweight guidance with user control for future multi-user MR systems.","1) Participants
• Number of Participants: 27 (21 male, 6 female)
• Age Range: 18 to 55 years (M = 28.13, SD = 7.32)
• Recruitment: Participants were recruited through advertisements and flyers at local universities and community centers.
• Vision: All participants had normal or corrected normal vision.
• Ethnicity: Diverse sample with 12 European, 6 Asian, 5 mixed race, 3 Latino/Hispanic, and 1 Pacific Islander.
• Experience with AR/VR: 18 participants reported no prior experience, while 9 had used AR/VR for an average of 10 hours in the past year https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=26633,27379, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=35018,35572.
2) Study Design
• Design Type: 4 × 2 mixed factorial design.
• Within-Subjects Variable: Viewpoint Condition (4 levels: No-view, Manual, Guided, Automatic).
• Between-Subjects Variable: Participants Role (2 levels: Local and Remote user) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=25696,26632, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=34106,34499.
3) Procedure
• Participants signed a consent form and provided demographic information.
• They were divided into groups of three (two remote experts and one local user).
• A 5-minute familiarization with the system was provided.
• Participants completed four conditions sequentially, with 5-minute breaks in between.
• Each condition lasted approximately 10-12 minutes, followed by subjective questionnaires https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=29727,30628, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=37434,37827.
4) Task
• The collaborative task involved multiple remote users guiding a local user in building a model using Legos and dominoes.
• The local user was in a task space where the materials were placed, 
while remote users provided instructions through a 360° camera 
livestream https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=15164,15896, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=28836,29726.
5) Metrics Collected
• Activity Logging: Voice chat and completion time were logged.
• Presence Measurement: Spatial presence was measured using the iGroup Presence Questionnaire (IPQ) with a 7-point Likert scale.
• Social Presence: A questionnaire was developed to assess social presence using various subscales.
• Workload Measurement: NASA-TLX was used to assess cognitive load.
• Usability Measurement: System Usability Scale (SUS) was employed.
• Simulator Sickness: Evaluated using the Simulator Sickness Questionnaire (SSQ) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=37434,37827, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/68133e7854d56ff7836bd14a/?range=37828,38136.
","Group Presence Questionnaire (IPQ),  NASA-TLX,  SSQ (Simulator Sickness Questionnaire),  Social Presence,  System Usability Scale (SUS)","Movement Trajectories,  Communication Analysis",,Completion Time,"Questionnaires,  Behavioural,  Performance"
ID181,Virtual Animals as Diegetic Attention Guidance Mechanisms in 360-Degree Experiences,"Norouzi  Nahal,  Bruder  Gerd,  Erickson  Austin,  Kim  Kangsoo,  Bailenson  Jeremy,  Wisniewski  Pamela,  Hughes  Charlie,  Welch  Greg",,10.1109/TVCG.2021.3106490,2021,Transactions on Visualization and Computer Graphics,"360-degree video, VR",Visualization Techniques,,Entertainment and Gaming," The paper investigates the effectiveness of virtual animals as diegetic attention guidance mechanisms compared to non-diegetic ones. Findings revealed that the acknowledging dog significantly improved user experience, increased sense of presence, and was more positively received than non-diegetic guidance methods.

Diegetic guidance refers to a method of directing a user's attention or actions within a virtual environment by using elements that are part of the narrative or story world itself. In immersive technologies like virtual reality (VR) or augmented reality (AR), diegetic elements are those that exist within the context of the experience and are perceived as natural components of the environment. In contrast, non-diegetic guidance would involve elements that are not part of the story world, such as on-screen menus, HUDs (heads-up displays), or other overlays that exist outside the narrative context. 

Mechanism Visibility Ratio: Measured the time each guidance mechanism (e.g., arrow, bird, dog) remained in the user’s field of view (FoV) during the guidance period.

Event Start Visibility: Recorded whether the beginning of each event (like dancing or exercising) was visible to the participant when it started, depending on the effectiveness of the guidance.

Event Visibility Ratio: Calculated the proportion of time each event remained in the participant’s FoV, indicating how long users observed the events after being guided to them.","### 1. Participants
- Number: 28 participants (10 male, 18 female)
- Age Range: 18–28 years (M = 21.21, SD = 2.84)


### 2. Study Design
- Type: Within-subjects design
- Independent Variable: Attention guidance mechanism with five levels:
  - No Guide (control)
  - Arrow (non-diegetic and non-acknowledging)
  - Bird (diegetic and non-acknowledging)
  - Dog (diegetic and non-acknowledging)
  - Acknowledging Dog (diegetic and acknowledging)
- Randomization: Latin Square was used to randomize the order of conditions to account for order effects.

### 3. Procedure
- Participants provided informed consent and completed the Simulator Sickness Questionnaire.
- They were briefed on the experiment's structure and allowed to explore a virtual neighborhood using an HTC Vive Pro HMD.
- A one-minute familiarization session was conducted before starting the experimental conditions.
- After each condition, participants answered several questionnaires on a computer and participated in a post-study interview.


### 4. Task
- Participants were instructed to freely explore the virtual neighborhood while observing various target events (e.g., people exercising, dancing, or playing).
- The task involved following or ignoring the guidance provided by the virtual animals or arrows, depending on the condition.


### 5. Metrics Collected
- Objective Measures:
  - Mechanism Visibility Ratio: Duration the attention guidance mechanisms were in the participant's field of view (FoV) during the guidance period.
  - Event Start: Whether the start of events was within the participant's FoVhttps://go.atlasti.com/DE7E574C-5FF1-4E75-BFD2-0454AB8FB2B3/documents/16B79CDE-F0E2-4BF0-8554-048F36305B69/range/80697-81548https://go.atlasti.com/DE7E574C-5FF1-4E75-BFD2-0454AB8FB2B3/documents/16B79CDE-F0E2-4BF0-8554-048F36305B69/range/44483-45349 https://www.notion.so/codeAll/10.
  
- Subjective Measures:
  - User Experience Questionnaire (UEQ): Assessed overall user experience.
  - Attention Guide Questionnaire (AGQ): Evaluated the effectiveness of the attention guidance mechanisms","AGQ,  Fear of Missing Out,  Presence – General,  SSQ (Simulator Sickness Questionnaire),  UEQ-S (Short User Experience Questionnaire),  User Preference Rankings",Visibility Metrics,,"Accuracy,  Completion Time","Questionnaires,  Behavioural,  Performance"
ID182,Virtual Humans with Pets and Robots: Exploring the Influence of Social Priming on One’s Perception of a Virtual Human,"Norouzi  Nahal,  Gottsacker  Matthew,  Bruder  Gerd,  Wisniewski  Pamela J.,  Bailenson  Jeremy,  Welch  Greg",,10.1109/VR51125.2022.00050,2022,Conference on Virtual Reality and 3D User Interfaces,AR,Embodiment Avatars & Social Presence,,"Entertainment and Gaming, Human-Computer Interaction (HCI)","The paper discusses the subjective and objective influences of social priming on user experience with virtual humans (VHs) in XR, indicating that social priming can enhance perceptions of VHs compared to non-social priming condition. The outcomes of the study suggest that social priming leads to a more positive user experience, which is a key aspect of understanding quality in XR.

social priming refers to the phenomenon where the observation of a virtual human (VH) engaged in social interactions with other entities (either real or virtual) influences the perceptions and behaviors of users in subsequent interactions with that VH. This concept is rooted in social psychology, where priming is defined as the incidental activation of knowledge structures, such as traits and stereotypes, by the current situational contex","### 1) Participants
- Number of Participants: 24 (7 female, 17 male)
- Age Range: 18–37 years (M = 24.17, SD = 5.18)


### 2) Study Design
- Type: Full-factorial within-subjects study using a 4×1 Latin Square design to counterbalance conditions.
- Conditions: Participants played a 20-questions guessing game with different virtual human (VH) partners across four conditions:
  - Social Priming with Virtual Human (STIM-VH)
  - Social Priming with Virtual Dog (STIM-DOG)
  - Social Priming with Virtual Robot (STIM-ROBOT)
  - Non-Social Priming (STIM-PHONE).


### 3) Procedure
- Participants were met in the lobby, guided to the experimental space, and provided informed consent.
- The experiment consisted of five stages:
  1. Donning the head-mounted display (HMD) and moving towards the doorway.
  2. Waiting for the GAME-VH to invite them in.
  3. Following GAME-VH's cues to start asking questions.
  4. Writing their last guess if they did not guess correctly.
  5. Leaving the room and doffing the HMD.


### 4) Task
- Main Task: Participants engaged in a 20-questions guessing game with a GAME-VH. The game was structured so that participants would always lose, ensuring that their perceptions of the GAME-VH were not influenced by winning or losing.


### 5) Metrics Collected
- Objective Measures:
  - Head Gaze: Logged to assess attention towards stimuli.
  
- Subjective Measures:
  - Social Presence: Assessed using a 9-item questionnaire.
  - Affective Attraction: Evaluated through a 5-item questionnaire.
  - Inclusion of Other in the Self: Assessed using a pictorial measure.","Affective Attraction,  Inclusion in the self scale,  Social Presence",Gaze Analysis,,,"Questionnaires,  Behavioural"
ID183,Virtual Reality Sickness Reduces Attention During Immersive Experiences,"Mimnaugh  Katherine J.,  Center  Evan G.,  Suomalainen  Markku,  Becerra  Israel,  Lozano  Eliezer,  Murrieta-Cid  Rafael,  Ojala  Timo,  LaValle  Steven M.,  Federmeier  Kara D.",,10.1109/TVCG.2023.3320222,2023,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,,"Human-Computer Interaction (HCI), VR museum gallery","The paper discusses the impact of VR sickness on attention, which relates to understanding user experience in XR.

- EEG: difference in P3b event-related potential (ERP) effect mean amplitudes

- The study provides evidence that individuals experiencing VR sickness exhibit reduced attentional capacity, which is measured through physiological metrics such as EEG (specifically the P3b amplitude) and self-reported metrics using the Simulator Sickness Questionnaire (SSQ).
The findings suggest that as VR sickness increases, the ability to focus on tasks diminishes, leading to impaired performance in immersive environments. This outcome highlights the importance of understanding VR sickness in the context of user experience and quality in XR applications","## 1) Participants
- The study involved 30 participants.


### 2) Study Design
- The study utilized a within-subjects design, where each participant experienced both the VR condition and a control condition. This design allowed for direct comparisons of attention levels under different conditions.


### 3) Procedure
- Participants first completed a Simulator Sickness Questionnaire (SSQ) to assess their baseline level of VR sickness. They then engaged in a VR experience using a head-mounted display (HMD) in a stationary VR museum gallery. After the VR experience, participants completed the SSQ again to measure any changes in sickness symptoms.


### 4) Task
- The primary task involved a dual-task oddball paradigm, where participants were required to respond to a target stimulus while ignoring non-target stimuli. This task was designed to measure attentional capacity and was conducted both inside and outside the VR environment.


### 5) Metrics Collected
- The following metrics were collected during the experiments:
  - Physiological Metrics: EEG data was collected to measure brain activity, specifically focusing on the P3b amplitude, which is associated with attentional processes.
  - Self-Reported Metrics: The Simulator Sickness Questionnaire (SSQ) was used to assess participants' symptoms of VR sickness before and after the VR experience.",SSQ (Simulator Sickness Questionnaire),,EEG,,"Questionnaires,  Physiological"
ID184,Virtual Reality User-Scene Interaction: Head-Rotation versus Joystick Movements,"Livatino  Salvatore,  Zocco  Alessandro,  Iqbal  Yasir,  Gainley  Phillip,  Morana  Giuseppe,  Farinella  Giovanni Maria",,10.1109/MetroXRAINE54828.2022.9967622,2022,IEEE International Conference on Metrology for Extended Reality Artificial Intelligence and Neural Engineering,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"The paper assesses differences between HMD + head-rotation and HMD + joystick-based interaction for changing observation viewpoints in a VR command and control application. It contributes to understanding how interaction methods affect user experience metrics, such as presence, ease of use, comfort, and performance (accuracy, response time).

- Interaction movement: Head movement vs. joystick input comparison","### 1. Participants
- Number of Participants: 12
- Demographics: Participants ranged in age from 20 to 55 years (average age: 27.4 years), with 53.3% being female. They had varying levels of experience with VR systems and computer games, uniformly distributed across the sample.


### 2. Study Design
- Type of Design: Within-subjects evaluation comparing two systems: 
  - HMD with joystick control (HMD-J)
  - HMD with head rotation control (HMD-H)
- Counterbalancing: The testing scenario and system sequence were assigned according to a predetermined counterbalanced schedule to avoid fatigue and learning effects.


### 3. Procedure
- Initial Steps: Participants were provided with information, a consent form, and a pre-test screening.
- Practice Session: A system practice session was administered to familiarize participants with the controls.
- Operation Sessions: Participants completed two repetitions of the operation session (R1, R2).
- Questionnaires: After the operation sessions, participants completed several questionnaires to assess their experiences.


### 4. Task
- Experimental Scenario: Participants monitored two clusters of drones represented by graphical icons, identifying hostile drones (colored red) that entered an area of interest (AOI). They needed to signal the hostile drones by mentioning their numbers.
- Challenges: The task required continuous changes in observation viewpoint to monitor incoming drones, which could hide from each othe.


### 5. Metrics Collected
- Objective Measurements:
  - Accuracy: Correct identification of hostile drones.
  - Time: Time taken to complete monitoring and identification tasks.
- Subjective Ratings:
  - Presence: Measured using the Igroup Presence Questionnaire (IPQ).
  - Comfort: Assessed using the Simulator Sickness Questionnaire (SSQ).
  - Ease of Use: Evaluated with the System Usability Scale (SUS).
  - Depth Perception and Learning: Assessed through a custom questionnaire addressing depth impression and object identification.","Custom made Depth Perception and Learning,  IPQ (Igroup Presence Questionnaire),  SSQ (Simulator Sickness Questionnaire),  System Usability Scale (SUS)",Movement Trajectories,,"Accuracy,  Response Time","Questionnaires,  Behavioural,  Performance"
ID185,Visual Fatigue Assessment Model Based on Eye-Related Data in Virtual Reality,"Chen  Xiao-Lin,  Hou  Wen-Jun",,10.1109/ICVR51878.2021.9483841,2021,International Conference on Virtual Reality,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Human-Computer Interaction (HCI),"The paper presents an objective visual fatigue assessment model using only an HMD-integrated eye tracker (HTC Vive Pro Eye). It identifies 13 eye-related features—including blink duration, fixation patterns, and pupil diameter—to predict fatigue levels through classification models (SVM-based). The model distinguishes low, medium, and high fatigue levels with accuracies of up to 91% for binary classification. It demonstrates that gaze and blink-related features vary significantly with time-on-task and fatigue, while pupil-related features remained stable due to the controlled VR environment. The model offers a low-cost, reliable method for continuous user fatigue monitoring in VR.

- Blink-Related Features: Number of blinks, total blink duration, average blink duration, variance of blink duration.
  - Gaze-Related Features: Fixation count, total fixation duration, average fixation duration, variance of fixation duration, total displacement of eye.
  - Pupil-Related Features: Average pupil diameter, standard deviation of pupil diameter, pupil diameter variation index","### 1. Participants
- Total Participants: 15 (6 male and 9 female)
- Age Range: 25 to 55 years (Mean = 42, SD = 14.4)

### 2. Study Design
- Type of Study: Experimental study involving repeated measures.
- Sessions: Each participant completed a total of 10 sessions, with each session consisting of 20 repetitions of a task.


### 3. Procedure
- Calibration: The eye tracker was recalibrated before each session to ensure accuracy, taking approximately 1 minute
- Session Structure: Each session began with a start target that participants had to select to initiate the task. After selection, the actual target would appear, and participants were instructed to select it as quickly and accurately as possible.

### 4. Task
- Types of Tasks: Two basic interactive tasks were designed:
  1. Teleporting to a Target Position: Participants used a controller to teleport to a designated location.
  2. Ray-Casting to Select a Target Sphere: Participants used ray-casting to select a target sphere.
- Repetitions: Each task was repeated 20 times per session, with the target's location randomized.


### 5. Metrics Collected
- Eye-Related Metrics: A total of 12 eye-related metrics were recorded, categorized into:
  - Blink-Related Features: Number of blinks, total blink duration, average blink duration, variance of blink duration.
  - Gaze-Related Features: Fixation count, total fixation duration, average fixation duration, variance of fixation duration, total displacement of eye.
  - Pupil-Related Features: Average pupil diameter, standard deviation of pupil diameter, pupil diameter variation index.",,,"Eye Blink Analysis,  Eye-Tracking,  Pupil Analysis",,Physiological
ID186,Visualization Placement for Outdoor Augmented Data Tours,"Ghaemi  Zeinab,  Ananta Satriadi  Kadek,  Engelke  Ulrich,  Ens  Barrett,  Jenny  Bernhard",,10.1145/3607822.3614518,2023,Symposium on Spatial User Interaction,AR,Visualization Techniques,,"Education and Training, Human-Computer Interaction (HCI)","This paper investigated how placing AR charts on a building façade, on the ground, or on a hand-held virtual map changes comprehension and user experience during an outdoor campus “net-zero” tour delivered through HoloLens 2. Eighteen participants walked a four-stop route and, at each stop, answered data-reading questions, then later recalled chart trends. Building-attached charts were most preferred and supported better recall than floor-level charts; map-attached charts yielded the highest reading accuracy but did not improve recall over building placement. Subjective ratings showed low workload and fatigue in all conditions. The authors recommend combining building placement for memorability with near-user map placement when precise reading is essential. ","It describes a user study designed to investigate the impact of different visualization placements on user experience and performance in an augmented reality (AR) context. 
Type of visualisation:
- On-building: Visualizations were placed on the actual buildings.
- On-floor: Visualizations were positioned on the ground close to the user.
- On-map: Visualizations were attached to a virtual map displayed in front of the user

Procedure: 
  - Participants completed an augmented data tour that included visualizations of water and electricity consumption, as well as CO2 emissions for various buildings on campus. Each participant experienced all three visualization placements, with different charts used for each
 - The tour included four checkpoints, and participants were required to stand at these checkpoints to view the visualizations. They listened to audio narratives and answered questions related to the information displayed

Tasks:
   - At each checkpoint, participants answered questions based on the visualizations. After completing the tour, they answered a general question to assess their recall of the informationhttps://go.atlasti.com/DE7E574C-5FF1-4E75-BFD2-0454AB8FB2B3/documents/EF0CC73C-64A9-41FD-A59E-1605964F7CB5/range/25128-26096","Custom made,  PAAS,  PAAS - Cognitive Load",,,"Accuracy,  Completion Time,  Memory Performance","Questionnaires,  Performance"
ID187,Where Is the Sound: User Sound Source Perception in Virtual Reality Environment,"Chang  Enyao,  Che  Xiaoping,  Qu  Chenxin,  Xiao  Ruoshi,  Hu  Die,  Li  Zonglun",,10.1109/SWC57546.2023.10449009,2023,IEEE Smart World Congress,VR,Content & System Design,,Human-Computer Interaction (HCI),"This paper investigated whether spatial-audio rendering beats conventional multichannel sound for locating sources in VR. Participants wearing an Oculus Quest 2 heard nine scenes in which static, moving, or 180°-surround sources appeared above, below, in front, behind, left, or right. After each 10-second clip they picked the perceived direction, and after all scenes they rated immersion on the IEQ scale. HRTF spatial audio delivered significantly higher localisation scores than both 5.1 and 7.1 for most down- and back-oriented static and moving sources and for up- and down-surround sources; the multichannel modes showed no advantage over each other. IEQ ratings mirrored the accuracy pattern, giving spatial audio the strongest immersion. The authors conclude that head-tracked HRTF rendering provides clearer spatial cues and a more immersive experience than traditional surround formats, especially for vertical and rear sources.","1) Participants
• Number: 66 participants (33 males, 33 females).
• Age: Mostly college or graduate students aged 18–25 (mean = 21.5, SD = 3.14).
• Hearing Status: All reported no hearing impairments.
• VR Experience:
    ◦ 5 always used VR, 15 usually, 16 often, 25 sometimes, 5 had no prior VR experience.
2) Study Design
• Within-subjects design: All participants experienced all sound conditions.
• Independent Variables:
    ◦ Sound Mode: HRTF spatial audio vs. 5.1 surround sound vs. 7.1 surround sound.
    ◦ Sound Source Type:
        ▪ Static sound source (fixed position: up, down, left, right, front, back).
        ▪ Moving sound source (approaching participant at uniform speed).
        ▪ Surround sound source (180° movement around participant).
• Dependent Variables:
    ◦ Perception Accuracy: Correct identification of sound source direction.
    ◦ Immersion Score: Rated via questionnaire.
3) Procedure
1. Pre-test Questionnaire: Collected demographics and VR experience.
2. VR Setup:
    ◦ Used Oculus Quest 2 (HMD) + AirPods Pro (noise-canceling).
    ◦ Virtual environment: Forest cabin scene (Unity 3D).
3. Sound Exposure:
    ◦ Participants listened to 9 VR scenes (3 sound modes × 3 sound types).
    ◦ Each scene played a 10-second audio clip (e.g., cat sounds).
4. Post-Sound Task:
    ◦ After each clip, participants identified the sound source direction.
    ◦ Rated perceived sound location on a 5-point Likert scale (1 = not at all, 5 = very much).
5. Post-Experiment:
    ◦ Completed the Immersive Experience Questionnaire (IEQ) for immersion ratings.
    ◦ Conducted semi-structured interviews for qualitative feedback.
4) Task
• Primary Task: Identify the direction (up, down, left, right, front, back) of sound sources.
• Secondary Task: Rate immersion after each condition.
5) Metrics Collected
• Questionnaires:
    ◦ Pre-test: Demographics, VR experience.
    ◦ Post-scene: Likert-scale ratings for sound perception.
    ◦ Post-experiment: IEQ (immersion score).
• Performance Metrics:
    ◦ Accuracy of sound source localization (0–4 scoring).
• Behavioral Metrics:
    ◦ Semi-structured interview responses (qualitative insights on immersion).
• Statistical Analysis:
    ◦ ANOVA to compare sound modes.
    ◦ Tukey HSD for post-hoc comparisons.
Key Findings
• HRTF spatial audio significantly outperformed 5.1/7.1 surround sound in:
    ◦ Accuracy for static (down/back), moving (down/back), and surround (up/down) sources.
    ◦ Higher immersion scores (quantitative + qualitative).
• No major effects from gender or VR experience, but slight age/education correlations.
This experiment provides a clear comparison of audio technologies in VR, validating HRTF spatial audio as superior for user perception and immersion.","IEQ (Immersion Experience Questionnaire),  Likert Scale sound perception,  Semi-structured Interviews",,,,Questionnaires
ID188,Which User Guidance Works Better in VR? A User Guidance Learning Effect Study in Virtual Environment,"Zhu  Shuqin,  Che  Xiaoping,  Qu  Chenxin,  Li  Haohang,  Wang  Siyuan",,10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00170,2022,IEEE Smart World Congress,VR,Interaction Techniques & input Modalities,,Human-Computer Interaction (HCI),"This paper investigated which tutorial format best helps novices master basic controls in diverse VR games and how immersion and workload relate to learning. Among 105 students, interactive tutorials consistently lifted heart-rate engagement and yielded the highest operation-learning rates, especially in the most complex game (No Man’s Sky VR). Text-and-image tutorials produced lower immersion and smaller learning gains, while videos sat in between. Across all games, a higher heart-rate change correlated positively with better learning, but only when the tutorial was interactive; workload and presence showed no such link. The authors conclude that step-by-step interactive guidance is the most effective, and that heart-rate change can serve as a rough real-time proxy for learning progress in VR tutorials.","Participants play a regular game of 5-10 minutes in VR. The game scene involved in each
experiment is an independent level system, and the difficulty increases as the game progresses.","Likert Scale (5 point) joy frustration boredom,  NASA-TLX,  Presence – General",,Heart Rate,,"Questionnaires,  Physiological"
ID189,"Who Says You Are so Sick? An Investigation on Individual Susceptibility to Cybersickness Triggers Using EEG, EGG and ECG","Tian  Nana,  Boulic  Ronan",,10.1109/TVCG.2024.3372066,2024,Transactions on Visualization and Computer Graphics,VR,User states: Cognitive & Affective Experience,Metric Design & Validation.,Human-Computer Interaction (HCI),"This paper investigates individual susceptibility to cybersickness triggers by examining how rotational axes (roll, pitch, yaw) interact with physiological signals (EEG, EGG, ECG). Thirty-five participants played a VR gaze-based asteroid game while undergoing controlled rotational exposures. Subjective measures (SSQ, FMS) were combined with objective recordings. Results show that EGG features (tachygastria ratio, normal ratio, mean dominant frequency)correlate strongly with nausea and are reliable indicators of cybersickness severity, while ECG revealed changes in HRV but was less dependable alone. EEG provided valuable insights into individual differences in susceptibility: high-sensitive individuals showed distinct Alpha, Beta, Theta, and Delta activity patterns, and EEG was able to differentiate roll- vs. pitch-dominant sensitivities. The study also pioneers the use of aperiodic EEG activity as a novel marker of cybersickness. Together, findings highlight EEG and EGG as promising objective metrics for predicting both severity and susceptibility, and reinforce the role of roll rotation as the strongest trigger.","1) Participants
• Total: 35 healthy adults (18 female, 17 male).
• Age range: 20–45 years (mean = 23.3, SD = 4.4).
• Exclusions: 7 dropped out during the first session; final analysis included 28 participants.
• Recruitment: Local university students/staff.
• Pre-experiment restrictions: No alcohol/motion-sickness medications (12h prior), fasting (2h prior).
2) Study Design
• Within-subjects design: All participants underwent four VR sessions with varying rotational stimuli.
• Independent Variables:
    ◦ Rotation axes: Roll, Pitch, Yaw (manipulated across sessions).
    ◦ Session types:
        ▪ S1 (Worst-case): All three axes activated.
        ▪ S2 (Baseline): No rotations.
        ▪ S3 & S4: Participants’ self-selected ""least sickness-inducing"" axes (based on prior rankings).
• LIA Protocol: ""Least Increasing Aversion"" – Participants avoided re-exposure to worst-case rotations.
3) Procedure
1. Pre-Session:
    ◦ Consent, demographics, motion sickness history (MSSQ).
    ◦ Pre-SSQ questionnaire.
    ◦ 5-minute baseline physiological recording (EEG, ECG, EGG) without VR.
2. VR Exposure (20 min per session):
    ◦ Task: Asteroid-destroying game using eye-gaze interaction (to minimize body movement).
    ◦ Rotations: Passive, scene-based (Roll/Pitch/Yaw).
    ◦ Subjective ratings: FMS (Fast Motion Sickness scale) every minute.
3. Post-Session:
    ◦ Post-SSQ questionnaire.
    ◦ Interview to rank axis sensitivity (0–10 scale).
4) Task
• Primary Task: Destroy asteroids by fixating gaze on targets (eye-tracking interaction).
• Secondary Goal: Endure rotational stimuli without quitting (cybersickness tolerance).
5) Metrics CollectedCategoryMetricsPurposePhysiological- EEG: Periodic (Alpha/Beta/Theta/Delta) & aperiodic (exponent/offset) activity.- ECG: Heart rate, HRV (LF, HF, LF/HF ratio).- EGG: Gastric rhythms (normal/tachy/brady ratios, dominant frequency).Quantify cybersickness severity and neural/autonomic responses.Subjective- SSQ (Simulator Sickness Questionnaire): Pre/post sessions.- FMS (Fast Motion Sickness): Minute-by-minute nausea ratings during VR.Self-reported sickness levels.Behavioral- Session dropout rates (forced exits due to discomfort).- Eye-gaze interaction data (indirectly, via task performance).Assess tolerance and interaction effects.Performance- Task persistence (time until quitting).- Axis sensitivity rankings (post-session interviews).Link rotations to individual susceptibility.
Key Insights from the Experiment
• EGG was most effective for detecting nausea (tachy ratio ↑ with sickness).
• EEG revealed neural signatures of cybersickness (↓ Delta/Theta, ↑ Beta, aperiodic changes).
• ECG showed limited reliability (HRV changes were session-dependent).
• Individual Differences: Roll rotations caused the most sickness; EEG could distinguish Roll- vs. Pitch-dominant users.
This study provides a multi-modal framework for evaluating cybersickness, combining physiological, subjective, and behavioral metrics in a controlled VR setting.","Least increasing aversion (LIA) protocol,  Fast Motion Sickness Scale,  SSQ (Simulator Sickness Questionnaire)",,"ECG,  EGG,  EEG",,"Questionnaires,  Physiological"
ID190,Will Neural 3D Object Representations Be the Silver Bullet for Improving VR Experience in HMDs?,"Hsu  Charlie,  Sun  Yuan-Chun,  Lee  Kuan-Yu,  Huang  Chun-Ying",,10.1109/MIPR62202.2024.00043,2024,International Conference on Multimedia Information Processing and Retrieval,VR,Content & System Design,Visualization Techniques.,3D representation evaluation,"This paper evaluates how different 3D object representations—traditional 3D meshes, point clouds, neural radiance fields (NeRF), and 3D Gaussian Splatting (3DGS)—and different interaction modes (0-, 3-, and 6-DoF) affect presence and cybersickness in VR. Twelve participants observed objects rendered with each representation and answered a series of presence and cybersickness questions. The study finds that neural 3D representations, particularly 3DGS, yield higher presence scores, lower cybersickness, higher rendering frame rates, and smaller storage requirements. Interaction modes with more DoF (especially 6-DoF) also improve presence and slightly reduce sickness. ","Participants: 12 (ages 16–54)
Design: Within-subject; 21 sessions per participant
Task: View 3D objects for 30s, verbally answer presence & cybersickness items
Conditions:
• 4 representations: Mesh, Point Cloud, NeRF, 3DGS
• 3 interaction modes: 0-DoF, 3-DoF, 6-DoF
Measures:
• Presence (PQ0–PQ6) from Tran et al., Usoh et al.
• Cybersickness (CQ0–CQ9) based on Tran et al., Singla et al.
Data collected: ACR scores for presence and sickness; framerate logs
Goal: Compare representations and rendering performance","Cybersickness,  Presence – General",,,,Questionnaires
ID191,“If It’s Not Me It Doesn’t Make a Difference” - The Impact of Avatar Personalization on User Experience and Body Awareness in Virtual Reality,"Döllinger  Nina,  Beck  Matthias,  Wolf  Erik,  Mal  David,  Botsch  Mario,  Latoschik  Marc Erich,  Wienrich  Carolin",,10.1109/ISMAR59233.2023.00063,2023,International Symposium on Mixed and Augmented Reality (ISMAR),VR,Embodiment Avatars & Social Presence,,Healthcare and Medical Training,"This study examined how avatar individualization—generic, customized, and photorealistic—impacts body awareness and virtual reality user experience (VR UX), particularly in therapeutic settings. Using a between-subjects design with 86 participants, it found that photorealistic personalized avatars increased virtual body ownership (VBO) but reduced body awareness, potentially due to cognitive load or visual distraction. The study also revealed that body awareness positively correlates with VBO and sense of presence (SoP) across all conditions, but increased eeriness was also observed with highly personalized avatars. The findings suggest a trade-off between realism and therapeutic effectiveness, emphasizing the need to carefully balance avatar design in body-centered VR interventions.","1. Participants
• A total of 94 individuals participated in the 
study, but after exclusions (due to factors like photosensitivity, 
severe visual impairments, mobility difficulties, simulation sickness, 
and language proficiency), 86 participants remained.
• The participants were divided into three conditions based on avatar individualization:
    ◦ Generic condition: 29 participants (Mean age = 23.10 years)
    ◦ Customized condition: 29 participants (Mean age = 25.03 years)
    ◦ Personalized condition: 28 participants (Mean age = 21.54 years) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=16888,17478 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=17479,18154.
2. Study Design
• The study employed a 3 × 1 between-subject design, where participants were randomly assigned to one of three conditions with varying levels of avatar individualization:
    1. Generic: Participants embodied a generic, realistic-looking humanoid avatar.
    2. Customized: Participants selected the appearance of their avatar from a set of options.
    3. Personalized: Participants embodied photorealistic scans of their own bodies https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=21915,22705 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=15315,16119.
3. Procedure
• The experimental procedure consisted of three phases:
    1. Pre-VR Phase: Participants read study information, signed consent forms, and completed pre-VR assessments (MAIA, SSQ, HCT).
    2. In-VR Phase: Participants engaged in VR movement 
tasks, including embodiment tasks and body awareness movement tasks, 
guided by audio instructions.
    3. Post-VR Phase: Participants completed post-VR assessments (HCT, VEQ, SMS, UVI, SSQ, VHPS) https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=30188,30836 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=27614,28354.
4. Task
• Participants performed a series of movement tasks designed to evoke a sense of embodiment and body awareness. These included:
    ◦ Embodiment Tasks: Movements such as waving, lifting knees, and rotating hips.
    ◦ Body Awareness Movement Tasks: Exercises like squats, rotations, waves, and pushes, focusing on slow and attentive movements https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=26748,27613, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=28355,29257.
5. Metrics Collected
• Various metrics were collected to assess body awareness and user experience, including:
    ◦ Pre-VR Assessments: MAIA (Multidimensional 
Assessment of Interoceptive Awareness), SSQ (Simulator Sickness 
Questionnaire), HCT (Heartbeat Counting Task).
    ◦ In-VR Metrics: Sense of Embodiment (SoE), mindfulness, and body awareness.
    ◦ Post-VR Assessments: VEQ (Virtual Embodiment 
Questionnaire), SMS (State Mindfulness Scale), UVI (Uncanny Valley 
Index), VHPS (Virtual Human Plausibility Scale), and demographic 
information https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=25370,26082, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b362b315600e3d2fdd88c6/?range=27614,28354.
This summary encapsulates the key aspects of the experiments 
conducted in the study, providing a clear overview of the participants, 
design, procedure, tasks, and metrics collected.","Multidimensional Assessment of Interoceptive Awareness (MAIA),  Objectified Body Consciousness Scale,  SSQ (Simulator Sickness Questionnaire),  Sense of Embodiment,  State Mindfulness Scale,  Uncanny Valley Index,  Virtual Human Plausibility Scale",,,,Questionnaires
ID192,“To Be or Not to Be Me?”: Exploration of Self-Similar Effects of Avatars on Social Virtual Reality Experiences,"Kim  Hayeon,  Park  Jinhyung,  Lee  In-Kwon",,10.1109/TVCG.2023.3320240,2023,Transactions on Visualization and Computer Graphics,VR,Embodiment Avatars & Social Presence,,Telecommunications and Collaboration,"This study explores how self-similarity of avatars—manipulated through appearance, voice, and name—affects social VR experiences such as embodiment, presence, social presence, immersion, and self-awareness. Using a within-subjects design and validated questionnaires, 48 participants engaged in a cooperative VR game across eight avatar conditions. Results show that higher self-similarity significantly increased embodiment and social presence, but reduced immersion, due to increased self-awareness and diminished anonymity. Voice emerged as the most impactful factor. The study provides nuanced, generalizable insights into avatar representation and identity perception in social VR, making it a strong fit for your review.","1. Participants
• Sample Size: 48 participants (24 males and 24 females).
• Age Range: 22-38 years (M = 28.67, SD = 3.48).
• Demographics: All participants were of East Asian 
descent and identified as Korean. They had normal or corrected vision 
and hearing, and no history of epilepsy or simulator sickness. Most 
participants had limited prior VR experience https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=36747,37367 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=33961,34967.
2. Study Design
• Design Type: Within-subjects design.
• Conditions: Each participant experienced eight 
avatar conditions based on three representational factors: appearance, 
voice, and name. The conditions were manipulated to either match or not 
match the participant's identity https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=20840,21693, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=21696,22487.
3. Procedure
• Day 1: Participants completed a game tutorial after
 providing consent and safety information. They engaged in a VR 
environment using HTC VIVE Pro and completed questionnaires after each 
iteration. The average duration was 70 minutes.
• Day 2: Participants tested the remaining avatar conditions, followed by a semi-structured interview https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=34968,36012, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=33961,34967.
• Environment: Participants were placed in a quiet 
room with minimal distractions, and the order of conditions was 
randomized and counterbalanced https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=33961,34967, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=34968,36012.
4. Task
• Main Task: Participants engaged in a social VR game
 experience designed to foster collaboration. The game included a lobby 
and main game with four collaborative challenges https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=26554,27259, https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=33961,34967.
5. Metrics Collected
• Questionnaires: Participants completed validated questionnaires assessing:
    ◦ Self-Similarity: Evaluated how well the avatar represented the user.
    ◦ Embodiment: Measured through the Avatar Embodiment Questionnaire.
    ◦ Presence: Assessed using the Igroup Presence Questionnaire.
    ◦ Social Presence: Evaluated with the Multi-modal Presence Scale Questionnaire.
    ◦ Self-Awareness: Measured using the Situational Self-Awareness Scale and the Self-Consciousness Questionnaire https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=74475,75021 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=30431,31250.
• Behavioral Metrics: Observations of user interactions and behaviors in the VR environment https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=66081,67042 https://web.atlasti.com/projects/6786635cd7366702b3fc4255/sources/67b6fc5ce30d3f8d408187fc/?range=7463,8451.
This structured approach allowed the researchers to explore the 
effects of avatar self-similarity on various aspects of user experience 
in social VR environments.","IEQ (Immersion Experience Questionnaire),  IPQ (Igroup Presence Questionnaire),  Multimodal Presence Scale,  Self-Consciousness Questionnaire,  Embodiment Questionnaire,  Situational Self-Awareness Scale",,,,Questionnaires
